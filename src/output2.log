58800/139232 train accuracy:0.815612256526947
1049it [06:16,  2.80it/s]Epoch: 18, Loss: 0.4068047514415923 63000/139232 train accuracy:0.8156190514564514
1119it [06:41,  2.80it/s]Epoch: 18, Loss: 0.41028163364955356 67200/139232 train accuracy:0.8155654668807983
1189it [07:06,  2.80it/s]Epoch: 18, Loss: 0.4114774286179316 71400/139232 train accuracy:0.8152241110801697
1259it [07:31,  2.80it/s]Epoch: 18, Loss: 0.4098032052176339 75600/139232 train accuracy:0.8149603009223938
1329it [07:56,  2.80it/s]Epoch: 18, Loss: 0.39137898763020834 79800/139232 train accuracy:0.8154135346412659
1399it [08:21,  2.79it/s]Epoch: 18, Loss: 0.4075640578497024 84000/139232 train accuracy:0.8157261610031128
1469it [08:46,  2.78it/s]Epoch: 18, Loss: 0.42006815592447916 88200/139232 train accuracy:0.8152720928192139
1539it [09:11,  2.81it/s]Epoch: 18, Loss: 0.4062789771670387 92400/139232 train accuracy:0.8152489066123962
1609it [09:36,  2.80it/s]Epoch: 18, Loss: 0.4066662307012649 96600/139232 train accuracy:0.8151242136955261
1679it [10:01,  2.79it/s]Epoch: 18, Loss: 0.3993066987537202 100800/139232 train accuracy:0.8151488304138184
1749it [10:26,  2.80it/s]Epoch: 18, Loss: 0.3860525367373512 105000/139232 train accuracy:0.8158285617828369
1819it [10:51,  2.81it/s]Epoch: 18, Loss: 0.39793462844122024 109200/139232 train accuracy:0.8160256147384644
1889it [11:16,  2.80it/s]Epoch: 18, Loss: 0.3863897414434524 113400/139232 train accuracy:0.816428542137146
1959it [11:41,  2.80it/s]Epoch: 18, Loss: 0.40583295549665177 117600/139232 train accuracy:0.8164795637130737
2029it [12:06,  2.80it/s]Epoch: 18, Loss: 0.409346923828125 121800/139232 train accuracy:0.8165927529335022
2099it [12:31,  2.79it/s]Epoch: 18, Loss: 0.4103429013206845 126000/139232 train accuracy:0.8163889050483704
2169it [12:56,  2.80it/s]Epoch: 18, Loss: 0.3863233584449405 130200/139232 train accuracy:0.8165898323059082
2239it [13:21,  2.80it/s]Epoch: 18, Loss: 0.41533967517671133 134400/139232 train accuracy:0.8163987994194031
2309it [13:46,  2.79it/s]Epoch: 18, Loss: 0.4158810279482887 138600/139232 train accuracy:0.8162553906440735
2321it [13:50,  2.79it/s]
Epoch: 18, epoch Loss: 56349.43359375  train accuracy:0.8162634968757629

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.11it/s]
Epoch: 18, Valid Loss: 6539.0478515625  valid accuracy:0.8082094192504883
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:08<00:00,  8.37it/s]
Epoch: 18,   test accuracy:0.8069000840187073
37it [00:13,  2.80it/s]Epoch: 19, Loss: 0.22316391717819942 2280/139232 train accuracy:0.8131579160690308
69it [00:25,  2.79it/s]Epoch: 19, Loss: 0.189892578125 4200/139232 train accuracy:0.8050000071525574
139it [00:50,  2.78it/s]Epoch: 19, Loss: 0.39136085146949406 8400/139232 train accuracy:0.8157142996788025
209it [01:15,  2.79it/s]Epoch: 19, Loss: 0.404893798828125 12600/139232 train accuracy:0.8149206638336182
279it [01:40,  2.79it/s]Epoch: 19, Loss: 0.4069994245256696 16800/139232 train accuracy:0.8156547546386719
349it [02:05,  2.79it/s]Epoch: 19, Loss: 0.3937318057105655 21000/139232 train accuracy:0.8166666626930237
419it [02:30,  2.80it/s]Epoch: 19, Loss: 0.4059330531529018 25200/139232 train accuracy:0.8166666626930237
489it [02:55,  2.80it/s]Epoch: 19, Loss: 0.3928133428664434 29400/139232 train accuracy:0.817959189414978
559it [03:20,  2.79it/s]Epoch: 19, Loss: 0.391376226515997 33600/139232 train accuracy:0.8185714483261108
629it [03:45,  2.81it/s]Epoch: 19, Loss: 0.39252066476004466 37800/139232 train accuracy:0.8188095092773438
699it [04:10,  2.80it/s]Epoch: 19, Loss: 0.39905267624627977 42000/139232 train accuracy:0.8189523816108704
769it [04:35,  2.80it/s]Epoch: 19, Loss: 0.4068864222935268 46200/139232 train accuracy:0.8185498118400574
839it [05:01,  2.80it/s]Epoch: 19, Loss: 0.3995509847005208 50400/139232 train accuracy:0.8190873265266418
909it [05:26,  2.80it/s]Epoch: 19, Loss: 0.40211021786644346 54600/139232 train accuracy:0.8194139003753662
979it [05:51,  2.80it/s]Epoch: 19, Loss: 0.38908982049851193 58800/139232 train accuracy:0.819727897644043
1049it [06:16,  2.79it/s]Epoch: 19, Loss: 0.3977626255580357 63000/139232 train accuracy:0.8199047446250916
1119it [06:41,  2.81it/s]Epoch: 19, Loss: 0.39156729561941966 67200/139232 train accuracy:0.8200743794441223
1189it [07:06,  2.78it/s]Epoch: 19, Loss: 0.40022056942894346 71400/139232 train accuracy:0.8198179006576538
1259it [07:31,  2.79it/s]Epoch: 19, Loss: 0.38848211379278275 75600/139232 train accuracy:0.8201984167098999
1329it [07:56,  2.80it/s]Epoch: 19, Loss: 0.40258751278831845 79800/139232 train accuracy:0.8204511404037476
1399it [08:21,  2.79it/s]Epoch: 19, Loss: 0.3954932512555804 84000/139232 train accuracy:0.8203214406967163
1469it [08:46,  2.79it/s]Epoch: 19, Loss: 0.39559730166480656 88200/139232 train accuracy:0.8202381134033203
1539it [09:11,  2.80it/s]Epoch: 19, Loss: 0.39943173363095236 92400/139232 train accuracy:0.8201731443405151
1609it [09:36,  2.80it/s]Epoch: 19, Loss: 0.39579092843191965 96600/139232 train accuracy:0.8202587962150574
1679it [10:01,  2.79it/s]Epoch: 19, Loss: 0.3986955333891369 100800/139232 train accuracy:0.8202381134033203
1749it [10:26,  2.80it/s]Epoch: 19, Loss: 0.3859538051060268 105000/139232 train accuracy:0.820638120174408
1819it [10:51,  2.80it/s]Epoch: 19, Loss: 0.40411533900669644 109200/139232 train accuracy:0.8203388452529907
1889it [11:16,  2.81it/s]Epoch: 19, Loss: 0.39905450730096725 113400/139232 train accuracy:0.8199294805526733
1959it [11:41,  2.81it/s]Epoch: 19, Loss: 0.4052354503813244 117600/139232 train accuracy:0.81990647315979
2029it [12:06,  2.81it/s]Epoch: 19, Loss: 0.39633056640625 121800/139232 train accuracy:0.8199753761291504
2099it [12:31,  2.79it/s]Epoch: 19, Loss: 0.42008370535714284 126000/139232 train accuracy:0.8193888664245605
2169it [12:56,  2.82it/s]Epoch: 19, Loss: 0.3897758265904018 130200/139232 train accuracy:0.8194316625595093
2239it [13:21,  2.81it/s]Epoch: 19, Loss: 0.4050846644810268 134400/139232 train accuracy:0.8193303346633911
2309it [13:46,  2.80it/s]Epoch: 19, Loss: 0.3864160737537202 138600/139232 train accuracy:0.8196248412132263
2321it [13:50,  2.79it/s]
Epoch: 19, epoch Loss: 55460.2734375  train accuracy:0.8196966052055359

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.09it/s]
Epoch: 19, Valid Loss: 6548.8466796875  valid accuracy:0.8070458769798279
37it [00:13,  2.80it/s]Epoch: 20, Loss: 0.219341793968564 2280/139232 train accuracy:0.8166666626930237
69it [00:25,  2.80it/s]Epoch: 20, Loss: 0.18561915806361606 4200/139232 train accuracy:0.8190476298332214
139it [00:50,  2.79it/s]Epoch: 20, Loss: 0.4006482224237351 8400/139232 train accuracy:0.8171428442001343
209it [01:15,  2.78it/s]Epoch: 20, Loss: 0.40137061709449406 12600/139232 train accuracy:0.8175396919250488
279it [01:40,  2.81it/s]Epoch: 20, Loss: 0.40910557338169645 16800/139232 train accuracy:0.8168452382087708
349it [02:05,  2.79it/s]Epoch: 20, Loss: 0.38674903506324404 21000/139232 train accuracy:0.8183809518814087
419it [02:30,  2.81it/s]Epoch: 20, Loss: 0.39080793108258927 25200/139232 train accuracy:0.8193650841712952
489it [02:55,  2.81it/s]Epoch: 20, Loss: 0.3988652111235119 29400/139232 train accuracy:0.8192517161369324
559it [03:20,  2.80it/s]Epoch: 20, Loss: 0.39777099609375 33600/139232 train accuracy:0.8191071152687073
629it [03:45,  2.81it/s]Epoch: 20, Loss: 0.3809172712053571 37800/139232 train accuracy:0.8199735283851624
699it [04:10,  2.82it/s]Epoch: 20, Loss: 0.38538652692522324 42000/139232 train accuracy:0.8206190466880798
769it [04:35,  2.82it/s]Epoch: 20, Loss: 0.40058660598028273 46200/139232 train accuracy:0.8202381134033203
839it [05:00,  2.81it/s]Epoch: 20, Loss: 0.3908370245070685 50400/139232 train accuracy:0.8206745982170105
909it [05:25,  2.79it/s]Epoch: 20, Loss: 0.3953013102213542 54600/139232 train accuracy:0.8207875490188599
979it [05:50,  2.81it/s]Epoch: 20, Loss: 0.3934322974795387 58800/139232 train accuracy:0.8207823038101196
1049it [06:15,  2.81it/s]Epoch: 20, Loss: 0.3789125279017857 63000/139232 train accuracy:0.8212063312530518
1119it [06:40,  2.80it/s]Epoch: 20, Loss: 0.4004643322172619 67200/139232 train accuracy:0.8208482265472412
1189it [07:05,  2.82it/s]Epoch: 20, Loss: 0.3983090355282738 71400/139232 train accuracy:0.8206442594528198
1259it [07:30,  2.79it/s]Epoch: 20, Loss: 0.3953620837983631 75600/139232 train accuracy:0.8206613659858704
1329it [07:55,  2.80it/s]Epoch: 20, Loss: 0.38607918875558034 79800/139232 train accuracy:0.8209899663925171
1399it [08:20,  2.80it/s]Epoch: 20, Loss: 0.3995074172247024 84000/139232 train accuracy:0.8210476040840149
1469it [08:45,  2.81it/s]Epoch: 20, Loss: 0.3811613246372768 88200/139232 train accuracy:0.8214399218559265
1539it [09:10,  2.80it/s]Epoch: 20, Loss: 0.38763933454241073 92400/139232 train accuracy:0.8218614459037781
1609it [09:34,  2.81it/s]Epoch: 20, Loss: 0.3935737827845982 96600/139232 train accuracy:0.8216149210929871
1679it [10:00,  2.80it/s]Epoch: 20, Loss: 0.3910375395275298 100800/139232 train accuracy:0.8215079307556152
1749it [10:25,  2.80it/s]Epoch: 20, Loss: 0.4025892857142857 105000/139232 train accuracy:0.8214476108551025
1819it [10:50,  2.81it/s]Epoch: 20, Loss: 0.38870625813802084 109200/139232 train accuracy:0.8216117024421692
1889it [11:15,  2.80it/s]Epoch: 20, Loss: 0.3983585030691964 113400/139232 train accuracy:0.8215961456298828
1959it [11:40,  2.81it/s]Epoch: 20, Loss: 0.4055775378999256 117600/139232 train accuracy:0.8213945627212524
2029it [12:05,  2.81it/s]Epoch: 20, Loss: 0.3879460216703869 121800/139232 train accuracy:0.8216255903244019
2099it [12:30,  2.80it/s]Epoch: 20, Loss: 0.39611075265066964 126000/139232 train accuracy:0.8216349482536316
2169it [12:55,  2.79it/s]Epoch: 20, Loss: 0.39170192173549107 130200/139232 train accuracy:0.8216820359230042
2239it [13:20,  2.80it/s]Epoch: 20, Loss: 0.4018788655598958 134400/139232 train accuracy:0.8214806318283081
2309it [13:45,  2.80it/s]Epoch: 20, Loss: 0.3822578938802083 138600/139232 train accuracy:0.8215945363044739
2321it [13:49,  2.80it/s]
Epoch: 20, epoch Loss: 54886.6015625  train accuracy:0.8215209245681763

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.11it/s]
Epoch: 20, Valid Loss: 6570.3359375  valid accuracy:0.8066580295562744
37it [00:13,  2.81it/s]Epoch: 21, Loss: 0.21077896844773064 2280/139232 train accuracy:0.8214912414550781
69it [00:25,  2.81it/s]Epoch: 21, Loss: 0.17417400541759673 4200/139232 train accuracy:0.8219047784805298
139it [00:50,  2.81it/s]Epoch: 21, Loss: 0.3918252999441964 8400/139232 train accuracy:0.8207142949104309
209it [01:15,  2.80it/s]Epoch: 21, Loss: 0.38982427687872023 12600/139232 train accuracy:0.8211110830307007
279it [01:40,  2.79it/s]Epoch: 21, Loss: 0.38944147019159225 16800/139232 train accuracy:0.8207142949104309
349it [02:05,  2.80it/s]Epoch: 21, Loss: 0.3838627697172619 21000/139232 train accuracy:0.8224285840988159
419it [02:30,  2.78it/s]Epoch: 21, Loss: 0.38206226167224705 25200/139232 train accuracy:0.822857141494751
489it [02:55,  2.80it/s]Epoch: 21, Loss: 0.37520048595610117 29400/139232 train accuracy:0.8240135908126831
559it [03:20,  2.80it/s]Epoch: 21, Loss: 0.37744041806175593 33600/139232 train accuracy:0.8247321248054504
629it [03:45,  2.79it/s]Epoch: 21, Loss: 0.39380952380952383 37800/139232 train accuracy:0.8248412609100342
699it [04:10,  2.81it/s]Epoch: 21, Loss: 0.3765487816220238 42000/139232 train accuracy:0.8256190419197083
769it [04:35,  2.82it/s]Epoch: 21, Loss: 0.3816723342168899 46200/139232 train accuracy:0.8260389566421509
839it [05:00,  2.81it/s]Epoch: 21, Loss: 0.38210196358816967 50400/139232 train accuracy:0.8263293504714966
909it [05:25,  2.80it/s]Epoch: 21, Loss: 0.37735775902157737 54600/139232 train accuracy:0.8267216086387634
979it [05:50,  2.81it/s]Epoch: 21, Loss: 0.38051891508556546 58800/139232 train accuracy:0.8265306353569031
1049it [06:15,  2.79it/s]Epoch: 21, Loss: 0.3788438197544643 63000/139232 train accuracy:0.8270000219345093
1119it [06:40,  2.80it/s]Epoch: 21, Loss: 0.3957937476748512 67200/139232 train accuracy:0.8264136910438538
1189it [07:05,  2.80it/s]Epoch: 21, Loss: 0.3911190650576637 71400/139232 train accuracy:0.8263305425643921
1259it [07:30,  2.79it/s]Epoch: 21, Loss: 0.39620050339471724 75600/139232 train accuracy:0.8259920477867126
1329it [07:55,  2.78it/s]Epoch: 21, Loss: 0.3997377232142857 79800/139232 train accuracy:0.8255137801170349
1399it [08:20,  2.80it/s]Epoch: 21, Loss: 0.38370596749441965 84000/139232 train accuracy:0.8258809447288513
1469it [08:45,  2.79it/s]Epoch: 21, Loss: 0.40248677571614583 88200/139232 train accuracy:0.8256009221076965
1539it [09:10,  2.81it/s]Epoch: 21, Loss: 0.3996978759765625 92400/139232 train accuracy:0.8253571391105652
1609it [09:35,  2.80it/s]Epoch: 21, Loss: 0.3789064534505208 96600/139232 train accuracy:0.8256003856658936
1679it [10:01,  2.76it/s]Epoch: 21, Loss: 0.3868308512369792 100800/139232 train accuracy:0.8258234262466431
1749it [10:26,  2.81it/s]Epoch: 21, Loss: 0.3944493756975446 105000/139232 train accuracy:0.8258000016212463
1819it [10:51,  2.81it/s]Epoch: 21, Loss: 0.38637820289248515 109200/139232 train accuracy:0.8258150219917297
1889it [11:16,  2.79it/s]Epoch: 21, Loss: 0.39205784388950893 113400/139232 train accuracy:0.8255026340484619
1959it [11:41,  2.81it/s]Epoch: 21, Loss: 0.39062677292596726 117600/139232 train accuracy:0.8255016803741455
2029it [12:06,  2.79it/s]Epoch: 21, Loss: 0.3886864943731399 121800/139232 train accuracy:0.8255665302276611
2099it [12:31,  2.79it/s]Epoch: 21, Loss: 0.39578572591145833 126000/139232 train accuracy:0.825420618057251
2169it [12:56,  2.81it/s]Epoch: 21, Loss: 0.39137515113467264 130200/139232 train accuracy:0.8254531621932983
2239it [13:21,  2.80it/s]Epoch: 21, Loss: 0.38570905412946427 134400/139232 train accuracy:0.8254464268684387
2309it [13:46,  2.80it/s]Epoch: 21, Loss: 0.39712207612537204 138600/139232 train accuracy:0.825194776058197
2321it [13:50,  2.79it/s]
Epoch: 21, epoch Loss: 54005.609375  train accuracy:0.8252341151237488

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.11it/s]
Epoch: 21, Valid Loss: 6591.44921875  valid accuracy:0.8043309450149536
37it [00:13,  2.80it/s]Epoch: 22, Loss: 0.21113553001767113 2280/139232 train accuracy:0.8280701637268066
69it [00:25,  2.80it/s]Epoch: 22, Loss: 0.17901075090680804 4200/139232 train accuracy:0.8254761695861816
139it [00:50,  2.82it/s]Epoch: 22, Loss: 0.37859880719866074 8400/139232 train accuracy:0.825952410697937
209it [01:15,  2.78it/s]Epoch: 22, Loss: 0.3758896891276042 12600/139232 train accuracy:0.8284127116203308
279it [01:40,  2.81it/s]Epoch: 22, Loss: 0.3746213204520089 16800/139232 train accuracy:0.8307142853736877
349it [02:04,  2.80it/s]Epoch: 22, Loss: 0.38227582659040177 21000/139232 train accuracy:0.8289047479629517
419it [02:29,  2.81it/s]Epoch: 22, Loss: 0.3740391613188244 25200/139232 train accuracy:0.8290476202964783
489it [02:54,  2.80it/s]Epoch: 22, Loss: 0.3817776053292411 29400/139232 train accuracy:0.82833331823349
559it [03:20,  2.81it/s]Epoch: 22, Loss: 0.3770322091238839 33600/139232 train accuracy:0.8291666507720947
629it [03:45,  2.81it/s]Epoch: 22, Loss: 0.3920889427548363 37800/139232 train accuracy:0.8279629349708557
699it [04:10,  2.81it/s]Epoch: 22, Loss: 0.3747207496279762 42000/139232 train accuracy:0.8284761905670166
769it [04:35,  2.82it/s]Epoch: 22, Loss: 0.37923868815104167 46200/139232 train accuracy:0.8282900452613831
839it [05:00,  2.81it/s]Epoch: 22, Loss: 0.3914774867466518 50400/139232 train accuracy:0.8284524083137512
909it [05:25,  2.81it/s]Epoch: 22, Loss: 0.3739155796595982 54600/139232 train accuracy:0.8288278579711914
979it [05:49,  2.81it/s]Epoch: 22, Loss: 0.38089942568824403 58800/139232 train accuracy:0.8285714387893677
1049it [06:14,  2.81it/s]Epoch: 22, Loss: 0.3768057105654762 63000/139232 train accuracy:0.8289047479629517
1119it [06:39,  2.80it/s]Epoch: 22, Loss: 0.399742925734747 67200/139232 train accuracy:0.8285416960716248
1189it [07:04,  2.82it/s]Epoch: 22, Loss: 0.3754942103794643 71400/139232 train accuracy:0.8286694884300232
1259it [07:29,  2.80it/s]Epoch: 22, Loss: 0.3675343540736607 75600/139232 train accuracy:0.828941822052002
1329it [07:54,  2.80it/s]Epoch: 22, Loss: 0.39670910063244047 79800/139232 train accuracy:0.8282957673072815
1399it [08:19,  2.80it/s]Epoch: 22, Loss: 0.3877716064453125 84000/139232 train accuracy:0.8283214569091797
1469it [08:44,  2.80it/s]Epoch: 22, Loss: 0.37430579775855655 88200/139232 train accuracy:0.8284693956375122
1539it [09:09,  2.81it/s]Epoch: 22, Loss: 0.3854327392578125 92400/139232 train accuracy:0.8283766508102417
1609it [09:34,  2.81it/s]Epoch: 22, Loss: 0.39005182175409225 96600/139232 train accuracy:0.8281055688858032
1679it [09:59,  2.74it/s]Epoch: 22, Loss: 0.3884272984095982 100800/139232 train accuracy:0.8281051516532898
1749it [10:24,  2.81it/s]Epoch: 22, Loss: 0.38883629208519344 105000/139232 train accuracy:0.8279713988304138
1819it [10:49,  2.81it/s]Epoch: 22, Loss: 0.38035226004464284 109200/139232 train accuracy:0.8280768990516663
1889it [11:14,  2.81it/s]Epoch: 22, Loss: 0.3691153099423363 113400/139232 train accuracy:0.82833331823349
1959it [11:39,  2.80it/s]Epoch: 22, Loss: 0.37319463820684523 117600/139232 train accuracy:0.8286224603652954
2029it [12:04,  2.82it/s]Epoch: 22, Loss: 0.37952061244419644 121800/139232 train accuracy:0.8287109732627869
2099it [12:29,  2.81it/s]Epoch: 22, Loss: 0.3768398902529762 126000/139232 train accuracy:0.8287063241004944
2169it [12:54,  2.79it/s]Epoch: 22, Loss: 0.38058724539620536 130200/139232 train accuracy:0.8287941813468933
2239it [13:19,  2.82it/s]Epoch: 22, Loss: 0.37100652785528276 134400/139232 train accuracy:0.828913688659668
2309it [13:44,  2.81it/s]Epoch: 22, Loss: 0.3758139183407738 138600/139232 train accuracy:0.8292496204376221
2321it [13:48,  2.80it/s]
Epoch: 22, epoch Loss: 53004.734375  train accuracy:0.82929927110672

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.12it/s]
Epoch: 22, Valid Loss: 6564.79345703125  valid accuracy:0.8084033727645874
37it [00:13,  2.81it/s]Epoch: 23, Loss: 0.19419451032366072 2280/139232 train accuracy:0.8390350937843323
69it [00:25,  2.81it/s]Epoch: 23, Loss: 0.16624758765811012 4200/139232 train accuracy:0.8366666436195374
139it [00:50,  2.81it/s]Epoch: 23, Loss: 0.3789164225260417 8400/139232 train accuracy:0.8336904644966125
209it [01:15,  2.80it/s]Epoch: 23, Loss: 0.3897213309151786 12600/139232 train accuracy:0.8307142853736877
279it [01:40,  2.78it/s]Epoch: 23, Loss: 0.3915895298549107 16800/139232 train accuracy:0.8276190757751465
349it [02:04,  2.81it/s]Epoch: 23, Loss: 0.37675086611793157 21000/139232 train accuracy:0.827571451663971
419it [02:29,  2.81it/s]Epoch: 23, Loss: 0.3806948997860863 25200/139232 train accuracy:0.8271031975746155
489it [02:54,  2.81it/s]Epoch: 23, Loss: 0.36311816987537204 29400/139232 train accuracy:0.8291836977005005
559it [03:20,  2.80it/s]Epoch: 23, Loss: 0.3766364978608631 33600/139232 train accuracy:0.8295535445213318
629it [03:45,  2.79it/s]Epoch: 23, Loss: 0.3854733131045387 37800/139232 train accuracy:0.8298677206039429
699it [04:10,  2.80it/s]Epoch: 23, Loss: 0.37577503022693454 42000/139232 train accuracy:0.829714298248291
769it [04:35,  2.80it/s]Epoch: 23, Loss: 0.3714110456194196 46200/139232 train accuracy:0.8301082253456116
839it [05:00,  2.80it/s]Epoch: 23, Loss: 0.3799749465215774 50400/139232 train accuracy:0.8298611044883728
909it [05:25,  2.80it/s]Epoch: 23, Loss: 0.3656386311848958 54600/139232 train accuracy:0.830531120300293
979it [05:50,  2.80it/s]Epoch: 23, Loss: 0.3701154436383929 58800/139232 train accuracy:0.8309863805770874
1049it [06:15,  2.80it/s]Epoch: 23, Loss: 0.37898399716331843 63000/139232 train accuracy:0.8309682607650757
1119it [06:40,  2.79it/s]Epoch: 23, Loss: 0.37046177455357143 67200/139232 train accuracy:0.831250011920929
1189it [07:05,  2.79it/s]Epoch: 23, Loss: 0.36432076590401785 71400/139232 train accuracy:0.83173668384552
1259it [07:30,  2.81it/s]Epoch: 23, Loss: 0.3689886765252976 75600/139232 train accuracy:0.8318915367126465
1329it [07:55,  2.78it/s]Epoch: 23, Loss: 0.3653544689360119 79800/139232 train accuracy:0.8321303129196167
1399it [08:20,  2.80it/s]Epoch: 23, Loss: 0.3763116745721726 84000/139232 train accuracy:0.8320476412773132
1469it [08:45,  2.81it/s]Epoch: 23, Loss: 0.3702223132905506 88200/139232 train accuracy:0.8321881890296936
1539it [09:10,  2.81it/s]Epoch: 23, Loss: 0.3706990269252232 92400/139232 train accuracy:0.8325324654579163
1609it [09:35,  2.80it/s]Epoch: 23, Loss: 0.3625166538783482 96600/139232 train accuracy:0.8327846527099609
1679it [10:00,  2.66it/s]Epoch: 23, Loss: 0.37285641624813987 100800/139232 train accuracy:0.8328670859336853
1749it [10:25,  2.81it/s]Epoch: 23, Loss: 0.3773967052641369 105000/139232 train accuracy:0.8328857421875
1819it [10:50,  2.80it/s]Epoch: 23, Loss: 0.3696575637090774 109200/139232 train accuracy:0.8329487442970276
1889it [11:15,  2.80it/s]Epoch: 23, Loss: 0.37664713541666667 113400/139232 train accuracy:0.8326454758644104
1959it [11:40,  2.81it/s]Epoch: 23, Loss: 0.38505013602120536 117600/139232 train accuracy:0.8323469161987305
2029it [12:05,  2.80it/s]Epoch: 23, Loss: 0.3916353062220982 121800/139232 train accuracy:0.8320443630218506
2099it [12:30,  2.81it/s]Epoch: 23, Loss: 0.37896815708705356 126000/139232 train accuracy:0.8319761753082275
2169it [12:55,  2.81it/s]Epoch: 23, Loss: 0.37415367489769347 130200/139232 train accuracy:0.8320583701133728
2239it [13:20,  2.80it/s]Epoch: 23, Loss: 0.37152945382254465 134400/139232 train accuracy:0.8322544693946838
2309it [13:45,  2.80it/s]Epoch: 23, Loss: 0.36543730236235117 138600/139232 train accuracy:0.8323160409927368
2321it [13:49,  2.80it/s]
Epoch: 23, epoch Loss: 52150.046875  train accuracy:0.8322655558586121

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.10it/s]
Epoch: 23, Valid Loss: 6606.93896484375  valid accuracy:0.8042016625404358
37it [00:13,  2.81it/s]Epoch: 24, Loss: 0.20034608386811756 2280/139232 train accuracy:0.8333333134651184
69it [00:25,  2.79it/s]Epoch: 24, Loss: 0.16158053443545387 4200/139232 train accuracy:0.8357142806053162
139it [00:50,  2.81it/s]Epoch: 24, Loss: 0.3834495617094494 8400/139232 train accuracy:0.8336904644966125
209it [01:15,  2.81it/s]Epoch: 24, Loss: 0.37649623325892856 12600/139232 train accuracy:0.8307936787605286
279it [01:40,  2.81it/s]Epoch: 24, Loss: 0.3535228038969494 16800/139232 train accuracy:0.8342261910438538
349it [02:05,  2.80it/s]Epoch: 24, Loss: 0.37449393136160714 21000/139232 train accuracy:0.8347142934799194
419it [02:30,  2.80it/s]Epoch: 24, Loss: 0.3699527122860863 25200/139232 train accuracy:0.8347222208976746
489it [02:55,  2.80it/s]Epoch: 24, Loss: 0.37357852027529764 29400/139232 train accuracy:0.8349660038948059
559it [03:20,  2.81it/s]Epoch: 24, Loss: 0.3731492978050595 33600/139232 train accuracy:0.8349106907844543
629it [03:45,  2.81it/s]Epoch: 24, Loss: 0.366322021484375 37800/139232 train accuracy:0.8351587057113647
699it [04:10,  2.80it/s]Epoch: 24, Loss: 0.3676904587518601 42000/139232 train accuracy:0.8352142572402954
769it [04:35,  2.81it/s]Epoch: 24, Loss: 0.373546142578125 46200/139232 train accuracy:0.8353896141052246
839it [05:00,  2.81it/s]Epoch: 24, Loss: 0.37078206380208334 50400/139232 train accuracy:0.8352976441383362
909it [05:25,  2.81it/s]Epoch: 24, Loss: 0.3664549328031994 54600/139232 train accuracy:0.8355494737625122
979it [05:50,  2.81it/s]Epoch: 24, Loss: 0.34966020856584823 58800/139232 train accuracy:0.8363945484161377
1049it [06:15,  2.81it/s]Epoch: 24, Loss: 0.36500982375372026 63000/139232 train accuracy:0.8367778062820435
1119it [06:40,  2.80it/s]Epoch: 24, Loss: 0.3729515148344494 67200/139232 train accuracy:0.8364136815071106
1189it [07:05,  2.82it/s]Epoch: 24, Loss: 0.35788809640066965 71400/139232 train accuracy:0.8367787003517151
1259it [07:30,  2.80it/s]Epoch: 24, Loss: 0.3625768461681548 75600/139232 train accuracy:0.8367725014686584
1329it [07:55,  2.80it/s]Epoch: 24, Loss: 0.3782473900204613 79800/139232 train accuracy:0.8364160656929016
1399it [08:20,  2.77it/s]Epoch: 24, Loss: 0.385666998000372 84000/139232 train accuracy:0.8359047770500183
1469it [08:45,  2.80it/s]Epoch: 24, Loss: 0.3849264962332589 88200/139232 train accuracy:0.8355555534362793
1539it [09:10,  2.77it/s]Epoch: 24, Loss: 0.3569854445684524 92400/139232 train accuracy:0.8358766436576843
1609it [09:35,  2.81it/s]Epoch: 24, Loss: 0.3603020949590774 96600/139232 train accuracy:0.8360558748245239
1679it [10:00,  2.51it/s]Epoch: 24, Loss: 0.3657608177548363 100800/139232 train accuracy:0.835972249507904
1749it [10:25,  2.77it/s]Epoch: 24, Loss: 0.36275224958147323 105000/139232 train accuracy:0.8360666632652283
1819it [10:51,  2.82it/s]Epoch: 24, Loss: 0.35700477236793154 109200/139232 train accuracy:0.8362179398536682
1889it [11:16,  2.78it/s]Epoch: 24, Loss: 0.3748126220703125 113400/139232 train accuracy:0.8361728191375732
1959it [11:41,  2.81it/s]Epoch: 24, Loss: 0.3674216715494792 117600/139232 train accuracy:0.8363180160522461
2029it [12:06,  2.78it/s]Epoch: 24, Loss: 0.37150974818638394 121800/139232 train accuracy:0.836223304271698
2099it [12:31,  2.79it/s]Epoch: 24, Loss: 0.3616757347470238 126000/139232 train accuracy:0.8362063765525818
2169it [12:56,  2.77it/s]Epoch: 24, Loss: 0.36814979189918157 130200/139232 train accuracy:0.8363978266716003
2239it [13:21,  2.80it/s]Epoch: 24, Loss: 0.3658954438709077 134400/139232 train accuracy:0.836510419845581
2309it [13:46,  2.78it/s]Epoch: 24, Loss: 0.3746822974795387 138600/139232 train accuracy:0.8363203406333923
2321it [13:50,  2.79it/s]
Epoch: 24, epoch Loss: 51274.0859375  train accuracy:0.8364312648773193

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.13it/s]
Epoch: 24, Valid Loss: 6580.1025390625  valid accuracy:0.8083387017250061
37it [00:13,  2.78it/s]Epoch: 25, Loss: 0.18610151018415177 2280/139232 train accuracy:0.851754367351532
69it [00:25,  2.77it/s]Epoch: 25, Loss: 0.152183111281622 4200/139232 train accuracy:0.8526190519332886
139it [00:50,  2.76it/s]Epoch: 25, Loss: 0.35682779947916665 8400/139232 train accuracy:0.8478571176528931
209it [01:15,  2.77it/s]Epoch: 25, Loss: 0.3577382114955357 12600/139232 train accuracy:0.8470634818077087
279it [01:40,  2.81it/s]Epoch: 25, Loss: 0.35338460286458334 16800/139232 train accuracy:0.8469047546386719
349it [02:05,  2.78it/s]Epoch: 25, Loss: 0.3690655808221726 21000/139232 train accuracy:0.8446666598320007
419it [02:31,  2.78it/s]Epoch: 25, Loss: 0.3845445324125744 25200/139232 train accuracy:0.841865062713623
489it [02:56,  2.78it/s]Epoch: 25, Loss: 0.3566475132533482 29400/139232 train accuracy:0.842380940914154
559it [03:21,  2.78it/s]Epoch: 25, Loss: 0.37709216889880953 33600/139232 train accuracy:0.8410714268684387
629it [03:46,  2.78it/s]Epoch: 25, Loss: 0.3515977550688244 37800/139232 train accuracy:0.841005265712738
699it [04:11,  2.81it/s]Epoch: 25, Loss: 0.3644905308314732 42000/139232 train accuracy:0.8405952453613281
769it [04:36,  2.81it/s]Epoch: 25, Loss: 0.3513917468843006 46200/139232 train accuracy:0.8417532444000244
839it [05:01,  2.78it/s]Epoch: 25, Loss: 0.35469447544642857 50400/139232 train accuracy:0.8415277600288391
909it [05:26,  2.77it/s]Epoch: 25, Loss: 0.37332568940662203 54600/139232 train accuracy:0.8402197957038879
979it [05:51,  2.79it/s]Epoch: 25, Loss: 0.3641832624162946 58800/139232 train accuracy:0.8403061032295227
1049it [06:17,  2.76it/s]Epoch: 25, Loss: 0.3544064767020089 63000/139232 train accuracy:0.8406349420547485
1119it [06:42,  2.81it/s]Epoch: 25, Loss: 0.3733669607979911 67200/139232 train accuracy:0.8402231931686401
1189it [07:07,  2.77it/s]Epoch: 25, Loss: 0.3520213099888393 71400/139232 train accuracy:0.8404902219772339
1259it [07:32,  2.78it/s]Epoch: 25, Loss: 0.3575985572451637 75600/139232 train accuracy:0.8405687808990479
1329it [07:57,  2.81it/s]Epoch: 25, Loss: 0.35237920851934523 79800/139232 train accuracy:0.8409398794174194
1399it [08:22,  2.78it/s]Epoch: 25, Loss: 0.3723642113095238 84000/139232 train accuracy:0.8403928279876709
1469it [08:47,  2.80it/s]Epoch: 25, Loss: 0.3575489734468006 88200/139232 train accuracy:0.8406009078025818
1539it [09:12,  2.82it/s]Epoch: 25, Loss: 0.3589213634672619 92400/139232 train accuracy:0.8405194878578186
1609it [09:37,  2.80it/s]Epoch: 25, Loss: 0.36375479561941965 96600/139232 train accuracy:0.8405796885490417
1679it [10:03,  2.28it/s]Epoch: 25, Loss: 0.34800981794084823 100800/139232 train accuracy:0.8407242298126221
1749it [10:28,  2.79it/s]Epoch: 25, Loss: 0.3715649123418899 105000/139232 train accuracy:0.8403618931770325
1819it [10:53,  2.80it/s]Epoch: 25, Loss: 0.35822021484375 109200/139232 train accuracy:0.8404578566551208
1889it [11:18,  2.79it/s]Epoch: 25, Loss: 0.3717213076636905 113400/139232 train accuracy:0.8399029970169067
1959it [11:43,  2.76it/s]Epoch: 25, Loss: 0.3694563220796131 117600/139232 train accuracy:0.8395153284072876
2029it [12:08,  2.81it/s]Epoch: 25, Loss: 0.36251255580357145 121800/139232 train accuracy:0.8395237922668457
2099it [12:33,  2.81it/s]Epoch: 25, Loss: 0.3689699009486607 126000/139232 train accuracy:0.8395158648490906
2169it [12:58,  2.80it/s]Epoch: 25, Loss: 0.3737568591889881 130200/139232 train accuracy:0.8393087387084961
2239it [13:23,  2.76it/s]Epoch: 25, Loss: 0.3506956845238095 134400/139232 train accuracy:0.8393303751945496
2309it [13:48,  2.78it/s]Epoch: 25, Loss: 0.37580970400855657 138600/139232 train accuracy:0.8391702771186829
2321it [13:53,  2.79it/s]
Epoch: 25, epoch Loss: 50416.1171875  train accuracy:0.839110255241394

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.12it/s]
Epoch: 25, Valid Loss: 6651.0546875  valid accuracy:0.8030381202697754
37it [00:13,  2.80it/s]Epoch: 26, Loss: 0.19245275588262648 2280/139232 train accuracy:0.8337719440460205
69it [00:25,  2.81it/s]Epoch: 26, Loss: 0.14957781110491072 4200/139232 train accuracy:0.8461904525756836
139it [00:50,  2.80it/s]Epoch: 26, Loss: 0.3676824951171875 8400/139232 train accuracy:0.840833306312561
209it [01:15,  2.78it/s]Epoch: 26, Loss: 0.3522612072172619 12600/139232 train accuracy:0.8434920907020569
279it [01:40,  2.78it/s]Epoch: 26, Loss: 0.36231619698660716 16800/139232 train accuracy:0.8446428775787354
349it [02:05,  2.82it/s]Epoch: 26, Loss: 0.337515375046503 21000/139232 train accuracy:0.8464285731315613
419it [02:30,  2.81it/s]Epoch: 26, Loss: 0.35878775460379464 25200/139232 train accuracy:0.8449603319168091
489it [02:55,  2.78it/s]Epoch: 26, Loss: 0.3576334926060268 29400/139232 train accuracy:0.844149649143219
559it [03:20,  2.81it/s]Epoch: 26, Loss: 0.3444179861886161 33600/139232 train accuracy:0.8448511958122253
629it [03:45,  2.80it/s]Epoch: 26, Loss: 0.36383443196614584 37800/139232 train accuracy:0.8447089791297913
699it [04:10,  2.81it/s]Epoch: 26, Loss: 0.36379478817894345 42000/139232 train accuracy:0.8442142605781555
769it [04:35,  2.77it/s]Epoch: 26, Loss: 0.3495845249720982 46200/139232 train accuracy:0.8442640900611877
839it [05:00,  2.76it/s]Epoch: 26, Loss: 0.34677545456659226 50400/139232 train accuracy:0.844781756401062
909it [05:25,  2.81it/s]Epoch: 26, Loss: 0.34855169387090773 54600/139232 train accuracy:0.8450366258621216
979it [05:50,  2.79it/s]Epoch: 26, Loss: 0.3612281436011905 58800/139232 train accuracy:0.8444387912750244
1049it [06:16,  2.78it/s]Epoch: 26, Loss: 0.3593989780970982 63000/139232 train accuracy:0.8441587090492249
1119it [06:41,  2.78it/s]Epoch: 26, Loss: 0.3551806640625 67200/139232 train accuracy:0.8443005681037903
1189it [07:06,  2.81it/s]Epoch: 26, Loss: 0.35762515113467264 71400/139232 train accuracy:0.8440476059913635
1259it [07:31,  2.81it/s]Epoch: 26, Loss: 0.3408158947172619 75600/139232 train accuracy:0.8447486758232117
1329it [07:56,  2.81it/s]Epoch: 26, Loss: 0.3554760451543899 79800/139232 train accuracy:0.8445739150047302
1399it [08:21,  2.79it/s]Epoch: 26, Loss: 0.36674961635044645 84000/139232 train accuracy:0.8442500233650208
1469it [08:46,  2.81it/s]Epoch: 26, Loss: 0.3402563185918899 88200/139232 train accuracy:0.8446031808853149
1539it [09:11,  2.80it/s]Epoch: 26, Loss: 0.33606648763020835 92400/139232 train accuracy:0.8449783325195312
1609it [09:36,  2.79it/s]Epoch: 26, Loss: 0.3652395193917411 96600/139232 train accuracy:0.8448033332824707
1679it [10:01,  2.81it/s]Epoch: 26, Loss: 0.35753470284598216 100800/139232 train accuracy:0.8445833325386047
1749it [10:26,  2.81it/s]Epoch: 26, Loss: 0.35912405831473215 105000/139232 train accuracy:0.8445333242416382
1819it [10:51,  2.81it/s]Epoch: 26, Loss: 0.3460871814546131 109200/139232 train accuracy:0.8444780111312866
1889it [11:16,  2.79it/s]Epoch: 26, Loss: 0.3454147484188988 113400/139232 train accuracy:0.8448412418365479
1959it [11:41,  2.80it/s]Epoch: 26, Loss: 0.35555873325892856 117600/139232 train accuracy:0.8447619080543518
2029it [12:06,  2.80it/s]Epoch: 26, Loss: 0.35018668038504464 121800/139232 train accuracy:0.8447372913360596
2099it [12:31,  2.80it/s]Epoch: 26, Loss: 0.35799220493861605 126000/139232 train accuracy:0.8448016047477722
2169it [12:56,  2.80it/s]Epoch: 26, Loss: 0.3566254243396577 130200/139232 train accuracy:0.8448233604431152
2239it [13:21,  2.79it/s]Epoch: 26, Loss: 0.3517541213262649 134400/139232 train accuracy:0.8446577191352844
2309it [13:46,  2.81it/s]Epoch: 26, Loss: 0.3439158412388393 138600/139232 train accuracy:0.8448196053504944
2321it [13:50,  2.79it/s]
Epoch: 26, epoch Loss: 49181.5625  train accuracy:0.8447770476341248

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.12it/s]
Epoch: 26, Valid Loss: 6654.34130859375  valid accuracy:0.8057530522346497
37it [00:13,  2.81it/s]Epoch: 27, Loss: 0.18569925944010415 2280/139232 train accuracy:0.8495613932609558
69it [00:25,  2.80it/s]Epoch: 27, Loss: 0.1524764142717634 4200/139232 train accuracy:0.8504762053489685
139it [00:50,  2.80it/s]Epoch: 27, Loss: 0.34685828799293156 8400/139232 train accuracy:0.8478571176528931
209it [01:15,  2.81it/s]Epoch: 27, Loss: 0.34680158342633927 12600/139232 train accuracy:0.8487301468849182
279it [01:40,  2.79it/s]Epoch: 27, Loss: 0.33661289760044644 16800/139232 train accuracy:0.8503571152687073
349it [02:05,  2.81it/s]Epoch: 27, Loss: 0.3520560709635417 21000/139232 train accuracy:0.8496190309524536
419it [02:30,  2.81it/s]Epoch: 27, Loss: 0.3622986130487351 25200/139232 train accuracy:0.8476190567016602
489it [02:55,  2.79it/s]Epoch: 27, Loss: 0.33604274204799106 29400/139232 train accuracy:0.847449004650116
559it [03:20,  2.80it/s]Epoch: 27, Loss: 0.3416471644810268 33600/139232 train accuracy:0.8482142686843872
629it [03:45,  2.81it/s]Epoch: 27, Loss: 0.34522115071614584 37800/139232 train accuracy:0.8484391570091248
699it [04:10,  2.80it/s]Epoch: 27, Loss: 0.3486580403645833 42000/139232 train accuracy:0.8482857346534729
769it [04:35,  2.79it/s]Epoch: 27, Loss: 0.34743108840215775 46200/139232 train accuracy:0.8483549952507019
839it [05:00,  2.80it/s]Epoch: 27, Loss: 0.3558685302734375 50400/139232 train accuracy:0.8479762077331543
909it [05:25,  2.80it/s]Epoch: 27, Loss: 0.34786844889322915 54600/139232 train accuracy:0.8483333587646484
979it [05:50,  2.78it/s]Epoch: 27, Loss: 0.3513558523995536 58800/139232 train accuracy:0.8481122255325317
1049it [06:15,  2.79it/s]Epoch: 27, Loss: 0.34495887393043156 63000/139232 train accuracy:0.8479999899864197
1119it [06:40,  2.81it/s]Epoch: 27, Loss: 0.34870265415736607 67200/139232 train accuracy:0.848229169845581
1189it [07:05,  2.80it/s]Epoch: 27, Loss: 0.34606677827380955 71400/139232 train accuracy:0.848109245300293
1259it [07:30,  2.81it/s]Epoch: 27, Loss: 0.3557225690569196 75600/139232 train accuracy:0.8474206328392029
1329it [07:55,  2.81it/s]Epoch: 27, Loss: 0.34525605701264883 79800/139232 train accuracy:0.8473809361457825
1399it [08:20,  2.80it/s]Epoch: 27, Loss: 0.33389453706287203 84000/139232 train accuracy:0.8477023839950562
1469it [08:45,  2.80it/s]Epoch: 27, Loss: 0.3368771507626488 88200/139232 train accuracy:0.8481292724609375
1539it [09:10,  2.79it/s]Epoch: 27, Loss: 0.34693827311197917 92400/139232 train accuracy:0.8479762077331543
1609it [09:35,  2.82it/s]Epoch: 27, Loss: 0.3453504289899554 96600/139232 train accuracy:0.847888171672821
1679it [10:00,  2.79it/s]Epoch: 27, Loss: 0.34527445475260415 100800/139232 train accuracy:0.8479762077331543
1749it [10:26,  2.80it/s]Epoch: 27, Loss: 0.32762108212425595 105000/139232 train accuracy:0.8485714197158813
1819it [10:51,  2.79it/s]Epoch: 27, Loss: 0.35375799269903274 109200/139232 train accuracy:0.8481684923171997
1889it [11:16,  2.80it/s]Epoch: 27, Loss: 0.3386724562872024 113400/139232 train accuracy:0.8484479784965515
1959it [11:41,  2.79it/s]Epoch: 27, Loss: 0.3292857142857143 117600/139232 train accuracy:0.8487585186958313
2029it [12:06,  2.79it/s]Epoch: 27, Loss: 0.3625973946707589 121800/139232 train accuracy:0.8484482765197754
2099it [12:31,  2.80it/s]Epoch: 27, Loss: 0.34305829729352677 126000/139232 train accuracy:0.848642885684967
2169it [12:56,  2.79it/s]Epoch: 27, Loss: 0.3511916097005208 130200/139232 train accuracy:0.8484715819358826
2239it [13:21,  2.80it/s]Epoch: 27, Loss: 0.3515425618489583 134400/139232 train accuracy:0.8482738137245178
2309it [13:46,  2.79it/s]Epoch: 27, Loss: 0.3611723109654018 138600/139232 train accuracy:0.8480880260467529
2321it [13:50,  2.79it/s]
Epoch: 27, epoch Loss: 48201.6015625  train accuracy:0.8480880856513977

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.12it/s]
Epoch: 27, Valid Loss: 6680.29931640625  valid accuracy:0.8037492036819458
37it [00:13,  2.78it/s]Epoch: 28, Loss: 0.18516903831845238 2280/139232 train accuracy:0.8521929979324341
69it [00:25,  2.79it/s]Epoch: 28, Loss: 0.14987227666945685 4200/139232 train accuracy:0.8557142615318298
139it [00:50,  2.80it/s]Epoch: 28, Loss: 0.33985438755580355 8400/139232 train accuracy:0.8559523820877075
209it [01:15,  2.81it/s]Epoch: 28, Loss: 0.33103626069568454 12600/139232 train accuracy:0.855555534362793
279it [01:40,  2.80it/s]Epoch: 28, Loss: 0.3261334809802827 16800/139232 train accuracy:0.8552976250648499
349it [02:05,  2.80it/s]Epoch: 28, Loss: 0.32879612513950895 21000/139232 train accuracy:0.8563809394836426
419it [02:30,  2.79it/s]Epoch: 28, Loss: 0.34618570963541667 25200/139232 train accuracy:0.8547618985176086
489it [02:55,  2.80it/s]Epoch: 28, Loss: 0.3358839925130208 29400/139232 train accuracy:0.8548639416694641
559it [03:20,  2.77it/s]Epoch: 28, Loss: 0.3454281761532738 33600/139232 train accuracy:0.8546130657196045
629it [03:45,  2.81it/s]Epoch: 28, Loss: 0.3371598888578869 37800/139232 train accuracy:0.8547883629798889
659it [03:56,  2.79it/s]^C
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/train_bert_proto_cascade_add_loss.py", line 565, in <module>
    gradients = tape.gradient(loss, ProtoCNN.trainable_weights)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py", line 1113, in gradient
    flat_grad = imperative_grad.imperative_grad(
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py", line 67, in imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py", line 160, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/ops/array_grad.py", line 801, in _ReshapeGrad
    array_ops.reshape(
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py", line 1176, in op_dispatch_handler
    return dispatch_target(*args, **kwargs)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py", line 199, in reshape
    result = gen_array_ops.reshape(tensor, shape, name)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py", line 8538, in reshape
    _result = pywrap_tfe.TFE_Py_FastPathExecute(
KeyboardInterrupt

(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ ls
 baseline_loss.txt         losses.pkl             process_data.py                  Prototype_CNN_Bert.py     runs              train_bert_cascade.py                  training_validation_loss.png
 Bert_cnn.py               loss_log.txt           proto_losses.pkl                 Prototype_CNN.ipynb       test.py           train_bert_proto_cascade_add_loss.py   train_proto_cascade_add_loss.py
 BERT_CNN.py               mainbalancedpickle.p   ProtoryNet.py                    Prototype_CNN-old.ipynb   text_cnn_tf2.py   train_bert_proto_cascade.py           'visualization analysis'
'Data Exploration.ipynb'   original               Prototype_cnn_bert_continue.py   PrototypeCNN.py           text_cnn_tf.py    train_cascade.py
 data_helpers.py           output.log             Prototype_CNN_Bert.ipynb         __pycache__               tmux-buffer.txt   train_glove_cnn_cascade.py
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ tmux capture-pane -pS - > output.log
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCA
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python train_bert_proto_cascade_add_loss.py
2024-01-01 13:33:22.967983: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:33:23.104672: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:33:23.655488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:33:23.655569: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:33:23.655591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
^Cdata loaded!
loading wgcca embeddings...
wgcca embeddings loaded
^CTraceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/train_bert_proto_cascade_add_loss.py", line 316, in <module>
    wgcca_dict[ids[i]] = int(i)
KeyboardInterrupt
^C
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ ^C
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 13:33:41.182726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:33:41.320969: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:33:41.874353: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:33:41.874434: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:33:41.874444: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 13:33:49.121527: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:33:49.233975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 13:33:49.416928: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 475, in <module>
    vect = ProtoCNN.embed(batch)
AttributeError: 'TextCNN' object has no attribute 'embed'
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 13:38:11.614907: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:38:11.751050: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:38:12.275198: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:38:12.275283: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:38:12.275292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 13:38:19.666324: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:38:19.768934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 13:38:19.951371: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 13:38:22.436585: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 495, in <module>
    vect = ProtoCNN.embed_user(batch)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 227, in embed_user
    user_embeddings = self.user_embedding(input_author)
NameError: name 'input_author' is not defined. Did you mean: 'test_author'?
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 13:39:19.654804: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:39:19.791387: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:39:20.340226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:39:20.340305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:39:20.340314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 13:39:27.730056: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:39:27.831835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 13:39:28.012866: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 13:39:30.413436: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 495, in <module>
    vect = ProtoCNN.embed_user(batch)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 227, in embed_user
    user_embeddings = self.user_embedding(x)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/backend.py", line 1596, in dtype
    return x.dtype.base_dtype.name
AttributeError: Exception encountered when calling layer "embedding" "                 f"(type Embedding).

'list' object has no attribute 'dtype'

Call arguments received by layer "embedding" "                 f"(type Embedding):
  • inputs=["'religion must have the answer'", '"it \'s obviously tracks from a giant water tractor , farming for giant arctic sea prawn !"', "'wow he smoked pot oh lord hes such a horrible person now'", '"wow , his girlfriend is uhm ah
fuck it , he \'s an olympic champion , who am i to pass judgement"', "'i think the government should track every mormon in the country for subversive activity'", "'another idea from the party that wants to get government off our backs'",
 "'oh right , both wars were just jewish conspiracies to reclaim the homeland of israel'", "'i know this seems strange but , what if he was talking about armenians'", "'good luck with that'", "'time to get that shack in montana'", "'i fo
r one , am glad we have priorities straight'", "'neato !'", "'then they must believe in god'", "'but i thought morality was supposed to be impossible without religion to provide it to us'", '"that \'s not a fair way to use that argument
!"', '"so you ca n\'t photograph police because of the anti terrorism law , a law designed to protect all of us from terrorism , well what \'s to stop the police and government from terrorizing us now \\\\?"', "'odd that they collided in
 the ocean , and not at takeoff or while merging into traffic at flight level 30'", "'they did a bang up job'", "'only \\\\?'", "'nothing says high intelligence like logical consistancy'", "'legends always have unfulfilled careers'", "'o
h wow i am so surprised i never saw this coming'", "'sweet can i check all my credit card with this'", "'this scam is so old the last time i entered my credit card number into it they used my information to buy a triceratops'", '"do n\'t
 forget homophobic vegetarian who drives a suv and grabs guns"', "'wow you read my mind'", "'really \\\\?'", "'oh i thought they were cursing coz they liked it too much'", '"only for folks who already have a net worth of 10 million or mo
re otherwise it \'s socialism"', "'too expensive , it should be given to only 27 year olds'", '"nothing like slave labor from people in college who wo n\'t be able to get a job without doing work for free !"', '"oh yeah , this totally do
es n\'t violate the fair labor standards act"', '"because they \'ll be too busy sucking cocks to notice they are getting shot at"', "'employers public and private are allowed to discriminate based on sexual orientation'", "'am i weird if
 almost every one of those pictures looks cultish to me \\\\?'", "'man are they gonna be pissed when they find out they picked the wrong religion'", "'i thought it was repetitive at first , but then i had to spacebar while suspended abov
e a fatal fall and had to move blocks around to prevent my own death'", '"ca n\'t wait for the app to come out for the insert apple portable device so i can enjoy this like the rest of you"', "'and this is exactly why i approve of death
panels'", "'good lord my chart is the exact opposite of this'", '"hahahaha they do n\'t speak english !"', "'why is this a picture of something happening in india , with a krishna figure behind the sign \\\\?'", '"i hear there \'s a comp
any making a stick on handle for it so you can use it like a phone surprised apple did n\'t think of that"', '"the day they add emacs to appstore , i \'ll buy an ipad"', "'i am deeply shocked and surprised at this development'", "'film a
t 11'", "'this is pure xenophobia against vaticanians !'", "'deja vu all over again'", "'but the terrorists !'", "'there are 58 now , they are still scanning the rest'"]
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 13:41:03.503935: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:41:03.640687: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:41:04.190086: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:41:04.190166: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:41:04.190176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 13:41:11.394310: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:41:11.494798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 13:41:11.675321: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 13:41:14.069978: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 495, in <module>
    vect = ProtoCNN.embed_user(batch)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 227, in embed_user
    user_embeddings = self.user_embedding(x)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/backend.py", line 1596, in dtype
    return x.dtype.base_dtype.name
AttributeError: Exception encountered when calling layer "embedding" "                 f"(type Embedding).

'list' object has no attribute 'dtype'

Call arguments received by layer "embedding" "                 f"(type Embedding):
  • inputs=['tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tenso
r(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int
32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shap
e=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)',
'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(),
dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Te
nsor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=
int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)']
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 13:43:26.107968: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:43:26.244096: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:43:26.793817: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:43:26.793900: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:43:26.793909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 13:43:34.096170: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:43:34.199588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 13:43:34.381350: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 13:43:36.784541: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 510, in <module>
    ProtoCNN.init_prototypelayer(res_cents, user_cents)
NameError: name 'res_cents' is not defined. Did you mean: 'k_cents'?
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 13:44:44.971089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:44:45.107434: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:44:45.657286: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:44:45.657367: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:44:45.657377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 13:44:52.904305: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:44:53.004988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 13:44:53.187601: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 13:44:55.664299: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 510, in <module>
    ProtoCNN.init_prototypelayer(res_cents, user_cents)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 156, in init_prototypelayer
    self.user_proto_layer = prototypeLayer(self.k_protos, self.vect_size, user_cents)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 74, in __init__
    self.prototypes = make_variables("prototypes", k_protos, vect_size,
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 24, in make_variables
    return tf.Variable(initializer(shape=[k1, k2], dtype=tf.float32), trainable=True, name=tf_name)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/ops/init_ops_v2.py", line 291, in __call__
    return constant_op.constant(self.value, dtype=dtype, shape=shape)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 267, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 279, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 326, in _constant_eager_impl
    raise TypeError("Eager execution of tf.constant with unsupported shape. "
TypeError: Eager execution of tf.constant with unsupported shape. Tensor [[ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 ...
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]] (converted from [[ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 ...
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]]) has 1500 elements, but got `shape` (15, 384) with 5760 elements).
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 13:47:30.507205: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:47:30.643431: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:47:31.191722: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:47:31.191805: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:47:31.191814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 13:47:38.429290: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:47:38.530368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 13:47:38.712328: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 13:47:41.199498: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 510, in <module>
    ProtoCNN.init_prototypelayer(res_cents, user_cents)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 156, in init_prototypelayer
    self.user_proto_layer = prototypeLayer(self.k_protos, 1500, user_cents)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 74, in __init__
    self.prototypes = make_variables("prototypes", k_protos, vect_size,
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 24, in make_variables
    return tf.Variable(initializer(shape=[k1, k2], dtype=tf.float32), trainable=True, name=tf_name)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/ops/init_ops_v2.py", line 291, in __call__
    return constant_op.constant(self.value, dtype=dtype, shape=shape)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 267, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 279, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 326, in _constant_eager_impl
    raise TypeError("Eager execution of tf.constant with unsupported shape. "
TypeError: Eager execution of tf.constant with unsupported shape. Tensor [[ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 ...
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]] (converted from [[ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 ...
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]]) has 1500 elements, but got `shape` (15, 1500) with 22500 elements).
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 13:50:55.943136: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:50:56.079634: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:50:56.626376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:50:56.626458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:50:56.626468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 13:51:03.936341: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:51:04.040443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 13:51:04.221146: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 13:51:06.610448: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 512, in <module>
    ProtoCNN.init_prototypelayer(res_cents, user_cents)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 156, in init_prototypelayer
    self.user_proto_layer = prototypeLayer(self.k_protos, 1500, user_cents)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 74, in __init__
    self.prototypes = make_variables("prototypes", k_protos, vect_size,
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 24, in make_variables
    return tf.Variable(initializer(shape=[k1, k2], dtype=tf.float32), trainable=True, name=tf_name)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/ops/init_ops_v2.py", line 291, in __call__
    return constant_op.constant(self.value, dtype=dtype, shape=shape)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 267, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 279, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 326, in _constant_eager_impl
    raise TypeError("Eager execution of tf.constant with unsupported shape. "
TypeError: Eager execution of tf.constant with unsupported shape. Tensor [[ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 ...
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]] (converted from [[ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 ...
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]
 [ 1.3315865   0.715279   -1.5454003  ...  0.08958761  0.82699865
  -1.9545121 ]]) has 1500 elements, but got `shape` (15, 1500) with 22500 elements).
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/P
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/P
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEAR
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 13:57:55.380491: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:57:55.518225: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 13:57:56.069229: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:57:56.069315: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 13:57:56.069324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 13:58:03.332499: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 13:58:03.432101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 13:58:03.614051: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 13:58:06.011809: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 515, in <module>
    predictions = ProtoCNN([x_train[:2].tolist(), author_train[:2], topic_train[:2]])
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 188, in call
    full_distances, user_protos = self.user_proto_layer(user_embeddings)
  File "/tmp/__autograph_generated_filewbddppkx.py", line 11, in tf__call
    tmp1 = ag__.converted_call(ag__.ld(tf).broadcast_to, (ag__.ld(tmp1), [ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(tmp1),), None, fscope)[0], ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(tmp1),), None, fscope)[1], ag__.ld(self)
.n_protos, ag__.ld(self).vect_size]), None, fscope)
ValueError: Exception encountered when calling layer "proto_layer" "                 f"(type prototypeLayer).

in user code:

    File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 81, in call  *
        tmp1 = tf.broadcast_to(tmp1, [tf.shape(tmp1)[0], tf.shape(tmp1)[1], self.n_protos, self.vect_size])

    ValueError: Dimensions must be equal, but are 100 and 15 for '{{node BroadcastTo}} = BroadcastTo[T=DT_FLOAT, Tidx=DT_INT32](ExpandDims, BroadcastTo/shape)' with input shapes: [2,100,1], [4] and with input tensors computed as partial
shapes: input[1] = [2,100,15,100].


Call arguments received by layer "proto_layer" "                 f"(type prototypeLayer):
  • inputs=tf.Tensor(shape=(2, 100), dtype=float32)
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 14:03:07.301257: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 14:03:07.438779: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 14:03:07.992785: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 14:03:07.992864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 14:03:07.992874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 14:03:15.449140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 14:03:15.559688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 14:03:15.744694: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 14:03:18.239722: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
37it [00:27,  2.89it/s]Epoch: 1, Loss: 0.3966073753720238 2280/139232 train accuracy:0.5688596367835999
69it [00:38,  2.88it/s]Epoch: 1, Loss: 0.33933933803013394 4200/139232 train accuracy:0.5719047784805298
139it [01:03,  2.88it/s]Epoch: 1, Loss: 0.7247982933407738 8400/139232 train accuracy:0.5690476298332214
209it [01:27,  2.87it/s]Epoch: 1, Loss: 0.72012939453125 12600/139232 train accuracy:0.5708730220794678
279it [01:52,  2.86it/s]Epoch: 1, Loss: 0.7012705775669643 16800/139232 train accuracy:0.5768452286720276
349it [02:16,  2.85it/s]Epoch: 1, Loss: 0.6915513392857143 21000/139232 train accuracy:0.5798571705818176
419it [02:41,  2.87it/s]Epoch: 1, Loss: 0.6894525437127976 25200/139232 train accuracy:0.5820634961128235
489it [03:05,  2.88it/s]Epoch: 1, Loss: 0.6819300478980654 29400/139232 train accuracy:0.5856122374534607
559it [03:30,  2.86it/s]Epoch: 1, Loss: 0.6685372488839286 33600/139232 train accuracy:0.5899106860160828
629it [03:54,  2.87it/s]Epoch: 1, Loss: 0.6706735956101191 37800/139232 train accuracy:0.5927248597145081
699it [04:18,  2.87it/s]Epoch: 1, Loss: 0.6524540783110119 42000/139232 train accuracy:0.5971428751945496
769it [04:43,  2.85it/s]Epoch: 1, Loss: 0.6475194149925595 46200/139232 train accuracy:0.600649356842041
839it [05:07,  2.86it/s]Epoch: 1, Loss: 0.6349454171316964 50400/139232 train accuracy:0.6049801707267761
909it [05:32,  2.85it/s]Epoch: 1, Loss: 0.6181806873139881 54600/139232 train accuracy:0.6090109944343567
979it [05:56,  2.86it/s]Epoch: 1, Loss: 0.6217332240513392 58800/139232 train accuracy:0.6134693622589111
1049it [06:21,  2.85it/s]Epoch: 1, Loss: 0.6180174618675596 63000/139232 train accuracy:0.617127001285553
1119it [06:46,  2.82it/s]Epoch: 1, Loss: 0.6093246023995535 67200/139232 train accuracy:0.6208482384681702
1189it [07:30,  1.62it/s]Epoch: 1, Loss: 0.61044921875 71400/139232 train accuracy:0.6241316795349121
1259it [07:55,  2.87it/s]Epoch: 1, Loss: 0.611330798921131 75600/139232 train accuracy:0.6271560788154602
1329it [08:19,  2.87it/s]Epoch: 1, Loss: 0.584902111235119 79800/139232 train accuracy:0.6313032507896423
1399it [08:44,  2.86it/s]Epoch: 1, Loss: 0.5770168922061012 84000/139232 train accuracy:0.6349285840988159
1469it [09:09,  2.87it/s]Epoch: 1, Loss: 0.577107166108631 88200/139232 train accuracy:0.6385147571563721
1539it [09:33,  2.85it/s]Epoch: 1, Loss: 0.5748906017485119 92400/139232 train accuracy:0.6419264078140259
1609it [09:58,  2.86it/s]Epoch: 1, Loss: 0.5769903273809524 96600/139232 train accuracy:0.6452070474624634
1679it [10:22,  2.86it/s]Epoch: 1, Loss: 0.5640406436011904 100800/139232 train accuracy:0.6483432650566101
1749it [10:47,  2.86it/s]Epoch: 1, Loss: 0.5646745372953869 105000/139232 train accuracy:0.6515142917633057
1819it [11:11,  2.86it/s]Epoch: 1, Loss: 0.5573692103794643 109200/139232 train accuracy:0.6542490720748901
1889it [11:36,  2.85it/s]Epoch: 1, Loss: 0.5581760370163691 113400/139232 train accuracy:0.6570899486541748
1959it [12:00,  2.86it/s]Epoch: 1, Loss: 0.5524376278831845 117600/139232 train accuracy:0.6596088409423828
2029it [12:25,  2.85it/s]Epoch: 1, Loss: 0.5503848702566965 121800/139232 train accuracy:0.6619293689727783
2099it [12:49,  2.85it/s]Epoch: 1, Loss: 0.5399958728608631 126000/139232 train accuracy:0.6645396947860718
2169it [13:14,  2.86it/s]Epoch: 1, Loss: 0.5333088611421131 130200/139232 train accuracy:0.6670045852661133
2239it [13:38,  2.85it/s]Epoch: 1, Loss: 0.5323135230654762 134400/139232 train accuracy:0.669330358505249
2309it [14:03,  2.86it/s]Epoch: 1, Loss: 0.5494319080171131 138600/139232 train accuracy:0.6713131070137024
2321it [14:15,  2.71it/s]
Epoch: 1, epoch Loss: 85601.2265625  train accuracy:0.6716559529304504

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.13it/s]
Epoch: 1, Valid Loss: 7478.89111328125  valid accuracy:0.790239155292511
find better loss
  0%|                                                                                                                                                                                     | 0/1078 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 685, in <module>
    predictions, _, _ = ProtoCNN([x_batch, author_batch, topic_batch], training=False)
ValueError: too many values to unpack (expected 3)
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 14:26:04.263978: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 14:26:04.400748: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 14:26:04.953650: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 14:26:04.953731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 14:26:04.953740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 14:26:12.212896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 14:26:12.325690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 14:26:12.509435: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 14:26:14.927263: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
37it [00:27,  2.90it/s]Epoch: 1, Loss: 0.6596097237723214 2280/139232 train accuracy:0.4942982494831085
69it [00:38,  2.90it/s]Epoch: 1, Loss: 0.49142333984375 4200/139232 train accuracy:0.5038095116615295
139it [01:03,  2.89it/s]Epoch: 1, Loss: 1.0246444847470237 8400/139232 train accuracy:0.5051190257072449
209it [01:27,  2.88it/s]Epoch: 1, Loss: 0.9565791248139881 12600/139232 train accuracy:0.5136507749557495
279it [01:52,  2.88it/s]Epoch: 1, Loss: 0.8830975632440476 16800/139232 train accuracy:0.5151190757751465
349it [02:16,  2.88it/s]Epoch: 1, Loss: 0.848326648530506 21000/139232 train accuracy:0.5216190218925476
419it [02:40,  2.89it/s]Epoch: 1, Loss: 0.8211027018229167 25200/139232 train accuracy:0.523809552192688
489it [03:04,  2.88it/s]Epoch: 1, Loss: 0.7657626488095238 29400/139232 train accuracy:0.5293877720832825
559it [03:29,  2.88it/s]Epoch: 1, Loss: 0.7383664085751488 33600/139232 train accuracy:0.5354166626930237
629it [03:53,  2.87it/s]Epoch: 1, Loss: 0.730893787202381 37800/139232 train accuracy:0.5392592549324036
699it [04:17,  2.87it/s]Epoch: 1, Loss: 0.718629150390625 42000/139232 train accuracy:0.543404757976532
769it [04:42,  2.87it/s]Epoch: 1, Loss: 0.6932766578311012 46200/139232 train accuracy:0.5482683777809143
839it [05:06,  2.87it/s]Epoch: 1, Loss: 0.6939743768601191 50400/139232 train accuracy:0.5532341003417969
909it [05:31,  2.88it/s]Epoch: 1, Loss: 0.6874568103608631 54600/139232 train accuracy:0.5574725270271301
979it [05:55,  2.88it/s]Epoch: 1, Loss: 0.6791423107328869 58800/139232 train accuracy:0.5612244606018066
1049it [06:19,  2.87it/s]Epoch: 1, Loss: 0.6890082077752976 63000/139232 train accuracy:0.5641428828239441
1119it [06:44,  2.86it/s]Epoch: 1, Loss: 0.6761460658482142 67200/139232 train accuracy:0.5674702525138855
1189it [07:08,  2.88it/s]Epoch: 1, Loss: 0.6688471912202381 71400/139232 train accuracy:0.5705041885375977
1259it [07:32,  2.87it/s]Epoch: 1, Loss: 0.6612859816778274 75600/139232 train accuracy:0.5741666555404663
1329it [07:57,  2.88it/s]Epoch: 1, Loss: 0.6418274507068452 79800/139232 train accuracy:0.5778571367263794
1399it [08:21,  2.86it/s]Epoch: 1, Loss: 0.6396825590587798 84000/139232 train accuracy:0.58134526014328
1469it [08:46,  2.87it/s]Epoch: 1, Loss: 0.6365131487165179 88200/139232 train accuracy:0.5846711993217468
1539it [09:10,  2.86it/s]Epoch: 1, Loss: 0.6412259347098215 92400/139232 train accuracy:0.5877705812454224
1609it [09:35,  2.86it/s]Epoch: 1, Loss: 0.6274665178571428 96600/139232 train accuracy:0.5910145044326782
1679it [09:59,  2.87it/s]Epoch: 1, Loss: 0.6270699637276785 100800/139232 train accuracy:0.5943750143051147
1749it [10:24,  2.87it/s]Epoch: 1, Loss: 0.6213481212797619 105000/139232 train accuracy:0.5972762107849121
1819it [10:48,  2.86it/s]Epoch: 1, Loss: 0.6161801292782738 109200/139232 train accuracy:0.6003296971321106
1889it [11:13,  2.87it/s]Epoch: 1, Loss: 0.6217244466145834 113400/139232 train accuracy:0.6030775904655457
1959it [11:37,  2.86it/s]Epoch: 1, Loss: 0.6160169619605654 117600/139232 train accuracy:0.6057907938957214
2029it [12:02,  2.86it/s]Epoch: 1, Loss: 0.5919524855840774 121800/139232 train accuracy:0.6088669896125793
2099it [12:26,  2.85it/s]Epoch: 1, Loss: 0.5871600632440476 126000/139232 train accuracy:0.6116666793823242
2169it [12:51,  2.87it/s]Epoch: 1, Loss: 0.5743864513578869 130200/139232 train accuracy:0.6147541999816895
2239it [13:15,  2.85it/s]Epoch: 1, Loss: 0.5820801362537202 134400/139232 train accuracy:0.6173363327980042
2256it [13:21,  2.81it/s]^C
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 587, in <module>
    gradients = tape.gradient(loss, ProtoCNN.trainable_weights)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py", line 1113, in gradient
    flat_grad = imperative_grad.imperative_grad(
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py", line 67, in imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py", line 160, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py", line 256, in _MeanGrad
    factor = constant_op.constant(factor, dtype=sum_grad.dtype)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 267, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 279, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 304, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 102, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
KeyboardInterrupt
^C
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEAR
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ python two-prototype-layer.py
2024-01-01 14:41:36.942919: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 14:41:37.079302: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-01 14:41:37.630552: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY
_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 14:41:37.630637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directo
ry; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:
2024-01-01 14:41:37.630646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentione
d above are installed properly.
loading data...
data loaded!
loading wgcca embeddings...
wgcca embeddings loaded
topic emb size:  100
Train/Dev split: 139232/15470
2024-01-01 14:41:45.010154: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operation
s:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-01 14:41:45.126674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0
, compute capability: 8.6
2024-01-01 14:41:45.308017: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
2024-01-01 14:41:47.705333: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700
37it [00:27,  2.91it/s]Epoch: 1, Loss: 0.450421142578125 2280/139232 train accuracy:0.5513157844543457
69it [00:38,  2.92it/s]Epoch: 1, Loss: 0.3640114339192708 4200/139232 train accuracy:0.5557143092155457
139it [01:02,  2.91it/s]Epoch: 1, Loss: 0.758521961030506 8400/139232 train accuracy:0.5610714554786682
209it [01:26,  2.90it/s]Epoch: 1, Loss: 0.7492838541666667 12600/139232 train accuracy:0.5621428489685059
279it [01:51,  2.90it/s]Epoch: 1, Loss: 0.7239880952380953 16800/139232 train accuracy:0.5653571486473083
349it [02:15,  2.89it/s]Epoch: 1, Loss: 0.7085816592261904 21000/139232 train accuracy:0.5692856907844543
419it [02:39,  2.89it/s]Epoch: 1, Loss: 0.7063396344866072 25200/139232 train accuracy:0.5740873217582703
489it [03:04,  2.91it/s]Epoch: 1, Loss: 0.6958798363095238 29400/139232 train accuracy:0.5796258449554443
559it [03:28,  2.89it/s]Epoch: 1, Loss: 0.6919703311011904 33600/139232 train accuracy:0.5854762196540833
629it [03:52,  2.89it/s]Epoch: 1, Loss: 0.6841563197544643 37800/139232 train accuracy:0.5886772274971008
699it [04:16,  2.89it/s]Epoch: 1, Loss: 0.6691834077380953 42000/139232 train accuracy:0.5934761762619019
769it [04:40,  2.88it/s]Epoch: 1, Loss: 0.6635389927455357 46200/139232 train accuracy:0.5965151786804199
839it [05:05,  2.88it/s]Epoch: 1, Loss: 0.684975120907738 50400/139232 train accuracy:0.5985714197158813
909it [05:29,  2.89it/s]Epoch: 1, Loss: 0.6510056849888393 54600/139232 train accuracy:0.6020329594612122
979it [05:53,  2.88it/s]Epoch: 1, Loss: 0.6591866629464286 58800/139232 train accuracy:0.6048639416694641
1049it [06:18,  2.87it/s]Epoch: 1, Loss: 0.6649265834263393 63000/139232 train accuracy:0.6074920892715454
1119it [06:42,  2.89it/s]Epoch: 1, Loss: 0.6456775483630952 67200/139232 train accuracy:0.6100297570228577
1189it [07:06,  2.87it/s]Epoch: 1, Loss: 0.6550433640252976 71400/139232 train accuracy:0.6125349998474121
1259it [07:31,  2.88it/s]Epoch: 1, Loss: 0.6421405901227678 75600/139232 train accuracy:0.6151058077812195
1329it [07:55,  2.89it/s]Epoch: 1, Loss: 0.6179153878348215 79800/139232 train accuracy:0.6183583736419678
1399it [08:19,  2.86it/s]Epoch: 1, Loss: 0.6084438941592262 84000/139232 train accuracy:0.6215833425521851
1469it [08:44,  2.87it/s]Epoch: 1, Loss: 0.6077674502418154 88200/139232 train accuracy:0.6243877410888672
1539it [09:08,  2.88it/s]Epoch: 1, Loss: 0.6171253603980654 92400/139232 train accuracy:0.6265367865562439
1609it [09:33,  2.86it/s]Epoch: 1, Loss: 0.5976285226004464 96600/139232 train accuracy:0.6289544701576233
1679it [09:57,  2.88it/s]Epoch: 1, Loss: 0.6142848423549108 100800/139232 train accuracy:0.630684494972229
1749it [10:21,  2.87it/s]Epoch: 1, Loss: 0.6069930013020833 105000/139232 train accuracy:0.6325333118438721
1819it [10:46,  2.87it/s]Epoch: 1, Loss: 0.6108032807849703 109200/139232 train accuracy:0.6341941356658936
1889it [11:10,  2.88it/s]Epoch: 1, Loss: 0.593538062686012 113400/139232 train accuracy:0.6365696787834167
1959it [11:34,  2.88it/s]Epoch: 1, Loss: 0.6049941871279761 117600/139232 train accuracy:0.638069748878479
2029it [11:59,  2.88it/s]Epoch: 1, Loss: 0.5851020159040179 121800/139232 train accuracy:0.640114963054657
2099it [12:23,  2.87it/s]Epoch: 1, Loss: 0.5985416666666666 126000/139232 train accuracy:0.6417618989944458
2169it [12:47,  2.88it/s]Epoch: 1, Loss: 0.5903291248139881 130200/139232 train accuracy:0.64370197057724
2239it [13:12,  2.86it/s]Epoch: 1, Loss: 0.582537086123512 134400/139232 train accuracy:0.6455580592155457
2309it [13:36,  2.86it/s]Epoch: 1, Loss: 0.5902936662946429 138600/139232 train accuracy:0.6471789479255676
2321it [13:48,  2.80it/s]
Epoch: 1, epoch Loss: 90654.6875  train accuracy:0.6474445462226868

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.18it/s]
Epoch: 1, Valid Loss: 7661.55419921875  valid accuracy:0.790239155292511
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:08<00:00,  8.39it/s]
Epoch: 1,   test accuracy:0.7915287613868713
37it [00:13,  2.90it/s]Epoch: 2, Loss: 0.31092735653831843 2280/139232 train accuracy:0.7250000238418579
69it [00:24,  2.91it/s]Epoch: 2, Loss: 0.2654028029668899 4200/139232 train accuracy:0.715238094329834
139it [00:48,  2.91it/s]Epoch: 2, Loss: 0.5737582542782738 8400/139232 train accuracy:0.7189285755157471
209it [01:13,  2.88it/s]Epoch: 2, Loss: 0.5647311546688988 12600/139232 train accuracy:0.7214285731315613
279it [01:37,  2.88it/s]Epoch: 2, Loss: 0.5680478050595238 16800/139232 train accuracy:0.7230356931686401
349it [02:01,  2.90it/s]Epoch: 2, Loss: 0.5653914388020833 21000/139232 train accuracy:0.724142849445343
419it [02:25,  2.89it/s]Epoch: 2, Loss: 0.5686768740699405 25200/139232 train accuracy:0.7238888740539551
489it [02:50,  2.89it/s]Epoch: 2, Loss: 0.5602877371651785 29400/139232 train accuracy:0.7252380847930908
559it [03:14,  2.89it/s]Epoch: 2, Loss: 0.5539061337425595 33600/139232 train accuracy:0.7269642949104309
629it [03:38,  2.87it/s]Epoch: 2, Loss: 0.5487864467075892 37800/139232 train accuracy:0.7283333539962769
699it [04:02,  2.88it/s]Epoch: 2, Loss: 0.5470027669270834 42000/139232 train accuracy:0.7303095459938049
769it [04:27,  2.87it/s]Epoch: 2, Loss: 0.5558026994977678 46200/139232 train accuracy:0.7306709885597229
839it [04:51,  2.87it/s]Epoch: 2, Loss: 0.557074962797619 50400/139232 train accuracy:0.7312698364257812
909it [05:15,  2.89it/s]Epoch: 2, Loss: 0.5512677873883929 54600/139232 train accuracy:0.7318314909934998
979it [05:40,  2.87it/s]Epoch: 2, Loss: 0.5483090936569941 58800/139232 train accuracy:0.7322959303855896
1049it [06:04,  2.88it/s]Epoch: 2, Loss: 0.549576416015625 63000/139232 train accuracy:0.7326984405517578
1119it [06:28,  2.88it/s]Epoch: 2, Loss: 0.5430303664434524 67200/139232 train accuracy:0.7331696152687073
1189it [06:53,  2.87it/s]Epoch: 2, Loss: 0.5419845726376488 71400/139232 train accuracy:0.7341456413269043
1259it [07:17,  2.87it/s]Epoch: 2, Loss: 0.5371212332589286 75600/139232 train accuracy:0.7350528836250305
1329it [07:42,  2.88it/s]Epoch: 2, Loss: 0.53845458984375 79800/139232 train accuracy:0.7359398603439331
1399it [08:06,  2.86it/s]Epoch: 2, Loss: 0.5424185616629464 84000/139232 train accuracy:0.7363333106040955
1469it [08:31,  2.87it/s]Epoch: 2, Loss: 0.5394949776785715 88200/139232 train accuracy:0.7370521426200867
1539it [08:55,  2.86it/s]Epoch: 2, Loss: 0.5282933407738095 92400/139232 train accuracy:0.7380519509315491
1609it [09:20,  2.88it/s]Epoch: 2, Loss: 0.5165815662202381 96600/139232 train accuracy:0.739016592502594
1679it [09:44,  2.87it/s]Epoch: 2, Loss: 0.5404443359375 100800/139232 train accuracy:0.7395833134651184
1749it [10:08,  2.87it/s]Epoch: 2, Loss: 0.5374956984747024 105000/139232 train accuracy:0.7402856945991516
1819it [10:33,  2.87it/s]Epoch: 2, Loss: 0.5230481538318452 109200/139232 train accuracy:0.7406593561172485
1889it [10:57,  2.87it/s]Epoch: 2, Loss: 0.5209309895833333 113400/139232 train accuracy:0.7412786483764648
1959it [11:22,  2.88it/s]Epoch: 2, Loss: 0.5187948172433036 117600/139232 train accuracy:0.7417346835136414
2029it [11:46,  2.88it/s]Epoch: 2, Loss: 0.5239746675037202 121800/139232 train accuracy:0.7423727512359619
2099it [12:10,  2.87it/s]Epoch: 2, Loss: 0.5215059407552083 126000/139232 train accuracy:0.7429364919662476
2169it [12:35,  2.87it/s]Epoch: 2, Loss: 0.5304908389136904 130200/139232 train accuracy:0.7434254884719849
2239it [12:59,  2.87it/s]Epoch: 2, Loss: 0.5299668666294642 134400/139232 train accuracy:0.743571400642395
2309it [13:24,  2.87it/s]Epoch: 2, Loss: 0.513458251953125 138600/139232 train accuracy:0.7442207932472229
2321it [13:28,  2.87it/s]
Epoch: 2, epoch Loss: 75640.765625  train accuracy:0.7444193959236145

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.21it/s]
Epoch: 2, Valid Loss: 7175.2509765625  valid accuracy:0.790239155292511
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.35it/s]
Epoch: 2,   test accuracy:0.7915287613868713
37it [00:13,  2.88it/s]Epoch: 3, Loss: 0.2672473725818452 2280/139232 train accuracy:0.7833333611488342
69it [00:24,  2.87it/s]Epoch: 3, Loss: 0.2340507289341518 4200/139232 train accuracy:0.7764285802841187
139it [00:49,  2.86it/s]Epoch: 3, Loss: 0.5210506766183036 8400/139232 train accuracy:0.7705952525138855
209it [01:13,  2.86it/s]Epoch: 3, Loss: 0.5200856817336309 12600/139232 train accuracy:0.7681745886802673
279it [01:38,  2.86it/s]Epoch: 3, Loss: 0.4984815615699405 16800/139232 train accuracy:0.768928587436676
349it [02:02,  2.86it/s]Epoch: 3, Loss: 0.5012114025297619 21000/139232 train accuracy:0.7699047327041626
419it [02:27,  2.86it/s]Epoch: 3, Loss: 0.5129995582217262 25200/139232 train accuracy:0.7690079212188721
489it [02:51,  2.86it/s]Epoch: 3, Loss: 0.5130115908668155 29400/139232 train accuracy:0.7695918083190918
559it [03:16,  2.87it/s]Epoch: 3, Loss: 0.5074265834263393 33600/139232 train accuracy:0.7695833444595337
629it [03:40,  2.85it/s]Epoch: 3, Loss: 0.5024568103608631 37800/139232 train accuracy:0.7702116370201111
699it [04:05,  2.86it/s]Epoch: 3, Loss: 0.5234794108072917 42000/139232 train accuracy:0.7697142958641052
769it [04:29,  2.86it/s]Epoch: 3, Loss: 0.5016688755580357 46200/139232 train accuracy:0.770519495010376
839it [04:54,  2.87it/s]Epoch: 3, Loss: 0.5174100748697916 50400/139232 train accuracy:0.7698809504508972
909it [05:18,  2.87it/s]Epoch: 3, Loss: 0.49216169084821426 54600/139232 train accuracy:0.7704212665557861
979it [05:43,  2.86it/s]Epoch: 3, Loss: 0.49476190476190474 58800/139232 train accuracy:0.770510196685791
1049it [06:07,  2.85it/s]Epoch: 3, Loss: 0.5003702799479167 63000/139232 train accuracy:0.770380973815918
1119it [06:32,  2.85it/s]Epoch: 3, Loss: 0.5107875279017857 67200/139232 train accuracy:0.7698958516120911
1189it [06:56,  2.86it/s]Epoch: 3, Loss: 0.5044090053013393 71400/139232 train accuracy:0.7700279951095581
1259it [07:21,  2.86it/s]Epoch: 3, Loss: 0.5007311430431548 75600/139232 train accuracy:0.7700926065444946
1329it [07:46,  2.87it/s]Epoch: 3, Loss: 0.5114253743489583 79800/139232 train accuracy:0.770087718963623
1399it [08:10,  2.86it/s]Epoch: 3, Loss: 0.5036063058035715 84000/139232 train accuracy:0.7697737812995911
1469it [08:35,  2.85it/s]Epoch: 3, Loss: 0.4973939732142857 88200/139232 train accuracy:0.7698186039924622
1539it [08:59,  2.85it/s]Epoch: 3, Loss: 0.4947499302455357 92400/139232 train accuracy:0.770422101020813
1609it [09:24,  2.86it/s]Epoch: 3, Loss: 0.5005974469866071 96600/139232 train accuracy:0.7704969048500061
1679it [09:48,  2.86it/s]Epoch: 3, Loss: 0.49795875186011906 100800/139232 train accuracy:0.7709127068519592
1749it [10:13,  2.86it/s]Epoch: 3, Loss: 0.4844592575799851 105000/139232 train accuracy:0.7713714241981506
1819it [10:37,  2.85it/s]Epoch: 3, Loss: 0.48827194940476193 109200/139232 train accuracy:0.771794855594635
1889it [11:02,  2.86it/s]Epoch: 3, Loss: 0.49539847237723217 113400/139232 train accuracy:0.7718165516853333
1959it [11:26,  2.86it/s]Epoch: 3, Loss: 0.48982200985863095 117600/139232 train accuracy:0.7720493078231812
2029it [11:51,  2.86it/s]Epoch: 3, Loss: 0.49807640438988093 121800/139232 train accuracy:0.7720279097557068
2099it [12:15,  2.86it/s]Epoch: 3, Loss: 0.49110171363467264 126000/139232 train accuracy:0.772095263004303
2169it [12:40,  2.87it/s]Epoch: 3, Loss: 0.4833959379650298 130200/139232 train accuracy:0.7722427248954773
2239it [13:04,  2.85it/s]Epoch: 3, Loss: 0.47463370186941967 134400/139232 train accuracy:0.7727231979370117
2309it [13:29,  2.87it/s]Epoch: 3, Loss: 0.48947364443824404 138600/139232 train accuracy:0.7728066444396973
2321it [13:33,  2.85it/s]
Epoch: 3, epoch Loss: 69712.015625  train accuracy:0.7728539705276489

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.21it/s]
Epoch: 3, Valid Loss: 6961.931640625  valid accuracy:0.790239155292511
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.35it/s]
Epoch: 3,   test accuracy:0.7915287613868713
37it [00:13,  2.86it/s]Epoch: 4, Loss: 0.27672185988653275 2280/139232 train accuracy:0.7675438523292542
69it [00:25,  2.85it/s]Epoch: 4, Loss: 0.21236700148809523 4200/139232 train accuracy:0.779285728931427
139it [00:49,  2.85it/s]Epoch: 4, Loss: 0.48663876488095237 8400/139232 train accuracy:0.7794047594070435
209it [01:14,  2.86it/s]Epoch: 4, Loss: 0.4958160109747024 12600/139232 train accuracy:0.7765873074531555
279it [01:38,  2.85it/s]Epoch: 4, Loss: 0.4644723074776786 16800/139232 train accuracy:0.7783928513526917
349it [02:03,  2.86it/s]Epoch: 4, Loss: 0.4824824451264881 21000/139232 train accuracy:0.779285728931427
419it [02:27,  2.85it/s]Epoch: 4, Loss: 0.5045271228608631 25200/139232 train accuracy:0.7784523963928223
489it [02:52,  2.87it/s]Epoch: 4, Loss: 0.4981437755766369 29400/139232 train accuracy:0.7774829864501953
559it [03:16,  2.87it/s]Epoch: 4, Loss: 0.4807075427827381 33600/139232 train accuracy:0.7779762148857117
629it [03:41,  2.86it/s]Epoch: 4, Loss: 0.4723036121186756 37800/139232 train accuracy:0.7785449624061584
699it [04:05,  2.87it/s]Epoch: 4, Loss: 0.4797428094773065 42000/139232 train accuracy:0.7786190509796143
769it [04:30,  2.86it/s]Epoch: 4, Loss: 0.47210425967261904 46200/139232 train accuracy:0.779675304889679
839it [04:54,  2.86it/s]Epoch: 4, Loss: 0.46951709565662203 50400/139232 train accuracy:0.7809523940086365
909it [05:19,  2.84it/s]Epoch: 4, Loss: 0.48487127394903273 54600/139232 train accuracy:0.7812637090682983
979it [05:43,  2.86it/s]Epoch: 4, Loss: 0.4748501150948661 58800/139232 train accuracy:0.7814965844154358
1049it [06:08,  2.85it/s]Epoch: 4, Loss: 0.48971859886532737 63000/139232 train accuracy:0.7810952663421631
1119it [06:32,  2.86it/s]Epoch: 4, Loss: 0.466843494233631 67200/139232 train accuracy:0.7813988327980042
1189it [06:57,  2.85it/s]Epoch: 4, Loss: 0.47024768647693455 71400/139232 train accuracy:0.781232476234436
1259it [07:22,  2.85it/s]Epoch: 4, Loss: 0.4736311267671131 75600/139232 train accuracy:0.7809523940086365
1329it [07:46,  2.85it/s]Epoch: 4, Loss: 0.4818037341889881 79800/139232 train accuracy:0.7807518839836121
1399it [08:11,  2.87it/s]Epoch: 4, Loss: 0.47140723818824404 84000/139232 train accuracy:0.781166672706604
1469it [08:35,  2.86it/s]Epoch: 4, Loss: 0.47816476004464287 88200/139232 train accuracy:0.7813491821289062
1539it [09:00,  2.86it/s]Epoch: 4, Loss: 0.46943091982886903 92400/139232 train accuracy:0.7815259695053101
1609it [09:24,  2.85it/s]Epoch: 4, Loss: 0.4533787609281994 96600/139232 train accuracy:0.7821635603904724
1679it [09:49,  2.86it/s]Epoch: 4, Loss: 0.45721229189918156 100800/139232 train accuracy:0.7826389074325562
1749it [10:13,  2.86it/s]Epoch: 4, Loss: 0.4663538178943452 105000/139232 train accuracy:0.782800018787384
1819it [10:38,  2.86it/s]Epoch: 4, Loss: 0.4633515857514881 109200/139232 train accuracy:0.7829853296279907
1889it [11:03,  2.86it/s]Epoch: 4, Loss: 0.4751736304873512 113400/139232 train accuracy:0.7826014161109924
1959it [11:27,  2.86it/s]Epoch: 4, Loss: 0.4640239025297619 117600/139232 train accuracy:0.782797634601593
2029it [11:52,  2.86it/s]Epoch: 4, Loss: 0.4663067917596726 121800/139232 train accuracy:0.7830131649971008
2099it [12:16,  2.86it/s]Epoch: 4, Loss: 0.4862706356956845 126000/139232 train accuracy:0.7830634713172913
2169it [12:40,  2.85it/s]Epoch: 4, Loss: 0.47437186104910717 130200/139232 train accuracy:0.7831797003746033
2239it [13:05,  2.86it/s]Epoch: 4, Loss: 0.47611976260230654 134400/139232 train accuracy:0.7831175327301025
2309it [13:29,  2.86it/s]Epoch: 4, Loss: 0.4560553850446429 138600/139232 train accuracy:0.7832467555999756
2321it [13:34,  2.85it/s]
Epoch: 4, epoch Loss: 66230.5390625  train accuracy:0.7831102013587952

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.20it/s]
Epoch: 4, Valid Loss: 6903.05908203125  valid accuracy:0.7967679500579834
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.35it/s]
Epoch: 4,   test accuracy:0.7974360585212708
37it [00:13,  2.86it/s]Epoch: 5, Loss: 0.2560265822637649 2280/139232 train accuracy:0.7763158082962036
69it [00:25,  2.85it/s]Epoch: 5, Loss: 0.21860151018415178 4200/139232 train accuracy:0.7757142782211304
139it [00:49,  2.84it/s]Epoch: 5, Loss: 0.4702628580729167 8400/139232 train accuracy:0.774404764175415
209it [01:14,  2.87it/s]Epoch: 5, Loss: 0.45871462867373514 12600/139232 train accuracy:0.7816666960716248
279it [01:38,  2.87it/s]Epoch: 5, Loss: 0.4653944614955357 16800/139232 train accuracy:0.7830356955528259
349it [02:03,  2.86it/s]Epoch: 5, Loss: 0.462547113327753 21000/139232 train accuracy:0.7850475907325745
419it [02:27,  2.84it/s]Epoch: 5, Loss: 0.47154700869605654 25200/139232 train accuracy:0.7859920859336853
489it [02:52,  2.85it/s]Epoch: 5, Loss: 0.4607638113839286 29400/139232 train accuracy:0.7861904501914978
559it [03:16,  2.86it/s]Epoch: 5, Loss: 0.44835289364769343 33600/139232 train accuracy:0.7876190543174744
629it [03:41,  2.86it/s]Epoch: 5, Loss: 0.466141851515997 37800/139232 train accuracy:0.7876719832420349
699it [04:05,  2.86it/s]Epoch: 5, Loss: 0.46343343098958334 42000/139232 train accuracy:0.7870000004768372
769it [04:30,  2.86it/s]Epoch: 5, Loss: 0.43948291596912203 46200/139232 train accuracy:0.78844153881073
839it [04:54,  2.84it/s]Epoch: 5, Loss: 0.4647012183779762 50400/139232 train accuracy:0.7885714173316956
909it [05:19,  2.86it/s]Epoch: 5, Loss: 0.45637564522879465 54600/139232 train accuracy:0.7890110015869141
979it [05:43,  2.85it/s]Epoch: 5, Loss: 0.47183146158854167 58800/139232 train accuracy:0.7889626026153564
1049it [06:08,  2.85it/s]Epoch: 5, Loss: 0.45831127348400297 63000/139232 train accuracy:0.7892380952835083
1119it [06:33,  2.85it/s]Epoch: 5, Loss: 0.45651910691034225 67200/139232 train accuracy:0.789657711982727
1189it [06:57,  2.86it/s]Epoch: 5, Loss: 0.4771377418154762 71400/139232 train accuracy:0.7894117832183838
1259it [07:22,  2.85it/s]Epoch: 5, Loss: 0.46522289457775295 75600/139232 train accuracy:0.7889682650566101
1329it [07:47,  2.85it/s]Epoch: 5, Loss: 0.46577598935081843 79800/139232 train accuracy:0.7884336113929749
1399it [08:11,  2.87it/s]Epoch: 5, Loss: 0.4697832961309524 84000/139232 train accuracy:0.7882024049758911
1469it [08:36,  2.87it/s]Epoch: 5, Loss: 0.47081932431175594 88200/139232 train accuracy:0.7876757383346558
1539it [09:00,  2.86it/s]Epoch: 5, Loss: 0.44948175339471724 92400/139232 train accuracy:0.787997841835022
1609it [09:25,  2.84it/s]Epoch: 5, Loss: 0.4551999918619792 96600/139232 train accuracy:0.7883850932121277
1679it [09:49,  2.85it/s]Epoch: 5, Loss: 0.45882850283668153 100800/139232 train accuracy:0.7883928418159485
1749it [10:14,  2.85it/s]Epoch: 5, Loss: 0.4564742024739583 105000/139232 train accuracy:0.7887333035469055
1819it [10:38,  2.85it/s]Epoch: 5, Loss: 0.45339619954427085 109200/139232 train accuracy:0.7890384793281555
1889it [11:03,  2.85it/s]Epoch: 5, Loss: 0.46018121628534225 113400/139232 train accuracy:0.7891005277633667
1959it [11:27,  2.87it/s]Epoch: 5, Loss: 0.4546248953683036 117600/139232 train accuracy:0.7893027067184448
2029it [11:52,  2.87it/s]Epoch: 5, Loss: 0.4589227004278274 121800/139232 train accuracy:0.7892774939537048
2099it [12:16,  2.85it/s]Epoch: 5, Loss: 0.47047052292596725 126000/139232 train accuracy:0.7889364957809448
2169it [12:41,  2.84it/s]Epoch: 5, Loss: 0.46112871442522324 130200/139232 train accuracy:0.7891013622283936
2239it [13:05,  2.85it/s]Epoch: 5, Loss: 0.46346441359747026 134400/139232 train accuracy:0.7889360189437866
2309it [13:30,  2.86it/s]Epoch: 5, Loss: 0.45673671177455355 138600/139232 train accuracy:0.7888960838317871
2321it [13:34,  2.85it/s]
Epoch: 5, epoch Loss: 64300.63671875  train accuracy:0.7888488173484802

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.21it/s]
Epoch: 5, Valid Loss: 6789.771484375  valid accuracy:0.7984486222267151
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:08<00:00,  8.37it/s]
Epoch: 5,   test accuracy:0.7996319532394409
37it [00:14,  2.82it/s]Epoch: 6, Loss: 0.24808721633184525 2280/139232 train accuracy:0.784649133682251
69it [00:25,  2.85it/s]Epoch: 6, Loss: 0.20313248407273066 4200/139232 train accuracy:0.7890475988388062
139it [00:49,  2.85it/s]Epoch: 6, Loss: 0.4472187732514881 8400/139232 train accuracy:0.7938095331192017
209it [01:14,  2.86it/s]Epoch: 6, Loss: 0.4457058570498512 12600/139232 train accuracy:0.7940475940704346
279it [01:38,  2.86it/s]Epoch: 6, Loss: 0.454713134765625 16800/139232 train accuracy:0.7936905026435852
349it [02:03,  2.84it/s]Epoch: 6, Loss: 0.46714457194010417 21000/139232 train accuracy:0.7921428680419922
419it [02:28,  2.85it/s]Epoch: 6, Loss: 0.45741597493489583 25200/139232 train accuracy:0.7916269898414612
489it [02:52,  2.86it/s]Epoch: 6, Loss: 0.4562711588541667 29400/139232 train accuracy:0.7907482981681824
559it [03:17,  2.86it/s]Epoch: 6, Loss: 0.4648644438244048 33600/139232 train accuracy:0.789642870426178,  2.85it/s545it [03:12,  2.86it/s]
629it [03:41,  2.86it/s]Epoch: 6, Loss: 0.46596249534970235 37800/139232 train accuracy:0.7894444465637207
699it [04:06,  2.85it/s]Epoch: 6, Loss: 0.43980800083705357 42000/139232 train accuracy:0.789976179599762
769it [04:30,  2.85it/s]Epoch: 6, Loss: 0.45287205287388393 46200/139232 train accuracy:0.7903463244438171
839it [04:55,  2.85it/s]Epoch: 6, Loss: 0.4492223830450149 50400/139232 train accuracy:0.79067462682724
909it [05:19,  2.86it/s]Epoch: 6, Loss: 0.4525888497488839 54600/139232 train accuracy:0.7910805940628052
979it [05:44,  2.85it/s]Epoch: 6, Loss: 0.45308477492559524 58800/139232 train accuracy:0.7910714149475098
1049it [06:08,  2.86it/s]Epoch: 6, Loss: 0.4395115734281994 63000/139232 train accuracy:0.7915555834770203
1119it [06:33,  2.84it/s]Epoch: 6, Loss: 0.4421945335751488 67200/139232 train accuracy:0.791994035243988
1189it [06:57,  2.86it/s]Epoch: 6, Loss: 0.45793369838169645 71400/139232 train accuracy:0.7920168042182922
1259it [07:22,  2.85it/s]Epoch: 6, Loss: 0.45478111630394347 75600/139232 train accuracy:0.7918915152549744
1329it [07:47,  2.85it/s]Epoch: 6, Loss: 0.4648618570963542 79800/139232 train accuracy:0.7915664315223694
1399it [08:11,  2.85it/s]Epoch: 6, Loss: 0.43817161923363096 84000/139232 train accuracy:0.7921071648597717
1469it [08:36,  2.86it/s]Epoch: 6, Loss: 0.44784766787574404 88200/139232 train accuracy:0.7924829721450806
1539it [09:01,  2.85it/s]Epoch: 6, Loss: 0.46175455729166665 92400/139232 train accuracy:0.7923917770385742
1609it [09:25,  2.85it/s]Epoch: 6, Loss: 0.4424645414806548 96600/139232 train accuracy:0.7928674817085266
1679it [09:50,  2.85it/s]Epoch: 6, Loss: 0.44742876325334824 100800/139232 train accuracy:0.7931746244430542
1749it [10:14,  2.86it/s]Epoch: 6, Loss: 0.4557746814546131 105000/139232 train accuracy:0.792980968952179
1819it [10:39,  2.85it/s]Epoch: 6, Loss: 0.43883428664434526 109200/139232 train accuracy:0.7933791279792786
1889it [11:03,  2.85it/s]Epoch: 6, Loss: 0.45166341145833333 113400/139232 train accuracy:0.79322749376297
1959it [11:28,  2.84it/s]Epoch: 6, Loss: 0.4380100795200893 117600/139232 train accuracy:0.7934523820877075
2029it [11:53,  2.85it/s]Epoch: 6, Loss: 0.4471054222470238 121800/139232 train accuracy:0.7937192320823669
2099it [12:17,  2.84it/s]Epoch: 6, Loss: 0.4470130557105655 126000/139232 train accuracy:0.7938412427902222
2169it [12:42,  2.85it/s]Epoch: 6, Loss: 0.4516562034970238 130200/139232 train accuracy:0.793832540512085
2239it [13:06,  2.85it/s]Epoch: 6, Loss: 0.4427864292689732 134400/139232 train accuracy:0.7938244342803955
2309it [13:31,  2.85it/s]Epoch: 6, Loss: 0.4451147170293899 138600/139232 train accuracy:0.7937878966331482
2321it [13:35,  2.85it/s]
Epoch: 6, epoch Loss: 62760.25  train accuracy:0.7938045859336853

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.19it/s]
Epoch: 6, Valid Loss: 6756.19580078125  valid accuracy:0.7985132336616516
find better loss
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.32it/s]
Epoch: 6,   test accuracy:0.7986422777175903
37it [00:14,  2.87it/s]Epoch: 7, Loss: 0.26195533389136905 2280/139232 train accuracy:0.7618421316146851
69it [00:25,  2.86it/s]Epoch: 7, Loss: 0.20697858537946429 4200/139232 train accuracy:0.7730952501296997
139it [00:49,  2.82it/s]Epoch: 7, Loss: 0.44418692452566966 8400/139232 train accuracy:0.7829762101173401
209it [01:14,  2.83it/s]Epoch: 7, Loss: 0.4368481154668899 12600/139232 train accuracy:0.7888095378875732
279it [01:39,  2.86it/s]Epoch: 7, Loss: 0.4420596168154762 16800/139232 train accuracy:0.7901785969734192
349it [02:03,  2.82it/s]Epoch: 7, Loss: 0.4448557826450893 21000/139232 train accuracy:0.7915714383125305
419it [02:28,  2.83it/s]Epoch: 7, Loss: 0.4432826450892857 25200/139232 train accuracy:0.7920634746551514
489it [02:53,  2.82it/s]Epoch: 7, Loss: 0.4430232456752232 29400/139232 train accuracy:0.7924149632453918
559it [03:18,  2.82it/s]Epoch: 7, Loss: 0.45128917875744046 33600/139232 train accuracy:0.7928571701049805
629it [03:43,  2.83it/s]Epoch: 7, Loss: 0.460125732421875 37800/139232 train accuracy:0.7918518781661987
699it [04:07,  2.86it/s]Epoch: 7, Loss: 0.4587856619698661 42000/139232 train accuracy:0.7912380695343018
769it [04:32,  2.86it/s]Epoch: 7, Loss: 0.4199634660993304 46200/139232 train accuracy:0.7925541400909424
839it [04:56,  2.86it/s]Epoch: 7, Loss: 0.4306782749720982 50400/139232 train accuracy:0.7931150794029236
909it [05:21,  2.85it/s]Epoch: 7, Loss: 0.4439503406343006 54600/139232 train accuracy:0.7934615612030029
979it [05:45,  2.85it/s]Epoch: 7, Loss: 0.4354007684616815 58800/139232 train accuracy:0.7939285635948181
1049it [06:10,  2.81it/s]Epoch: 7, Loss: 0.4406700497581845 63000/139232 train accuracy:0.7940793633460999
1119it [06:35,  2.86it/s]Epoch: 7, Loss: 0.43637701125372025 67200/139232 train accuracy:0.7945833206176758
1189it [07:00,  2.77it/s]Epoch: 7, Loss: 0.43648687453497026 71400/139232 train accuracy:0.7951260209083557
1259it [07:25,  2.80it/s]Epoch: 7, Loss: 0.4456867908296131 75600/139232 train accuracy:0.7955819964408875
1329it [07:49,  2.86it/s]Epoch: 7, Loss: 0.4352253941127232 79800/139232 train accuracy:0.7958019971847534
1399it [08:14,  2.82it/s]Epoch: 7, Loss: 0.4380568440755208 84000/139232 train accuracy:0.7961547374725342
1469it [08:39,  2.84it/s]Epoch: 7, Loss: 0.4445129685174851 88200/139232 train accuracy:0.7964398860931396
1539it [09:03,  2.86it/s]Epoch: 7, Loss: 0.44357750302269344 92400/139232 train accuracy:0.7965475916862488
1609it [09:28,  2.82it/s]Epoch: 7, Loss: 0.438355712890625 96600/139232 train accuracy:0.7966769933700562
1679it [09:53,  2.86it/s]Epoch: 7, Loss: 0.45098440987723215 100800/139232 train accuracy:0.7963888645172119
1749it [10:18,  2.79it/s]Epoch: 7, Loss: 0.43835670107886904 105000/139232 train accuracy:0.7964666485786438
1819it [10:43,  2.80it/s]Epoch: 7, Loss: 0.4544413539341518 109200/139232 train accuracy:0.7962087988853455
1889it [11:08,  2.82it/s]Epoch: 7, Loss: 0.43025797526041665 113400/139232 train accuracy:0.7965696454048157
1959it [11:32,  2.86it/s]Epoch: 7, Loss: 0.4397120593843006 117600/139232 train accuracy:0.796700656414032
2029it [11:57,  2.85it/s]Epoch: 7, Loss: 0.4496934581938244 121800/139232 train accuracy:0.7966420650482178
2099it [12:22,  2.83it/s]Epoch: 7, Loss: 0.44178859165736606 126000/139232 train accuracy:0.7965872883796692
2169it [12:46,  2.87it/s]Epoch: 7, Loss: 0.4439061046781994 130200/139232 train accuracy:0.7967434525489807
2239it [13:11,  2.85it/s]Epoch: 7, Loss: 0.4517554873511905 134400/139232 train accuracy:0.7965327501296997
2309it [13:35,  2.85it/s]Epoch: 7, Loss: 0.4371761648995536 138600/139232 train accuracy:0.7966955304145813
2321it [13:39,  2.83it/s]
Epoch: 7, epoch Loss: 61687.09375  train accuracy:0.796720564365387

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.14it/s]
Epoch: 7, Valid Loss: 6713.4521484375  valid accuracy:0.8001939058303833
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:10<00:00,  8.28it/s]
Epoch: 7,   test accuracy:0.8009618520736694
37it [00:14,  2.82it/s]Epoch: 8, Loss: 0.23294823056175595 2280/139232 train accuracy:0.8118420839309692
69it [00:25,  2.83it/s]Epoch: 8, Loss: 0.1998974900018601 4200/139232 train accuracy:0.8030952215194702
139it [00:50,  2.84it/s]Epoch: 8, Loss: 0.4244333321707589 8400/139232 train accuracy:0.8029761910438538
209it [01:14,  2.86it/s]Epoch: 8, Loss: 0.44392630440848213 12600/139232 train accuracy:0.7994444370269775
279it [01:39,  2.86it/s]Epoch: 8, Loss: 0.4358635021391369 16800/139232 train accuracy:0.7980952262878418
349it [02:03,  2.85it/s]Epoch: 8, Loss: 0.45169878278459824 21000/139232 train accuracy:0.7973809242248535
419it [02:28,  2.84it/s]Epoch: 8, Loss: 0.424815673828125 25200/139232 train accuracy:0.7986111044883728
489it [02:52,  2.86it/s]Epoch: 8, Loss: 0.4439991978236607 29400/139232 train accuracy:0.7987074851989746
559it [03:17,  2.83it/s]Epoch: 8, Loss: 0.4472181047712054 33600/139232 train accuracy:0.798035740852356
629it [03:41,  2.85it/s]Epoch: 8, Loss: 0.4321424211774554 37800/139232 train accuracy:0.7991005182266235
699it [04:06,  2.81it/s]Epoch: 8, Loss: 0.4531098865327381 42000/139232 train accuracy:0.7979999780654907
769it [04:31,  2.87it/s]Epoch: 8, Loss: 0.4329802594866071 46200/139232 train accuracy:0.7986796498298645
839it [04:55,  2.85it/s]Epoch: 8, Loss: 0.4430333600725446 50400/139232 train accuracy:0.7983730435371399
909it [05:20,  2.85it/s]Epoch: 8, Loss: 0.42101739792596726 54600/139232 train accuracy:0.7991209030151367
979it [05:45,  2.83it/s]Epoch: 8, Loss: 0.445669671921503 58800/139232 train accuracy:0.7991666793823242
1049it [06:09,  2.82it/s]Epoch: 8, Loss: 0.4523920259021577 63000/139232 train accuracy:0.7979999780654907
1119it [06:34,  2.87it/s]Epoch: 8, Loss: 0.43129827590215775 67200/139232 train accuracy:0.7981845140457153
1189it [06:59,  2.83it/s]Epoch: 8, Loss: 0.44156514485677084 71400/139232 train accuracy:0.7978151440620422
1259it [07:24,  2.84it/s]Epoch: 8, Loss: 0.4305743698846726 75600/139232 train accuracy:0.798042356967926
1329it [07:48,  2.85it/s]Epoch: 8, Loss: 0.4249732317243304 79800/139232 train accuracy:0.7986591458320618
1399it [08:13,  2.85it/s]Epoch: 8, Loss: 0.4504123360770089 84000/139232 train accuracy:0.7987856864929199
1469it [08:37,  2.84it/s]Epoch: 8, Loss: 0.42923334030877974 88200/139232 train accuracy:0.7991949915885925
1539it [09:02,  2.85it/s]Epoch: 8, Loss: 0.44055027553013393 92400/139232 train accuracy:0.7992208003997803
1609it [09:27,  2.84it/s]Epoch: 8, Loss: 0.4275984409877232 96600/139232 train accuracy:0.7995962500572205
1679it [09:51,  2.84it/s]Epoch: 8, Loss: 0.43432843889508926 100800/139232 train accuracy:0.7996527552604675
1749it [10:16,  2.85it/s]Epoch: 8, Loss: 0.4310493396577381 105000/139232 train accuracy:0.7998095154762268
1819it [10:40,  2.85it/s]Epoch: 8, Loss: 0.4319650123232887 109200/139232 train accuracy:0.7996886372566223
1889it [11:05,  2.85it/s]Epoch: 8, Loss: 0.4507386416480655 113400/139232 train accuracy:0.7995766997337341
1959it [11:30,  2.85it/s]Epoch: 8, Loss: 0.43228620256696426 117600/139232 train accuracy:0.7997108697891235
2029it [11:54,  2.85it/s]Epoch: 8, Loss: 0.42464335123697916 121800/139232 train accuracy:0.7999753952026367
2099it [12:19,  2.87it/s]Epoch: 8, Loss: 0.4279869442894345 126000/139232 train accuracy:0.8000397086143494
2169it [12:43,  2.86it/s]Epoch: 8, Loss: 0.43403369721912205 130200/139232 train accuracy:0.8000307083129883
2239it [13:08,  2.84it/s]Epoch: 8, Loss: 0.4246365501767113 134400/139232 train accuracy:0.800178587436676
2309it [13:32,  2.85it/s]Epoch: 8, Loss: 0.4279513404482887 138600/139232 train accuracy:0.8002741932868958
2321it [13:37,  2.84it/s]
Epoch: 8, epoch Loss: 60653.60546875  train accuracy:0.8004050850868225

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.16it/s]
Epoch: 8, Valid Loss: 6668.54248046875  valid accuracy:0.8009696006774902
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.30it/s]
Epoch: 8,   test accuracy:0.8022453784942627
37it [00:13,  2.86it/s]Epoch: 9, Loss: 0.2395135498046875 2280/139232 train accuracy:0.8026315569877625
69it [00:24,  2.85it/s]Epoch: 9, Loss: 0.19192843482607888 4200/139232 train accuracy:0.8080952167510986
139it [00:49,  2.85it/s]Epoch: 9, Loss: 0.4360401843843006 8400/139232 train accuracy:0.8001190423965454
209it [01:13,  2.85it/s]Epoch: 9, Loss: 0.4291352771577381 12600/139232 train accuracy:0.8029364943504333
279it [01:38,  2.86it/s]Epoch: 9, Loss: 0.41035714285714286 16800/139232 train accuracy:0.8069642782211304
349it [02:03,  2.85it/s]Epoch: 9, Loss: 0.4232279459635417 21000/139232 train accuracy:0.8080952167510986
419it [02:27,  2.85it/s]Epoch: 9, Loss: 0.42612281436011906 25200/139232 train accuracy:0.807619035243988
489it [02:52,  2.86it/s]Epoch: 9, Loss: 0.44108584449404764 29400/139232 train accuracy:0.8059864044189453
559it [03:16,  2.85it/s]Epoch: 9, Loss: 0.42558035714285714 33600/139232 train accuracy:0.8053273558616638
629it [03:41,  2.85it/s]Epoch: 9, Loss: 0.4166054861886161 37800/139232 train accuracy:0.80693119764328
699it [04:06,  2.85it/s]Epoch: 9, Loss: 0.42210056849888394 42000/139232 train accuracy:0.8065237998962402
769it [04:30,  2.85it/s]Epoch: 9, Loss: 0.4128645833333333 46200/139232 train accuracy:0.8080303072929382
839it [04:55,  2.86it/s]Epoch: 9, Loss: 0.42180980864025297 50400/139232 train accuracy:0.807956337928772
909it [05:19,  2.85it/s]Epoch: 9, Loss: 0.41927283877418153 54600/139232 train accuracy:0.8084432482719421
979it [05:44,  2.86it/s]Epoch: 9, Loss: 0.438213123139881 58800/139232 train accuracy:0.8074659705162048
1049it [06:09,  2.87it/s]Epoch: 9, Loss: 0.4345742652529762 63000/139232 train accuracy:0.8070952296257019
1119it [06:33,  2.85it/s]Epoch: 9, Loss: 0.43000331333705355 67200/139232 train accuracy:0.8067708611488342
1189it [06:58,  2.84it/s]Epoch: 9, Loss: 0.436303216843378 71400/139232 train accuracy:0.8066806793212891
1259it [07:22,  2.85it/s]Epoch: 9, Loss: 0.4481499953497024 75600/139232 train accuracy:0.8059391379356384
1329it [07:47,  2.85it/s]Epoch: 9, Loss: 0.430443609328497 79800/139232 train accuracy:0.8058145642280579
1399it [08:11,  2.86it/s]Epoch: 9, Loss: 0.4398014904203869 84000/139232 train accuracy:0.805476188659668
1469it [08:36,  2.86it/s]Epoch: 9, Loss: 0.4182391357421875 88200/139232 train accuracy:0.8057823181152344
1539it [09:01,  2.84it/s]Epoch: 9, Loss: 0.44662679036458336 92400/139232 train accuracy:0.8052705526351929
1609it [09:25,  2.85it/s]Epoch: 9, Loss: 0.4417167154947917 96600/139232 train accuracy:0.8051035404205322
1679it [09:50,  2.85it/s]Epoch: 9, Loss: 0.42719354538690474 100800/139232 train accuracy:0.8053174614906311
1749it [10:14,  2.85it/s]Epoch: 9, Loss: 0.4359824044363839 105000/139232 train accuracy:0.805304765701294
1819it [10:39,  2.85it/s]Epoch: 9, Loss: 0.42391267322358633 109200/139232 train accuracy:0.8054670095443726
1889it [11:04,  2.86it/s]Epoch: 9, Loss: 0.4339980933779762 113400/139232 train accuracy:0.8053968548774719
1959it [11:28,  2.86it/s]Epoch: 9, Loss: 0.43669061569940476 117600/139232 train accuracy:0.8053061366081238
2029it [11:53,  2.84it/s]Epoch: 9, Loss: 0.43767246791294645 121800/139232 train accuracy:0.8049097061157227
2099it [12:17,  2.85it/s]Epoch: 9, Loss: 0.4265602039155506 126000/139232 train accuracy:0.8046666383743286
2169it [12:42,  2.85it/s]Epoch: 9, Loss: 0.42526358468191966 130200/139232 train accuracy:0.8047465682029724
2239it [13:07,  2.84it/s]Epoch: 9, Loss: 0.418379400344122 134400/139232 train accuracy:0.8049776554107666
2309it [13:31,  2.84it/s]Epoch: 9, Loss: 0.434688720703125 138600/139232 train accuracy:0.8048484921455383
2321it [13:35,  2.84it/s]
Epoch: 9, epoch Loss: 59853.48046875  train accuracy:0.8047359585762024

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.18it/s]
Epoch: 9, Valid Loss: 6668.1083984375  valid accuracy:0.8025209903717041
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.34it/s]
Epoch: 9,   test accuracy:0.8028175830841064
37it [00:13,  2.85it/s]Epoch: 10, Loss: 0.24245647612072171 2280/139232 train accuracy:0.792543888092041
69it [00:24,  2.86it/s]Epoch: 10, Loss: 0.18847606840587797 4200/139232 train accuracy:0.8028571605682373
139it [00:49,  2.83it/s]Epoch: 10, Loss: 0.4303775460379464 8400/139232 train accuracy:0.8030952215194702
209it [01:13,  2.85it/s]Epoch: 10, Loss: 0.4206966727120536 12600/139232 train accuracy:0.8043650984764099
279it [01:38,  2.86it/s]Epoch: 10, Loss: 0.4370418294270833 16800/139232 train accuracy:0.8033928275108337
349it [02:02,  2.86it/s]Epoch: 10, Loss: 0.43401236397879467 21000/139232 train accuracy:0.8014761805534363
419it [02:27,  2.86it/s]Epoch: 10, Loss: 0.41897635323660715 25200/139232 train accuracy:0.8032143115997314
489it [02:52,  2.84it/s]Epoch: 10, Loss: 0.41191627139136905 29400/139232 train accuracy:0.8039795756340027
559it [03:16,  2.84it/s]Epoch: 10, Loss: 0.42129603794642856 33600/139232 train accuracy:0.805059552192688
629it [03:41,  2.85it/s]Epoch: 10, Loss: 0.4212731642950149 37800/139232 train accuracy:0.8052380681037903
699it [04:06,  2.81it/s]Epoch: 10, Loss: 0.4167335437593006 42000/139232 train accuracy:0.8066904544830322
769it [04:30,  2.85it/s]Epoch: 10, Loss: 0.4276095435732887 46200/139232 train accuracy:0.806774914264679
839it [04:55,  2.85it/s]Epoch: 10, Loss: 0.4174655006045387 50400/139232 train accuracy:0.807103157043457
909it [05:19,  2.85it/s]Epoch: 10, Loss: 0.42703633626302084 54600/139232 train accuracy:0.8069413900375366
979it [05:44,  2.86it/s]Epoch: 10, Loss: 0.415579601469494 58800/139232 train accuracy:0.8075169920921326
1049it [06:09,  2.85it/s]Epoch: 10, Loss: 0.4232988048735119 63000/139232 train accuracy:0.8071746230125427
1119it [06:33,  2.86it/s]Epoch: 10, Loss: 0.43799653552827383 67200/139232 train accuracy:0.8066964149475098
1189it [06:58,  2.86it/s]Epoch: 10, Loss: 0.4130708821614583 71400/139232 train accuracy:0.8067787289619446
1259it [07:22,  2.85it/s]Epoch: 10, Loss: 0.41014241536458335 75600/139232 train accuracy:0.8070105910301208
1329it [07:47,  2.86it/s]Epoch: 10, Loss: 0.432639886765253 79800/139232 train accuracy:0.8068045377731323
1399it [08:11,  2.85it/s]Epoch: 10, Loss: 0.4216344924200149 84000/139232 train accuracy:0.8069166541099548
1469it [08:36,  2.85it/s]Epoch: 10, Loss: 0.4368292526971726 88200/139232 train accuracy:0.8066666722297668
1539it [09:00,  2.86it/s]Epoch: 10, Loss: 0.41828287760416666 92400/139232 train accuracy:0.8067315816879272
1609it [09:25,  2.85it/s]Epoch: 10, Loss: 0.423523443312872 96600/139232 train accuracy:0.806418240070343
1679it [09:50,  2.84it/s]Epoch: 10, Loss: 0.4328838820684524 100800/139232 train accuracy:0.8064782023429871
1749it [10:14,  2.85it/s]Epoch: 10, Loss: 0.4371941266741071 105000/139232 train accuracy:0.8063619136810303
1819it [10:39,  2.85it/s]Epoch: 10, Loss: 0.42630452473958336 109200/139232 train accuracy:0.8063827753067017
1889it [11:04,  2.86it/s]Epoch: 10, Loss: 0.42069585890997024 113400/139232 train accuracy:0.8063580393791199
1959it [11:28,  2.85it/s]Epoch: 10, Loss: 0.42537821451822916 117600/139232 train accuracy:0.8063690662384033
2029it [11:53,  2.84it/s]Epoch: 10, Loss: 0.41215099516369047 121800/139232 train accuracy:0.8068719506263733
2099it [12:17,  2.86it/s]Epoch: 10, Loss: 0.4094454810732887 126000/139232 train accuracy:0.8071110844612122
2169it [12:42,  2.85it/s]Epoch: 10, Loss: 0.434278564453125 130200/139232 train accuracy:0.8069047331809998
2239it [13:06,  2.85it/s]Epoch: 10, Loss: 0.41158854166666664 134400/139232 train accuracy:0.8070907592773438
2309it [13:31,  2.86it/s]Epoch: 10, Loss: 0.4143954031808036 138600/139232 train accuracy:0.8072077631950378
2321it [13:35,  2.85it/s]
Epoch: 10, epoch Loss: 58994.359375  train accuracy:0.8070774078369141

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.19it/s]
Epoch: 10, Valid Loss: 6622.05810546875  valid accuracy:0.8032320737838745
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.33it/s]
Epoch: 10,   test accuracy:0.8031268119812012
37it [00:13,  2.84it/s]Epoch: 11, Loss: 0.23011343819754465 2280/139232 train accuracy:0.803947389125824
69it [00:24,  2.86it/s]Epoch: 11, Loss: 0.1807816714332217 4200/139232 train accuracy:0.8138095140457153
139it [00:49,  2.86it/s]Epoch: 11, Loss: 0.44151178269159225 8400/139232 train accuracy:0.8058333396911621
209it [01:13,  2.86it/s]Epoch: 11, Loss: 0.4219866943359375 12600/139232 train accuracy:0.8073809742927551
279it [01:38,  2.85it/s]Epoch: 11, Loss: 0.42885381789434524 16800/139232 train accuracy:0.805654764175415
349it [02:03,  2.84it/s]Epoch: 11, Loss: 0.42224708193824406 21000/139232 train accuracy:0.8069999814033508
419it [02:27,  2.85it/s]Epoch: 11, Loss: 0.42587931315104166 25200/139232 train accuracy:0.8071428537368774
489it [02:52,  2.86it/s]Epoch: 11, Loss: 0.4208666701543899 29400/139232 train accuracy:0.8071428537368774
559it [03:16,  2.85it/s]Epoch: 11, Loss: 0.40440679640997024 33600/139232 train accuracy:0.808779776096344
629it [03:41,  2.84it/s]Epoch: 11, Loss: 0.42078218005952384 37800/139232 train accuracy:0.8085978627204895
699it [04:05,  2.85it/s]Epoch: 11, Loss: 0.42779915945870534 42000/139232 train accuracy:0.8078571557998657
769it [04:30,  2.86it/s]Epoch: 11, Loss: 0.42567036946614584 46200/139232 train accuracy:0.8069480657577515
839it [04:55,  2.85it/s]Epoch: 11, Loss: 0.4139696103050595 50400/139232 train accuracy:0.8077976107597351
909it [05:19,  2.84it/s]Epoch: 11, Loss: 0.4275349353608631 54600/139232 train accuracy:0.8074725270271301
979it [05:44,  2.85it/s]Epoch: 11, Loss: 0.4264839390345982 58800/139232 train accuracy:0.8075340390205383
1049it [06:09,  2.86it/s]Epoch: 11, Loss: 0.4148080880301339 63000/139232 train accuracy:0.8082539439201355
1119it [06:33,  2.84it/s]Epoch: 11, Loss: 0.41311055501302085 67200/139232 train accuracy:0.8088392615318298
1189it [06:58,  2.86it/s]Epoch: 11, Loss: 0.4080719284784226 71400/139232 train accuracy:0.8092716932296753
1259it [07:22,  2.85it/s]Epoch: 11, Loss: 0.418653564453125 75600/139232 train accuracy:0.8094311952590942
1329it [07:47,  2.86it/s]Epoch: 11, Loss: 0.42684215727306546 79800/139232 train accuracy:0.809248149394989
1399it [08:11,  2.85it/s]Epoch: 11, Loss: 0.4039875720796131 84000/139232 train accuracy:0.8095238208770752
1469it [08:36,  2.84it/s]Epoch: 11, Loss: 0.399239996047247 88200/139232 train accuracy:0.8101927638053894
1539it [09:01,  2.87it/s]Epoch: 11, Loss: 0.4105818103608631 92400/139232 train accuracy:0.8102813959121704
1609it [09:25,  2.86it/s]Epoch: 11, Loss: 0.40696713402157736 96600/139232 train accuracy:0.8107246160507202
1679it [09:50,  2.86it/s]Epoch: 11, Loss: 0.423952142624628 100800/139232 train accuracy:0.8108928799629211
1749it [10:14,  2.85it/s]Epoch: 11, Loss: 0.4189285132998512 105000/139232 train accuracy:0.8108095526695251
1819it [10:39,  2.85it/s]Epoch: 11, Loss: 0.410401611328125 109200/139232 train accuracy:0.8110988736152649
1889it [11:04,  2.83it/s]Epoch: 11, Loss: 0.39963936941964284 113400/139232 train accuracy:0.8114109635353088
1959it [11:28,  2.85it/s]Epoch: 11, Loss: 0.4107641020275298 117600/139232 train accuracy:0.8113775253295898
2029it [11:53,  2.86it/s]Epoch: 11, Loss: 0.4110257103329613 121800/139232 train accuracy:0.8115024566650391
2099it [12:18,  2.86it/s]Epoch: 11, Loss: 0.4175899832589286 126000/139232 train accuracy:0.8112618923187256
2169it [12:42,  2.85it/s]Epoch: 11, Loss: 0.4197208077566964 130200/139232 train accuracy:0.8112749457359314
2239it [13:07,  2.84it/s]Epoch: 11, Loss: 0.4136719912574405 134400/139232 train accuracy:0.8112351298332214
2309it [13:31,  2.86it/s]Epoch: 11, Loss: 0.41575982956659224 138600/139232 train accuracy:0.8111544251441956
2321it [13:35,  2.84it/s]
Epoch: 11, epoch Loss: 58061.9140625  train accuracy:0.8111640810966492

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.18it/s]
Epoch: 11, Valid Loss: 6612.47802734375  valid accuracy:0.8005171418190002
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.34it/s]
Epoch: 11,   test accuracy:0.8022763133049011
37it [00:13,  2.86it/s]Epoch: 12, Loss: 0.21795731317429315 2280/139232 train accuracy:0.8232455849647522
69it [00:24,  2.85it/s]Epoch: 12, Loss: 0.18392598470052082 4200/139232 train accuracy:0.8199999928474426
139it [00:49,  2.85it/s]Epoch: 12, Loss: 0.41814081101190476 8400/139232 train accuracy:0.8166666626930237
209it [01:13,  2.86it/s]Epoch: 12, Loss: 0.4020428466796875 12600/139232 train accuracy:0.8152381181716919
279it [01:38,  2.85it/s]Epoch: 12, Loss: 0.40789225260416667 16800/139232 train accuracy:0.8152381181716919
349it [02:02,  2.85it/s]Epoch: 12, Loss: 0.40500418526785714 21000/139232 train accuracy:0.8148571252822876
419it [02:27,  2.87it/s]Epoch: 12, Loss: 0.41653753371465774 25200/139232 train accuracy:0.814047634601593
489it [02:52,  2.85it/s]Epoch: 12, Loss: 0.3963709949311756 29400/139232 train accuracy:0.8153061270713806
559it [03:16,  2.85it/s]Epoch: 12, Loss: 0.4125784447079613 33600/139232 train accuracy:0.815416693687439
629it [03:41,  2.85it/s]Epoch: 12, Loss: 0.41897106352306546 37800/139232 train accuracy:0.8148412704467773
699it [04:05,  2.86it/s]Epoch: 12, Loss: 0.4038679431733631 42000/139232 train accuracy:0.8151666522026062
769it [04:30,  2.85it/s]Epoch: 12, Loss: 0.40755469912574405 46200/139232 train accuracy:0.8151731491088867
839it [04:55,  2.84it/s]Epoch: 12, Loss: 0.4061497279575893 50400/139232 train accuracy:0.8153968453407288
909it [05:19,  2.86it/s]Epoch: 12, Loss: 0.4101591273716518 54600/139232 train accuracy:0.815201461315155
979it [05:44,  2.86it/s]Epoch: 12, Loss: 0.4102919224330357 58800/139232 train accuracy:0.8151190280914307
1049it [06:08,  2.85it/s]Epoch: 12, Loss: 0.4104010881696429 63000/139232 train accuracy:0.8151745796203613
1119it [06:33,  2.86it/s]Epoch: 12, Loss: 0.41614281063988096 67200/139232 train accuracy:0.8151339292526245
1189it [06:58,  2.85it/s]Epoch: 12, Loss: 0.4169547816685268 71400/139232 train accuracy:0.8149439692497253
1259it [07:22,  2.84it/s]Epoch: 12, Loss: 0.40441632952008927 75600/139232 train accuracy:0.8148677349090576
1329it [07:47,  2.85it/s]Epoch: 12, Loss: 0.41369774228050593 79800/139232 train accuracy:0.8145363330841064
1399it [08:11,  2.85it/s]Epoch: 12, Loss: 0.4218028913225446 84000/139232 train accuracy:0.8143095374107361
1469it [08:36,  2.83it/s]Epoch: 12, Loss: 0.405411376953125 88200/139232 train accuracy:0.8146031498908997
1539it [09:01,  2.85it/s]Epoch: 12, Loss: 0.4168620954241071 92400/139232 train accuracy:0.8144155740737915
1609it [09:25,  2.84it/s]Epoch: 12, Loss: 0.41059398832775296 96600/139232 train accuracy:0.8143374919891357
1679it [09:50,  2.84it/s]Epoch: 12, Loss: 0.4145941162109375 100800/139232 train accuracy:0.8142956495285034
1749it [10:14,  2.85it/s]Epoch: 12, Loss: 0.41714134579613094 105000/139232 train accuracy:0.8142666816711426
1819it [10:39,  2.85it/s]Epoch: 12, Loss: 0.412525634765625 109200/139232 train accuracy:0.8142856955528259
1889it [11:04,  2.29it/s]Epoch: 12, Loss: 0.4233627464657738 113400/139232 train accuracy:0.8140652775764465
1959it [11:29,  2.85it/s]Epoch: 12, Loss: 0.40739438011532736 117600/139232 train accuracy:0.814081609249115
2029it [11:53,  2.85it/s]Epoch: 12, Loss: 0.40448000953311014 121800/139232 train accuracy:0.813908040523529
2099it [12:18,  2.86it/s]Epoch: 12, Loss: 0.4151226515997024 126000/139232 train accuracy:0.8140397071838379
2169it [12:42,  2.85it/s]Epoch: 12, Loss: 0.41210675920758927 130200/139232 train accuracy:0.8139324188232422
2239it [13:07,  2.86it/s]Epoch: 12, Loss: 0.3982904343377976 134400/139232 train accuracy:0.8142931461334229
2309it [13:31,  2.85it/s]Epoch: 12, Loss: 0.4140332321893601 138600/139232 train accuracy:0.8141558170318604
2321it [13:35,  2.84it/s]
Epoch: 12, epoch Loss: 57183.53125  train accuracy:0.8141088485717773

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.20it/s]
Epoch: 12, Valid Loss: 6605.4765625  valid accuracy:0.8025209903717041
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.33it/s]
Epoch: 12,   test accuracy:0.8043020963668823
37it [00:13,  2.86it/s]Epoch: 13, Loss: 0.22286403111049108 2280/139232 train accuracy:0.821052610874176
69it [00:24,  2.85it/s]Epoch: 13, Loss: 0.18229008265904018 4200/139232 train accuracy:0.822857141494751
139it [00:49,  2.82it/s]Epoch: 13, Loss: 0.3995695568266369 8400/139232 train accuracy:0.8235714435577393
209it [01:14,  2.82it/s]Epoch: 13, Loss: 0.4060176304408482 12600/139232 train accuracy:0.8223015666007996
279it [01:38,  2.85it/s]Epoch: 13, Loss: 0.403736078171503 16800/139232 train accuracy:0.8197023868560791
349it [02:03,  2.84it/s]Epoch: 13, Loss: 0.4090342203776042 21000/139232 train accuracy:0.8195714354515076
419it [02:28,  2.84it/s]Epoch: 13, Loss: 0.4021771530877976 25200/139232 train accuracy:0.8196428418159485
489it [02:52,  2.85it/s]Epoch: 13, Loss: 0.40849981398809526 29400/139232 train accuracy:0.8196938633918762
559it [03:17,  2.84it/s]Epoch: 13, Loss: 0.4063479759579613 33600/139232 train accuracy:0.8189583420753479
629it [03:42,  2.83it/s]Epoch: 13, Loss: 0.40449393136160716 37800/139232 train accuracy:0.8190211653709412
699it [04:06,  2.84it/s]Epoch: 13, Loss: 0.40714599609375 42000/139232 train accuracy:0.8184999823570251
769it [04:31,  2.85it/s]Epoch: 13, Loss: 0.38605317615327384 46200/139232 train accuracy:0.8195021748542786
839it [04:56,  2.86it/s]Epoch: 13, Loss: 0.41674793061755955 50400/139232 train accuracy:0.8185714483261108
909it [05:20,  2.87it/s]Epoch: 13, Loss: 0.42262169247581843 54600/139232 train accuracy:0.8176190257072449
979it [05:45,  2.86it/s]Epoch: 13, Loss: 0.4022912888299851 58800/139232 train accuracy:0.8176700472831726
1049it [06:09,  2.84it/s]Epoch: 13, Loss: 0.40452767508370535 63000/139232 train accuracy:0.8177936673164368
1119it [06:34,  2.86it/s]Epoch: 13, Loss: 0.40600806826636904 67200/139232 train accuracy:0.8177232146263123
1189it [06:59,  2.86it/s]Epoch: 13, Loss: 0.4097471400669643 71400/139232 train accuracy:0.8172128796577454
1259it [07:23,  2.86it/s]Epoch: 13, Loss: 0.40323527018229166 75600/139232 train accuracy:0.8170635104179382
1329it [07:48,  2.86it/s]Epoch: 13, Loss: 0.412568359375 79800/139232 train accuracy:0.8170551657676697
1399it [08:12,  2.84it/s]Epoch: 13, Loss: 0.4051195126488095 84000/139232 train accuracy:0.8172619342803955
1469it [08:37,  2.83it/s]Epoch: 13, Loss: 0.40047691708519345 88200/139232 train accuracy:0.817426323890686
1539it [09:02,  2.86it/s]Epoch: 13, Loss: 0.4147522844587054 92400/139232 train accuracy:0.8170238137245178
1609it [09:26,  2.82it/s]Epoch: 13, Loss: 0.4123427908761161 96600/139232 train accuracy:0.8168426752090454
1679it [09:51,  2.81it/s]Epoch: 13, Loss: 0.42016116187686015 100800/139232 train accuracy:0.816527783870697
1749it [10:16,  2.80it/s]Epoch: 13, Loss: 0.4091377185639881 105000/139232 train accuracy:0.8164666891098022
1819it [10:41,  2.84it/s]Epoch: 13, Loss: 0.4045013718377976 109200/139232 train accuracy:0.8162912130355835
1889it [11:05,  2.84it/s]Epoch: 13, Loss: 0.41221226283482143 113400/139232 train accuracy:0.8161110877990723
1959it [11:30,  2.85it/s]Epoch: 13, Loss: 0.41367507207961307 117600/139232 train accuracy:0.8157227635383606
2029it [11:55,  2.84it/s]Epoch: 13, Loss: 0.4031957426525298 121800/139232 train accuracy:0.8157225251197815
2099it [12:19,  2.85it/s]Epoch: 13, Loss: 0.39873616536458334 126000/139232 train accuracy:0.816095232963562
2169it [12:44,  2.85it/s]Epoch: 13, Loss: 0.3908558291480655 130200/139232 train accuracy:0.8162903189659119
2239it [13:09,  2.85it/s]Epoch: 13, Loss: 0.40246724446614585 134400/139232 train accuracy:0.8163467049598694
2309it [13:33,  2.81it/s]Epoch: 13, Loss: 0.3909471784319196 138600/139232 train accuracy:0.8165873289108276
2321it [13:38,  2.84it/s]
Epoch: 13, epoch Loss: 56509.09765625  train accuracy:0.8165867328643799

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.20it/s]
Epoch: 13, Valid Loss: 6622.8115234375  valid accuracy:0.7981253862380981
37it [00:13,  2.86it/s]Epoch: 14, Loss: 0.22109110514322916 2280/139232 train accuracy:0.8114035129547119
69it [00:24,  2.85it/s]Epoch: 14, Loss: 0.18745184035528273 4200/139232 train accuracy:0.8152381181716919
139it [00:49,  2.83it/s]Epoch: 14, Loss: 0.3956822567894345 8400/139232 train accuracy:0.8192856907844543
209it [01:14,  2.82it/s]Epoch: 14, Loss: 0.4007666596912202 12600/139232 train accuracy:0.8180158734321594
279it [01:39,  2.85it/s]Epoch: 14, Loss: 0.3912345958891369 16800/139232 train accuracy:0.8189880847930908
349it [02:03,  2.82it/s]Epoch: 14, Loss: 0.3864940534319196 21000/139232 train accuracy:0.8216190338134766
419it [02:28,  2.82it/s]Epoch: 14, Loss: 0.3942965262276786 25200/139232 train accuracy:0.8207142949104309
489it [02:52,  2.81it/s]Epoch: 14, Loss: 0.39683753603980654 29400/139232 train accuracy:0.8207823038101196
559it [03:17,  2.83it/s]Epoch: 14, Loss: 0.38999677385602677 33600/139232 train accuracy:0.8216071724891663
629it [03:42,  2.82it/s]Epoch: 14, Loss: 0.40586864653087795 37800/139232 train accuracy:0.8207672238349915
699it [04:07,  2.81it/s]Epoch: 14, Loss: 0.3924802943638393 42000/139232 train accuracy:0.8206428289413452
769it [04:32,  2.82it/s]Epoch: 14, Loss: 0.4044649251302083 46200/139232 train accuracy:0.8203246593475342
839it [04:57,  2.81it/s]Epoch: 14, Loss: 0.40499790736607144 50400/139232 train accuracy:0.8201984167098999
909it [05:22,  2.79it/s]Epoch: 14, Loss: 0.3939015125093006 54600/139232 train accuracy:0.8199267387390137
979it [05:47,  2.81it/s]Epoch: 14, Loss: 0.40896365211123514 58800/139232 train accuracy:0.8202040791511536
1049it [06:12,  2.86it/s]Epoch: 14, Loss: 0.3906177048456101 63000/139232 train accuracy:0.8204761743545532
1119it [06:36,  2.86it/s]Epoch: 14, Loss: 0.3887246268136161 67200/139232 train accuracy:0.8205654621124268
1189it [07:01,  2.84it/s]Epoch: 14, Loss: 0.4044905017671131 71400/139232 train accuracy:0.820280134677887
1259it [07:25,  2.86it/s]Epoch: 14, Loss: 0.3856051781063988 75600/139232 train accuracy:0.8205687999725342
1329it [07:50,  2.84it/s]Epoch: 14, Loss: 0.4140287562779018 79800/139232 train accuracy:0.820200502872467
1399it [08:14,  2.87it/s]Epoch: 14, Loss: 0.40187738327752975 84000/139232 train accuracy:0.8200713992118835
1469it [08:39,  2.85it/s]Epoch: 14, Loss: 0.3987010846819196 88200/139232 train accuracy:0.819807231426239
1539it [09:03,  2.86it/s]Epoch: 14, Loss: 0.4009466552734375 92400/139232 train accuracy:0.819718599319458
1609it [09:28,  2.85it/s]Epoch: 14, Loss: 0.3962416294642857 96600/139232 train accuracy:0.8199275135993958
1679it [09:52,  2.86it/s]Epoch: 14, Loss: 0.3926142810639881 100800/139232 train accuracy:0.8201091289520264
1749it [10:17,  2.86it/s]Epoch: 14, Loss: 0.39908179873511906 105000/139232 train accuracy:0.8198094964027405
1819it [10:41,  2.85it/s]Epoch: 14, Loss: 0.3913465808686756 109200/139232 train accuracy:0.820155680179596
1889it [11:06,  2.86it/s]Epoch: 14, Loss: 0.41743960425967264 113400/139232 train accuracy:0.8197001814842224
1959it [11:31,  2.84it/s]Epoch: 14, Loss: 0.39547935849144344 117600/139232 train accuracy:0.819940447807312
2029it [11:55,  2.86it/s]Epoch: 14, Loss: 0.3918568638392857 121800/139232 train accuracy:0.8202627301216125
2099it [12:20,  2.87it/s]Epoch: 14, Loss: 0.40187889462425597 126000/139232 train accuracy:0.8200158476829529
2169it [12:44,  2.87it/s]Epoch: 14, Loss: 0.40485299246651785 130200/139232 train accuracy:0.8198770880699158
2239it [13:09,  2.86it/s]Epoch: 14, Loss: 0.40158717564174107 134400/139232 train accuracy:0.8198809623718262
2309it [13:33,  2.87it/s]Epoch: 14, Loss: 0.39748683384486605 138600/139232 train accuracy:0.8199639320373535
2321it [13:38,  2.84it/s]
Epoch: 14, epoch Loss: 55494.8828125  train accuracy:0.8199264407157898

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.19it/s]
Epoch: 14, Valid Loss: 6654.6416015625  valid accuracy:0.7985779047012329
37it [00:13,  2.86it/s]Epoch: 15, Loss: 0.2127036830357143 2280/139232 train accuracy:0.8298245668411255
69it [00:24,  2.86it/s]Epoch: 15, Loss: 0.18426845005580358 4200/139232 train accuracy:0.8202381134033203
139it [00:49,  2.86it/s]Epoch: 15, Loss: 0.3999624197823661 8400/139232 train accuracy:0.8208333253860474
209it [01:13,  2.86it/s]Epoch: 15, Loss: 0.3896485828218006 12600/139232 train accuracy:0.8220635056495667
279it [01:38,  2.84it/s]Epoch: 15, Loss: 0.39589314778645834 16800/139232 train accuracy:0.8217262029647827
349it [02:03,  2.85it/s]Epoch: 15, Loss: 0.40591671898251486 21000/139232 train accuracy:0.8207142949104309
419it [02:27,  2.81it/s]Epoch: 15, Loss: 0.3925631859188988 25200/139232 train accuracy:0.8212301731109619
489it [02:52,  2.82it/s]Epoch: 15, Loss: 0.40091593424479166 29400/139232 train accuracy:0.8204081654548645
559it [03:17,  2.84it/s]Epoch: 15, Loss: 0.38773672921316965 33600/139232 train accuracy:0.821071445941925
629it [03:42,  2.84it/s]Epoch: 15, Loss: 0.3760594540550595 37800/139232 train accuracy:0.822857141494751
699it [04:06,  2.85it/s]Epoch: 15, Loss: 0.3877462623232887 42000/139232 train accuracy:0.8239762187004089
769it [04:31,  2.82it/s]Epoch: 15, Loss: 0.39465887160528274 46200/139232 train accuracy:0.8233982920646667
839it [04:56,  2.83it/s]Epoch: 15, Loss: 0.37764378138950894 50400/139232 train accuracy:0.8242658972740173
909it [05:21,  2.84it/s]Epoch: 15, Loss: 0.38430315290178574 54600/139232 train accuracy:0.8246703147888184
979it [05:45,  2.82it/s]Epoch: 15, Loss: 0.3894287109375 58800/139232 train accuracy:0.8246088624000549
1049it [06:10,  2.86it/s]Epoch: 15, Loss: 0.39446998232886904 63000/139232 train accuracy:0.8247301578521729
1119it [06:35,  2.85it/s]Epoch: 15, Loss: 0.38930841355096724 67200/139232 train accuracy:0.8246577382087708
1189it [06:59,  2.85it/s]Epoch: 15, Loss: 0.3849598621186756 71400/139232 train accuracy:0.8248039484024048
1259it [07:24,  2.86it/s]Epoch: 15, Loss: 0.39326041085379465 75600/139232 train accuracy:0.8246296048164368
1329it [07:49,  2.86it/s]Epoch: 15, Loss: 0.3888308570498512 79800/139232 train accuracy:0.8249623775482178
1399it [08:13,  2.86it/s]Epoch: 15, Loss: 0.38388282412574404 84000/139232 train accuracy:0.8249523639678955
1469it [08:38,  2.86it/s]Epoch: 15, Loss: 0.3716635567801339 88200/139232 train accuracy:0.8254308104515076
1539it [09:02,  2.85it/s]Epoch: 15, Loss: 0.38467947823660714 92400/139232 train accuracy:0.8253571391105652
1609it [09:27,  2.86it/s]Epoch: 15, Loss: 0.3947782389322917 96600/139232 train accuracy:0.8252587914466858
1679it [09:51,  2.87it/s]Epoch: 15, Loss: 0.4033184814453125 100800/139232 train accuracy:0.8249900937080383
1749it [10:16,  2.86it/s]Epoch: 15, Loss: 0.40413309733072916 105000/139232 train accuracy:0.82474285364151
1819it [10:40,  2.87it/s]Epoch: 15, Loss: 0.40262765066964284 109200/139232 train accuracy:0.8243772983551025
1889it [11:05,  2.85it/s]Epoch: 15, Loss: 0.3903375825427827 113400/139232 train accuracy:0.8243562579154968
1959it [11:30,  2.78it/s]Epoch: 15, Loss: 0.3912652587890625 117600/139232 train accuracy:0.8241751790046692
2029it [11:54,  2.86it/s]Epoch: 15, Loss: 0.39719258626302084 121800/139232 train accuracy:0.8239408731460571
2099it [12:19,  2.86it/s]Epoch: 15, Loss: 0.3989234851655506 126000/139232 train accuracy:0.8238412737846375
2169it [12:43,  2.86it/s]Epoch: 15, Loss: 0.3865078880673363 130200/139232 train accuracy:0.823917031288147
2239it [13:08,  2.86it/s]Epoch: 15, Loss: 0.38745579310825895 134400/139232 train accuracy:0.8239434361457825
2309it [13:32,  2.87it/s]Epoch: 15, Loss: 0.4024194917224702 138600/139232 train accuracy:0.8238311409950256
2321it [13:37,  2.84it/s]
Epoch: 15, epoch Loss: 54538.5  train accuracy:0.8239269852638245

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.18it/s]
Epoch: 15, Valid Loss: 6577.18798828125  valid accuracy:0.8036845326423645
find better loss
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1078/1078 [02:09<00:00,  8.31it/s]
Epoch: 15,   test accuracy:0.8044876456260681
37it [00:13,  2.82it/s]Epoch: 16, Loss: 0.21401002429780505 2280/139232 train accuracy:0.824999988079071
69it [00:25,  2.80it/s]Epoch: 16, Loss: 0.18196278889973957 4200/139232 train accuracy:0.8152381181716919
139it [00:49,  2.82it/s]Epoch: 16, Loss: 0.3817497326078869 8400/139232 train accuracy:0.8240476250648499
209it [01:14,  2.85it/s]Epoch: 16, Loss: 0.3882931082589286 12600/139232 train accuracy:0.8257936239242554
279it [01:39,  2.82it/s]Epoch: 16, Loss: 0.37974920363653275 16800/139232 train accuracy:0.8286904692649841
349it [02:03,  2.80it/s]Epoch: 16, Loss: 0.3780405970982143 21000/139232 train accuracy:0.8298095464706421
419it [02:28,  2.85it/s]Epoch: 16, Loss: 0.38149727957589286 25200/139232 train accuracy:0.8299603462219238
489it [02:53,  2.84it/s]Epoch: 16, Loss: 0.3905242629278274 29400/139232 train accuracy:0.8296598792076111
559it [03:17,  2.86it/s]Epoch: 16, Loss: 0.39784787132626487 33600/139232 train accuracy:0.8285416960716248
629it [03:42,  2.81it/s]Epoch: 16, Loss: 0.38738182431175594 37800/139232 train accuracy:0.8286243677139282
699it [04:07,  2.85it/s]Epoch: 16, Loss: 0.3819490269252232 42000/139232 train accuracy:0.8288571238517761
769it [04:31,  2.85it/s]Epoch: 16, Loss: 0.389114990234375 46200/139232 train accuracy:0.8285064697265625
839it [04:56,  2.84it/s]Epoch: 16, Loss: 0.36713062104724703 50400/139232 train accuracy:0.8297023773193359
909it [05:21,  2.85it/s]Epoch: 16, Loss: 0.39647925967261904 54600/139232 train accuracy:0.8290293216705322
979it [05:45,  2.85it/s]Epoch: 16, Loss: 0.3877332996186756 58800/139232 train accuracy:0.8288945555686951
1049it [06:10,  2.81it/s]Epoch: 16, Loss: 0.38399498349144345 63000/139232 train accuracy:0.8286666870117188
1119it [06:35,  2.86it/s]Epoch: 16, Loss: 0.38724708193824403 67200/139232 train accuracy:0.8286160826683044
1189it [06:59,  2.87it/s]Epoch: 16, Loss: 0.39225222632998513 71400/139232 train accuracy:0.8283193111419678
1259it [07:24,  2.84it/s]Epoch: 16, Loss: 0.3887953404017857 75600/139232 train accuracy:0.8281348943710327
1329it [07:48,  2.85it/s]Epoch: 16, Loss: 0.38315325055803573 79800/139232 train accuracy:0.82776939868927
1399it [08:13,  2.84it/s]Epoch: 16, Loss: 0.40067743210565476 84000/139232 train accuracy:0.8272737860679626
1469it [08:38,  2.82it/s]Epoch: 16, Loss: 0.38624645414806547 88200/139232 train accuracy:0.8271995186805725
1539it [09:02,  2.84it/s]Epoch: 16, Loss: 0.3848890904017857 92400/139232 train accuracy:0.8274567127227783
1609it [09:27,  2.85it/s]Epoch: 16, Loss: 0.3894173177083333 96600/139232 train accuracy:0.8272567391395569
1679it [09:52,  2.83it/s]Epoch: 16, Loss: 0.37413521902901786 100800/139232 train accuracy:0.8274503946304321
1749it [10:16,  2.84it/s]Epoch: 16, Loss: 0.3892435709635417 105000/139232 train accuracy:0.8273142576217651
1819it [10:41,  2.85it/s]Epoch: 16, Loss: 0.39749776204427084 109200/139232 train accuracy:0.8268772959709167
1889it [11:05,  2.85it/s]Epoch: 16, Loss: 0.3752336193266369 113400/139232 train accuracy:0.8269752860069275
1959it [11:30,  2.84it/s]Epoch: 16, Loss: 0.3913425990513393 117600/139232 train accuracy:0.8269217610359192
2029it [11:55,  2.82it/s]Epoch: 16, Loss: 0.3855731201171875 121800/139232 train accuracy:0.8271675109863281
2099it [12:20,  2.81it/s]Epoch: 16, Loss: 0.36846348353794645 126000/139232 train accuracy:0.8275873064994812
2169it [12:44,  2.84it/s]Epoch: 16, Loss: 0.38659525553385415 130200/139232 train accuracy:0.8276113867759705
2239it [13:09,  2.86it/s]Epoch: 16, Loss: 0.4070758347284226 134400/139232 train accuracy:0.8272916674613953
2309it [13:34,  2.85it/s]Epoch: 16, Loss: 0.39115196591331847 138600/139232 train accuracy:0.8271717429161072
2321it [13:38,  2.84it/s]
Epoch: 16, epoch Loss: 53855.97265625  train accuracy:0.82719486951828

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.18it/s]
Epoch: 16, Valid Loss: 6609.2392578125  valid accuracy:0.8007757067680359
37it [00:13,  2.83it/s]Epoch: 17, Loss: 0.19797206333705358 2280/139232 train accuracy:0.8311403393745422
69it [00:24,  2.83it/s]Epoch: 17, Loss: 0.17423732212611606 4200/139232 train accuracy:0.8330952525138855
139it [00:49,  2.85it/s]Epoch: 17, Loss: 0.3752893647693452 8400/139232 train accuracy:0.835357129573822
209it [01:14,  2.85it/s]Epoch: 17, Loss: 0.3783859107607887 12600/139232 train accuracy:0.8341270089149475
279it [01:38,  2.83it/s]Epoch: 17, Loss: 0.37419709705171134 16800/139232 train accuracy:0.8338690400123596
349it [02:03,  2.85it/s]Epoch: 17, Loss: 0.36640049525669643 21000/139232 train accuracy:0.8340952396392822
419it [02:28,  2.82it/s]Epoch: 17, Loss: 0.3896022542317708 25200/139232 train accuracy:0.832698404788971
489it [02:52,  2.85it/s]Epoch: 17, Loss: 0.371695556640625 29400/139232 train accuracy:0.8335034251213074
559it [03:17,  2.85it/s]Epoch: 17, Loss: 0.37119390578497025 33600/139232 train accuracy:0.8335416913032532
629it [03:42,  2.85it/s]Epoch: 17, Loss: 0.3793302699497768 37800/139232 train accuracy:0.8334127068519592
699it [04:06,  2.85it/s]Epoch: 17, Loss: 0.38338463192894345 42000/139232 train accuracy:0.8326904773712158
769it [04:31,  2.81it/s]Epoch: 17, Loss: 0.39795224144345237 46200/139232 train accuracy:0.8313419818878174
839it [04:56,  2.78it/s]Epoch: 17, Loss: 0.37460489908854167 50400/139232 train accuracy:0.8318849205970764
909it [05:21,  2.85it/s]Epoch: 17, Loss: 0.37050345284598213 54600/139232 train accuracy:0.8319780230522156
979it [05:45,  2.84it/s]Epoch: 17, Loss: 0.39963483537946426 58800/139232 train accuracy:0.8310714364051819
1049it [06:10,  2.85it/s]Epoch: 17, Loss: 0.37993384951636905 63000/139232 train accuracy:0.8308571577072144
1119it [06:35,  2.84it/s]Epoch: 17, Loss: 0.38154959542410716 67200/139232 train accuracy:0.8312946557998657
1189it [07:00,  2.87it/s]Epoch: 17, Loss: 0.3818060012090774 71400/139232 train accuracy:0.8308543562889099
1259it [07:24,  2.80it/s]Epoch: 17, Loss: 0.38584833054315476 75600/139232 train accuracy:0.8306481242179871
1329it [07:49,  2.82it/s]Epoch: 17, Loss: 0.37315589541480654 79800/139232 train accuracy:0.8307393193244934
1399it [08:14,  2.81it/s]Epoch: 17, Loss: 0.3897925967261905 84000/139232 train accuracy:0.8306309580802917
1469it [08:39,  2.81it/s]Epoch: 17, Loss: 0.3857484363374256 88200/139232 train accuracy:0.8306916356086731
1539it [09:04,  2.82it/s]Epoch: 17, Loss: 0.3757487560453869 92400/139232 train accuracy:0.8308225274085999
1609it [09:29,  2.82it/s]Epoch: 17, Loss: 0.3690919131324405 96600/139232 train accuracy:0.8311594128608704
1679it [09:53,  2.82it/s]Epoch: 17, Loss: 0.38486487978980655 100800/139232 train accuracy:0.8308333158493042
1749it [10:18,  2.86it/s]Epoch: 17, Loss: 0.3856028529575893 105000/139232 train accuracy:0.8306190371513367
1819it [10:43,  2.85it/s]Epoch: 17, Loss: 0.3774954078311012 109200/139232 train accuracy:0.8307509422302246
1889it [11:07,  2.86it/s]Epoch: 17, Loss: 0.3762171572730655 113400/139232 train accuracy:0.8309524059295654
1959it [11:32,  2.83it/s]Epoch: 17, Loss: 0.3918121337890625 117600/139232 train accuracy:0.8307483196258545
2029it [11:57,  2.81it/s]Epoch: 17, Loss: 0.3804832531156994 121800/139232 train accuracy:0.8308128118515015
2099it [12:22,  2.84it/s]Epoch: 17, Loss: 0.38399117606026784 126000/139232 train accuracy:0.8307619094848633
2169it [12:47,  2.81it/s]Epoch: 17, Loss: 0.369304431733631 130200/139232 train accuracy:0.8309062719345093
2239it [13:12,  2.82it/s]Epoch: 17, Loss: 0.3754133823939732 134400/139232 train accuracy:0.8308631181716919
2309it [13:36,  2.81it/s]Epoch: 17, Loss: 0.3790497116815476 138600/139232 train accuracy:0.8309018611907959
2321it [13:41,  2.83it/s]
Epoch: 17, epoch Loss: 52873.87109375  train accuracy:0.8308075666427612

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.19it/s]
Epoch: 17, Valid Loss: 6652.474609375  valid accuracy:0.800000011920929
37it [00:13,  2.82it/s]Epoch: 18, Loss: 0.2068350365048363 2280/139232 train accuracy:0.8258771896362305
69it [00:25,  2.86it/s]Epoch: 18, Loss: 0.16913000197637648 4200/139232 train accuracy:0.8269047737121582
139it [00:49,  2.82it/s]Epoch: 18, Loss: 0.36097240629650296 8400/139232 train accuracy:0.8360714316368103
209it [01:14,  2.84it/s]Epoch: 18, Loss: 0.36698887416294645 12600/139232 train accuracy:0.8360317349433899
279it [01:39,  2.80it/s]Epoch: 18, Loss: 0.36828023274739585 16800/139232 train accuracy:0.8360714316368103
349it [02:03,  2.86it/s]Epoch: 18, Loss: 0.3766825067429315 21000/139232 train accuracy:0.8345714211463928
419it [02:28,  2.82it/s]Epoch: 18, Loss: 0.3794051106770833 25200/139232 train accuracy:0.8344444632530212
489it [02:53,  2.85it/s]Epoch: 18, Loss: 0.3610687255859375 29400/139232 train accuracy:0.8351020216941833
559it [03:18,  2.84it/s]Epoch: 18, Loss: 0.37477617536272323 33600/139232 train accuracy:0.835565447807312
629it [03:42,  2.86it/s]Epoch: 18, Loss: 0.3772724841889881 37800/139232 train accuracy:0.8348941802978516
699it [04:07,  2.85it/s]Epoch: 18, Loss: 0.3768082101004464 42000/139232 train accuracy:0.8342618942260742
769it [04:31,  2.85it/s]Epoch: 18, Loss: 0.3741110374813988 46200/139232 train accuracy:0.8341991305351257
839it [04:56,  2.85it/s]Epoch: 18, Loss: 0.3698695010230655 50400/139232 train accuracy:0.8348214030265808
909it [05:21,  2.86it/s]Epoch: 18, Loss: 0.37602704729352676 54600/139232 train accuracy:0.8349267244338989
979it [05:46,  2.84it/s]Epoch: 18, Loss: 0.3666296096075149 58800/139232 train accuracy:0.834540843963623
1049it [06:10,  2.86it/s]Epoch: 18, Loss: 0.3732721237909226 63000/139232 train accuracy:0.8347777724266052
1119it [06:35,  2.85it/s]Epoch: 18, Loss: 0.3788995651971726 67200/139232 train accuracy:0.8346428275108337
1189it [06:59,  2.86it/s]Epoch: 18, Loss: 0.37206095377604165 71400/139232 train accuracy:0.8345097899436951
1259it [07:24,  2.85it/s]Epoch: 18, Loss: 0.3726847621372768 75600/139232 train accuracy:0.8341931104660034
1329it [07:48,  2.85it/s]Epoch: 18, Loss: 0.3558968970889137 79800/139232 train accuracy:0.8345488905906677
1399it [08:13,  2.85it/s]Epoch: 18, Loss: 0.37574526832217264 84000/139232 train accuracy:0.8347142934799194
1469it [08:38,  2.79it/s]Epoch: 18, Loss: 0.38450125558035714 88200/139232 train accuracy:0.8343310952186584
1539it [09:02,  2.86it/s]Epoch: 18, Loss: 0.36862040201822915 92400/139232 train accuracy:0.8345237970352173
1609it [09:27,  2.85it/s]Epoch: 18, Loss: 0.3724888974144345 96600/139232 train accuracy:0.8342649936676025
1679it [09:52,  2.84it/s]Epoch: 18, Loss: 0.3678898111979167 100800/139232 train accuracy:0.8343650698661804
1749it [10:16,  2.84it/s]Epoch: 18, Loss: 0.35479335239955356 105000/139232 train accuracy:0.8347523808479309
1819it [10:41,  2.85it/s]Epoch: 18, Loss: 0.367417980375744 109200/139232 train accuracy:0.8348351716995239
1889it [11:05,  2.83it/s]Epoch: 18, Loss: 0.3608203415643601 113400/139232 train accuracy:0.8349118232727051
1959it [11:30,  2.83it/s]Epoch: 18, Loss: 0.36974859328497023 117600/139232 train accuracy:0.8349149823188782
2029it [11:55,  2.31it/s]Epoch: 18, Loss: 0.3748321533203125 121800/139232 train accuracy:0.8348357677459717
2099it [12:19,  2.81it/s]Epoch: 18, Loss: 0.37500953311011903 126000/139232 train accuracy:0.8346904516220093
2169it [12:44,  2.86it/s]Epoch: 18, Loss: 0.35543904622395833 130200/139232 train accuracy:0.8351459503173828
2239it [13:09,  2.82it/s]Epoch: 18, Loss: 0.37298621768043155 134400/139232 train accuracy:0.8351413607597351
2309it [13:34,  2.81it/s]Epoch: 18, Loss: 0.3842173549107143 138600/139232 train accuracy:0.8349711298942566
2321it [13:38,  2.84it/s]
Epoch: 18, epoch Loss: 51649.05859375  train accuracy:0.8349517583847046

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.19it/s]
Epoch: 18, Valid Loss: 6635.2001953125  valid accuracy:0.8006464242935181
37it [00:13,  2.82it/s]Epoch: 19, Loss: 0.19998651413690477 2280/139232 train accuracy:0.8429824709892273
69it [00:24,  2.84it/s]Epoch: 19, Loss: 0.17437504359654019 4200/139232 train accuracy:0.8354762196540833
139it [00:49,  2.82it/s]Epoch: 19, Loss: 0.366116943359375 8400/139232 train accuracy:0.8385714292526245
209it [01:14,  2.81it/s]Epoch: 19, Loss: 0.36625668480282736 12600/139232 train accuracy:0.8394444584846497
279it [01:39,  2.85it/s]Epoch: 19, Loss: 0.36624337332589285 16800/139232 train accuracy:0.8402976393699646
349it [02:03,  2.85it/s]Epoch: 19, Loss: 0.3692010207403274 21000/139232 train accuracy:0.8394761681556702
419it [02:28,  2.86it/s]Epoch: 19, Loss: 0.3725880068824405 25200/139232 train accuracy:0.8391270041465759
489it [02:52,  2.86it/s]Epoch: 19, Loss: 0.3602841331845238 29400/139232 train accuracy:0.8396939039230347
559it [03:17,  2.85it/s]Epoch: 19, Loss: 0.35474022274925593 33600/139232 train accuracy:0.8412500023841858
629it [03:41,  2.86it/s]Epoch: 19, Loss: 0.36112714494977677 37800/139232 train accuracy:0.8407672047615051
699it [04:06,  2.84it/s]Epoch: 19, Loss: 0.3598944963727679 42000/139232 train accuracy:0.8406904935836792
769it [04:31,  2.82it/s]Epoch: 19, Loss: 0.37150213332403276 46200/139232 train accuracy:0.8407142758369446
839it [04:56,  2.86it/s]Epoch: 19, Loss: 0.37137102399553573 50400/139232 train accuracy:0.8401190638542175
909it [05:21,  2.80it/s]Epoch: 19, Loss: 0.37573832194010415 54600/139232 train accuracy:0.8395421504974365
979it [05:45,  2.84it/s]Epoch: 19, Loss: 0.3630480666387649 58800/139232 train accuracy:0.8393707275390625
1049it [06:10,  2.83it/s]Epoch: 19, Loss: 0.3679414295014881 63000/139232 train accuracy:0.8390317559242249
1119it [06:34,  2.85it/s]Epoch: 19, Loss: 0.3590236118861607 67200/139232 train accuracy:0.8391517996788025
1189it [06:59,  2.82it/s]Epoch: 19, Loss: 0.3682941545758929 71400/139232 train accuracy:0.8387535214424133
1259it [07:24,  2.82it/s]Epoch: 19, Loss: 0.3607449486142113 75600/139232 train accuracy:0.8389682769775391
1329it [07:49,  2.86it/s]Epoch: 19, Loss: 0.3649593098958333 79800/139232 train accuracy:0.8388972282409668
1399it [08:13,  2.85it/s]Epoch: 19, Loss: 0.35503011067708334 84000/139232 train accuracy:0.8390238285064697
1469it [08:38,  2.85it/s]Epoch: 19, Loss: 0.3664286004929315 88200/139232 train accuracy:0.8391609787940979
1539it [09:02,  2.84it/s]Epoch: 19, Loss: 0.36736040387834823 92400/139232 train accuracy:0.8391558527946472
1609it [09:27,  2.85it/s]Epoch: 19, Loss: 0.3601114036923363 96600/139232 train accuracy:0.8393892049789429
1679it [09:51,  2.85it/s]Epoch: 19, Loss: 0.3529275076729911 100800/139232 train accuracy:0.8396329283714294
1749it [10:16,  2.86it/s]Epoch: 19, Loss: 0.3516308303106399 105000/139232 train accuracy:0.8401142954826355
1819it [10:40,  2.86it/s]Epoch: 19, Loss: 0.36559942336309526 109200/139232 train accuracy:0.8400458097457886
1889it [11:05,  2.85it/s]Epoch: 19, Loss: 0.3634898449125744 113400/139232 train accuracy:0.8399735689163208
1959it [11:30,  2.87it/s]Epoch: 19, Loss: 0.37248038155691965 117600/139232 train accuracy:0.8399234414100647
2029it [11:54,  2.86it/s]Epoch: 19, Loss: 0.3617201160249256 121800/139232 train accuracy:0.8399096727371216
2099it [12:19,  2.87it/s]Epoch: 19, Loss: 0.3866615222749256 126000/139232 train accuracy:0.839206337928772
2169it [12:44,  2.86it/s]Epoch: 19, Loss: 0.35379010881696427 130200/139232 train accuracy:0.8393010497093201
2239it [13:08,  2.84it/s]Epoch: 19, Loss: 0.3651639229910714 134400/139232 train accuracy:0.8393154740333557
2309it [13:33,  2.86it/s]Epoch: 19, Loss: 0.3484811256045387 138600/139232 train accuracy:0.8397330641746521
2321it [13:37,  2.84it/s]
Epoch: 19, epoch Loss: 50736.50390625  train accuracy:0.8397350907325745

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.21it/s]
Epoch: 19, Valid Loss: 6702.15673828125  valid accuracy:0.7962508201599121
37it [00:13,  2.86it/s]Epoch: 20, Loss: 0.19578622000558035 2280/139232 train accuracy:0.8473684191703796
69it [00:24,  2.85it/s]Epoch: 20, Loss: 0.16814642043340775 4200/139232 train accuracy:0.8430952429771423
139it [00:49,  2.86it/s]Epoch: 20, Loss: 0.3612463960193452 8400/139232 train accuracy:0.8409523963928223
209it [01:13,  2.86it/s]Epoch: 20, Loss: 0.3640152122860863 12600/139232 train accuracy:0.8409523963928223
279it [01:38,  2.82it/s]Epoch: 20, Loss: 0.3685014415922619 16800/139232 train accuracy:0.8395237922668457
349it [02:03,  2.86it/s]Epoch: 20, Loss: 0.34504798525855657 21000/139232 train accuracy:0.8413333296775818
419it [02:28,  2.85it/s]Epoch: 20, Loss: 0.3572929454985119 25200/139232 train accuracy:0.8416666388511658
489it [02:52,  2.86it/s]Epoch: 20, Loss: 0.3561033993675595 29400/139232 train accuracy:0.8421428799629211
559it [03:16,  2.87it/s]Epoch: 20, Loss: 0.3595176478794643 33600/139232 train accuracy:0.8418452143669128
629it [03:41,  2.86it/s]Epoch: 20, Loss: 0.34459568568638393 37800/139232 train accuracy:0.8432539701461792
699it [04:06,  2.87it/s]Epoch: 20, Loss: 0.34959373837425595 42000/139232 train accuracy:0.8433571457862854
769it [04:30,  2.86it/s]Epoch: 20, Loss: 0.35994128999255953 46200/139232 train accuracy:0.8429004549980164
839it [04:55,  2.86it/s]Epoch: 20, Loss: 0.35461687360491073 50400/139232 train accuracy:0.8434325456619263
909it [05:20,  2.53it/s]Epoch: 20, Loss: 0.3575750441778274 54600/139232 train accuracy:0.8430769443511963
979it [05:44,  2.82it/s]Epoch: 20, Loss: 0.35292718796502975 58800/139232 train accuracy:0.8432313203811646
1049it [06:09,  2.87it/s]Epoch: 20, Loss: 0.3510969470796131 63000/139232 train accuracy:0.8437460064888
1119it [06:33,  2.87it/s]Epoch: 20, Loss: 0.36673450288318454 67200/139232 train accuracy:0.8432291746139526
1189it [06:58,  2.87it/s]Epoch: 20, Loss: 0.36009701683407735 71400/139232 train accuracy:0.843137264251709
1259it [07:23,  2.83it/s]Epoch: 20, Loss: 0.3564588855561756 75600/139232 train accuracy:0.8431349396705627
1329it [07:47,  2.86it/s]Epoch: 20, Loss: 0.3506064278738839 79800/139232 train accuracy:0.8434461355209351
1399it [08:12,  2.81it/s]Epoch: 20, Loss: 0.36082810174851193 84000/139232 train accuracy:0.8430356979370117
1469it [08:36,  2.87it/s]Epoch: 20, Loss: 0.3457989792596726 88200/139232 train accuracy:0.843696117401123
1539it [09:01,  2.85it/s]Epoch: 20, Loss: 0.3456834484281994 92400/139232 train accuracy:0.843971848487854
1609it [09:26,  2.85it/s]Epoch: 20, Loss: 0.35098228817894345 96600/139232 train accuracy:0.8439958691596985
1679it [09:50,  2.87it/s]Epoch: 20, Loss: 0.3453936476934524 100800/139232 train accuracy:0.8442757725715637
1749it [10:15,  2.85it/s]Epoch: 20, Loss: 0.3595715622674851 105000/139232 train accuracy:0.8444761633872986
1819it [10:39,  2.87it/s]Epoch: 20, Loss: 0.3549847702752976 109200/139232 train accuracy:0.844578742980957
1889it [11:04,  2.85it/s]Epoch: 20, Loss: 0.3589681861514137 113400/139232 train accuracy:0.8447442650794983
1959it [11:28,  2.86it/s]Epoch: 20, Loss: 0.37022995721726193 117600/139232 train accuracy:0.8443877696990967
2029it [11:53,  2.86it/s]Epoch: 20, Loss: 0.3581113106863839 121800/139232 train accuracy:0.8443103432655334
2099it [12:17,  2.87it/s]Epoch: 20, Loss: 0.35614818754650296 126000/139232 train accuracy:0.8441349267959595
2169it [12:42,  2.86it/s]Epoch: 20, Loss: 0.3636632719494048 130200/139232 train accuracy:0.843855619430542
2239it [13:06,  2.84it/s]Epoch: 20, Loss: 0.36460045224144344 134400/139232 train accuracy:0.8437574505805969
2309it [13:31,  2.85it/s]Epoch: 20, Loss: 0.345682373046875 138600/139232 train accuracy:0.8438816666603088
2321it [13:35,  2.85it/s]
Epoch: 20, epoch Loss: 49631.55859375  train accuracy:0.8437715172767639

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.20it/s]
Epoch: 20, Valid Loss: 6638.7021484375  valid accuracy:0.8046541810035706
37it [00:13,  2.85it/s]Epoch: 21, Loss: 0.19527608235677082 2280/139232 train accuracy:0.8258771896362305
69it [00:24,  2.86it/s]Epoch: 21, Loss: 0.15741443452380952 4200/139232 train accuracy:0.8376190662384033
139it [00:49,  2.87it/s]Epoch: 21, Loss: 0.36052815755208334 8400/139232 train accuracy:0.8388095498085022
209it [01:13,  2.87it/s]Epoch: 21, Loss: 0.3498538062686012 12600/139232 train accuracy:0.8415079116821289
279it [01:38,  2.86it/s]Epoch: 21, Loss: 0.35074079241071426 16800/139232 train accuracy:0.8419047594070435
349it [02:02,  2.86it/s]Epoch: 21, Loss: 0.3505625406901042 21000/139232 train accuracy:0.8441428542137146
419it [02:27,  2.83it/s]Epoch: 21, Loss: 0.34262570335751485 25200/139232 train accuracy:0.845119059085846
489it [02:51,  2.87it/s]Epoch: 21, Loss: 0.3419346400669643 29400/139232 train accuracy:0.8467006683349609
559it [03:16,  2.86it/s]Epoch: 21, Loss: 0.341038818359375 33600/139232 train accuracy:0.8473214507102966
629it [03:40,  2.86it/s]Epoch: 21, Loss: 0.36030093238467265 37800/139232 train accuracy:0.8467989563941956
699it [04:05,  2.86it/s]Epoch: 21, Loss: 0.3430017380487351 42000/139232 train accuracy:0.8470476269721985
769it [04:29,  2.86it/s]Epoch: 21, Loss: 0.3463797142392113 46200/139232 train accuracy:0.8474242687225342
839it [04:54,  2.85it/s]Epoch: 21, Loss: 0.3445167759486607 50400/139232 train accuracy:0.8479166626930237
909it [05:18,  2.84it/s]Epoch: 21, Loss: 0.33976507277715773 54600/139232 train accuracy:0.8481501936912537
979it [05:43,  2.82it/s]Epoch: 21, Loss: 0.33954319545200895 58800/139232 train accuracy:0.8483503460884094
1049it [06:08,  2.85it/s]Epoch: 21, Loss: 0.34926074800037205 63000/139232 train accuracy:0.8483492136001587
1119it [06:33,  2.87it/s]Epoch: 21, Loss: 0.3529331461588542 67200/139232 train accuracy:0.8476934432983398
1189it [06:57,  2.86it/s]Epoch: 21, Loss: 0.3555637904575893 71400/139232 train accuracy:0.8477030992507935
1259it [07:22,  2.82it/s]Epoch: 21, Loss: 0.3542829822358631 75600/139232 train accuracy:0.8476322889328003
1329it [07:47,  2.86it/s]Epoch: 21, Loss: 0.36564354306175595 79800/139232 train accuracy:0.8470301032066345
1399it [08:11,  2.84it/s]Epoch: 21, Loss: 0.340096435546875 84000/139232 train accuracy:0.8472023606300354
1469it [08:36,  2.81it/s]Epoch: 21, Loss: 0.36399852934337795 88200/139232 train accuracy:0.8468027114868164
1539it [09:00,  2.84it/s]Epoch: 21, Loss: 0.35729131789434526 92400/139232 train accuracy:0.8467965126037598
1609it [09:25,  2.85it/s]Epoch: 21, Loss: 0.3439305187406994 96600/139232 train accuracy:0.8469772338867188
1679it [09:50,  2.83it/s]Epoch: 21, Loss: 0.34535345168340775 100800/139232 train accuracy:0.8475297689437866
1749it [10:14,  2.83it/s]Epoch: 21, Loss: 0.35314354306175594 105000/139232 train accuracy:0.8474952578544617
1819it [10:39,  2.84it/s]Epoch: 21, Loss: 0.34108404250372026 109200/139232 train accuracy:0.8476556539535522
1889it [11:04,  2.85it/s]Epoch: 21, Loss: 0.3477618117559524 113400/139232 train accuracy:0.8476895689964294
1959it [11:28,  2.87it/s]Epoch: 21, Loss: 0.34374654134114585 117600/139232 train accuracy:0.8478571176528931
2029it [11:53,  2.85it/s]Epoch: 21, Loss: 0.3536976550874256 121800/139232 train accuracy:0.8478571176528931
2099it [12:17,  2.86it/s]Epoch: 21, Loss: 0.35832603817894343 126000/139232 train accuracy:0.8474920392036438
2169it [12:42,  2.86it/s]Epoch: 21, Loss: 0.3473050944010417 130200/139232 train accuracy:0.8475115299224854
2239it [13:07,  2.85it/s]Epoch: 21, Loss: 0.34253548758370533 134400/139232 train accuracy:0.8475446701049805
2309it [13:31,  2.86it/s]Epoch: 21, Loss: 0.3507528250558036 138600/139232 train accuracy:0.8474242687225342
2321it [13:35,  2.84it/s]
Epoch: 21, epoch Loss: 48649.37890625  train accuracy:0.8474201560020447

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 258/258 [00:31<00:00,  8.20it/s]
Epoch: 21, Valid Loss: 6810.44482421875  valid accuracy:0.795281171798706
15it [00:05,  2.54it/s]^C
Traceback (most recent call last):
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 585, in <module>
    predictions= ProtoCNN([x_batch, author_batch, topic_batch], training=True)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
    return fn(*args, **kwargs)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/engine/training.py", line 557, in __call__
    return super().__call__(*args, **kwargs)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
    return fn(*args, **kwargs)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/engine/base_layer.py", line 1097, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 96, in error_handler
    return fn(*args, **kwargs)
  File "/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src/two-prototype-layer.py", line 166, in call
    x = self.tokenizer(input_content, padding = "max_length", max_length=self.max_l, return_tensors ="tf",truncation = True )
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2455, in __call__
    return self.batch_encode_plus(
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2646, in batch_encode_plus
    return self._batch_encode_plus(
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 730, in _batch_encode_plus
    first_ids = get_input_ids(ids)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 697, in get_input_ids
    tokens = self.tokenize(text, **kwargs)
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 546, in tokenize
    tokenized_text.extend(self._tokenize(token))
  File "/big/xw384/venvs/BERT/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py", line 227, in _tokenize
    if token in self.basic_tokenizer.never_split:
KeyboardInterrupt
^C
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ ^C
(BERT) xw384@cci-bvm74:/big/xw384/schoolwork/NLP+DEEP LEARNING/Project/CASCADE/src$ tmux capture-pane -pS - > output2.log

