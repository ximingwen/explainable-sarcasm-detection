{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe02cf71-be41-41d4-b169-1e73c62069fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec208827-5edd-427c-a0d3-484072872a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c1634f-ee95-4817-b262-6caaf8781b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 19:39:57.530589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-10 19:39:57.668107: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-10 19:39:58.253383: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:\n",
      "2024-01-10 19:39:58.253454: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64:\n",
      "2024-01-10 19:39:58.253460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/big/xw384/venvs/BERT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "from time import sleep\n",
    "import pickle, argparse\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "from tensorflow import keras \n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7669af3-aa13-4cd0-a896-b4217f1cf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_variables(tf_name, k1, k2, initializer):\n",
    "     \n",
    "    return tf.Variable(initializer(shape=[k1, k2], dtype=tf.float32), trainable=True, name=tf_name)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data, batch_size=200, shuffle=True):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the number of batches\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffles the indexes if required\n",
    "        data = pd.DataFrame(self.data).to_numpy()\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int((len(data)-1)/self.batch_size) + 1\n",
    "      \n",
    "        if self.shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * self.batch_size\n",
    "            end_index = min((batch_num + 1) * self.batch_size, data_size)\n",
    "            output = list(zip(*shuffled_data[start_index:end_index]))\n",
    "            yield output[0],  output[1],  output[2],  output[3]\n",
    "       \n",
    "\n",
    "            \n",
    "#prototype layer\n",
    "class prototypeLayer(keras.layers.Layer):\n",
    "    def __init__(self, k_protos, vect_size, k_cents):\n",
    "        super(prototypeLayer, self).__init__(name='proto_layer')\n",
    "        self.n_protos = k_protos\n",
    "        self.vect_size = vect_size\n",
    "        self.prototypes = make_variables(\"prototypes\", k_protos, vect_size,\n",
    "                                         initializer=tf.constant_initializer(k_cents))\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        tmp1 = tf.expand_dims(inputs, 2)\n",
    "\n",
    "        tmp1 = tf.broadcast_to(tmp1, [tf.shape(tmp1)[0], tf.shape(tmp1)[1], self.n_protos, self.vect_size])\n",
    "        tmp2 = tf.broadcast_to(self.prototypes,\n",
    "                               [tf.shape(tmp1)[0], tf.shape(tmp1)[1], self.n_protos, self.vect_size])\n",
    "        tmp3 = tmp1 - tmp2\n",
    "        tmp4 = tmp3 * tmp3\n",
    "        distances = tf.reduce_sum(tmp4, axis=3)\n",
    "\n",
    "        return distances, self.prototypes\n",
    "\n",
    "#distance layer: to convert the full distance matrix to sparse similarity matrix\n",
    "class distanceLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(distanceLayer, self).__init__(name='distance_layer')\n",
    "        self.a = 0.1\n",
    "        self.beta = 1e6\n",
    "\n",
    "    def e_func(self, x, e=2.7182818284590452353602874713527):\n",
    "        return tf.math.pow(e, -(self.a * x))\n",
    "\n",
    "    # @tf.function\n",
    "    # def call(self, full_distances):\n",
    "    #     min_dist_ind = tf.nn.softmax(-full_distances * self.beta)\n",
    "    #     e_dist = self.e_func(full_distances) + 1e-8\n",
    "    #     dist_hot_vect = min_dist_ind * e_dist\n",
    "    #     return dist_hot_vect\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, full_distances):\n",
    "        e_dist = self.e_func(full_distances) + 1e-8\n",
    "        dist_hot_vect = tf.squeeze(e_dist, axis=0)\n",
    "        return dist_hot_vect\n",
    "\n",
    "\n",
    "\n",
    "class TextCNN(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A CNN for text classification in TensorFlow 2.x.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling, and softmax layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, sequence_length, num_classes, tokenizer, bert_model, user_embeddings, topic_embeddings, embedding_size, filter_sizes, num_filters, l2_reg_lambda, dropout_keep_prob, k_protos, vect_size):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.max_l = sequence_length\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        l2_regularizer = tf.keras.regularizers.l2(l2_reg_lambda)\n",
    "        # Embedding layer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding = bert_model\n",
    "        self.num_filters = num_filters\n",
    "        self.filters_sizes = filter_sizes\n",
    "        self.k_protos = k_protos\n",
    "        self.vect_size = vect_size\n",
    "        \n",
    "        self.user_embedding = tf.keras.layers.Embedding(input_dim=user_embeddings.shape[0], output_dim=user_embeddings.shape[1], weights=[user_embeddings], trainable=False)\n",
    "        self.topic_embedding = tf.keras.layers.Embedding(input_dim=topic_embeddings.shape[0], output_dim=topic_embeddings.shape[1], weights=[topic_embeddings], trainable=False)\n",
    "        self.distance_layer = distanceLayer()\n",
    "\n",
    "        self.conv_layers = []\n",
    "        for filter_size in filter_sizes:\n",
    "            conv_block = tf.keras.Sequential([\n",
    "                layers.Conv2D(num_filters, (filter_size, embedding_size), \n",
    "                              padding='valid', activation='relu'),\n",
    "                layers.MaxPooling2D(pool_size=(sequence_length - filter_size + 1, 1), \n",
    "                                    strides=(1,1), padding='valid')])\n",
    "            self.conv_layers.append(conv_block)\n",
    "        self.concat_layer = tf.keras.layers.Concatenate()\n",
    "        #self.last_dense = tf.keras.layers.Dense(100, activation='relu')\n",
    "        self.user_topic_dense = tf.keras.layers.Dense(400, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(1 - dropout_keep_prob)\n",
    "        # Final dense layer with L2 regularization\n",
    "        self.final_dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=l2_regularizer)\n",
    "\n",
    "    def init_prototypelayer(self, res_cents, user_cents):\n",
    "        \n",
    "        self.response_proto_layer = prototypeLayer(self.k_protos, self.vect_size, res_cents)\n",
    "        self.user_proto_layer = prototypeLayer(self.k_protos, 100, user_cents)\n",
    "       \n",
    "\n",
    "    def call(self, inputs):\n",
    "       \n",
    "\n",
    "        input_content, input_author, input_topic = inputs\n",
    "\n",
    "       \n",
    "         # Embedding layer\n",
    "        x = self.tokenizer(input_content, padding = \"max_length\", max_length=self.max_l, return_tensors =\"tf\",truncation = True )\n",
    "        x = self.embedding(input_ids = x[\"input_ids\"], attention_mask = x[\"attention_mask\"], output_hidden_states =True)[0]\n",
    "        x = tf.expand_dims(x, -1)\n",
    "\n",
    "\n",
    "        pooled_outputs = []\n",
    "        for conv in self.conv_layers:\n",
    "            conv_out = conv(x)\n",
    "            pooled_outputs.append(conv_out)\n",
    "\n",
    "        num_filters_total = self.num_filters * len(self.filters_sizes)\n",
    "        h_pool = tf.concat(pooled_outputs, axis=1)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "        \n",
    "        x = tf.expand_dims(h_pool_flat, axis=0)\n",
    "        full_distances, res_protos = self.response_proto_layer(x)\n",
    "        res_vect = self.distance_layer(full_distances)\n",
    "\n",
    "        user_embeddings = self.user_embedding(input_author)\n",
    "\n",
    "      \n",
    "        full_distances, user_protos = self.user_proto_layer(tf.expand_dims(user_embeddings, axis=0))\n",
    "        user_vect = self.distance_layer(full_distances)\n",
    "\n",
    "\n",
    "        topic_embeddings = self.topic_embedding(input_topic)\n",
    "        \n",
    "        combined_vectors = self.concat_layer([res_vect, user_vect, topic_embeddings])\n",
    "        combined_vector_final = self.user_topic_dense(combined_vectors)\n",
    "\n",
    "        \n",
    "        combined_vector_final = self.dropout(combined_vector_final)\n",
    "\n",
    "        scores = self.final_dense(combined_vector_final)\n",
    "\n",
    "        #return scores,  full_distances, res_protos, user_protos\n",
    "        return scores\n",
    "    def embed_res(self, x):\n",
    "         # Embedding layer\n",
    "        x = self.tokenizer(x, padding = \"max_length\", max_length=self.max_l, return_tensors =\"tf\",truncation = True )\n",
    "        x = self.embedding(input_ids = x[\"input_ids\"], attention_mask = x[\"attention_mask\"], output_hidden_states =True)[0]\n",
    "        x = tf.expand_dims(x, -1)\n",
    "\n",
    "\n",
    "        \n",
    "        pooled_outputs = []\n",
    "        for conv in self.conv_layers:\n",
    "            conv_out = conv(x)\n",
    "            pooled_outputs.append(conv_out)\n",
    "\n",
    "           \n",
    "        \n",
    "        num_filters_total = self.num_filters * len(self.filters_sizes)\n",
    "        \n",
    "        h_pool = tf.concat(pooled_outputs, axis=3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "    \n",
    "        return h_pool_flat\n",
    "\n",
    "    def embed_user(self, x):\n",
    "\n",
    "        user_embeddings = self.user_embedding(x)\n",
    "\n",
    "        return user_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58f0efa-5d8a-46cd-8bfd-8fb551ff21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b468ec-7175-4aa3-b403-c4f675e54b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "data loaded!\n",
      "loading wgcca embeddings...\n",
      "wgcca embeddings loaded\n",
      "topic emb size:  100\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "print(\"loading data...\")\n",
    "x = pickle.load(open(\"./mainbalancedpickle.p\",\"rb\"))\n",
    "revs, W, W2, word_idx_map, vocab, max_l = x[0], x[1], x[2], x[3], x[4], x[5]\n",
    "print(\"data loaded!\")# Load data\n",
    "\n",
    "print('loading wgcca embeddings...')\n",
    "wgcca_embeddings = np.load('./../users/user_embeddings/user_gcca_embeddings.npz')\n",
    "print('wgcca embeddings loaded')\n",
    "\n",
    "\n",
    "ids = np.concatenate((np.array([\"unknown\"]), wgcca_embeddings['ids']), axis=0)\n",
    "user_embeddings = wgcca_embeddings['G']\n",
    "unknown_vector = np.random.normal(size=(1,100))\n",
    "user_embeddings = np.concatenate((unknown_vector, user_embeddings), axis=0)\n",
    "user_embeddings = user_embeddings.astype(dtype='float32')\n",
    "\n",
    "wgcca_dict = {}\n",
    "for i in range(len(ids)):\n",
    "    wgcca_dict[ids[i]] = int(i)\n",
    "\n",
    "csv_reader = csv.reader(open(\"./../discourse/discourse_features/discourse.csv\"))\n",
    "topic_embeddings = []\n",
    "topic_ids = []\n",
    "for line in csv_reader:\n",
    "    topic_ids.append(line[0])\n",
    "    topic_embeddings.append(line[1:])\n",
    "topic_embeddings = np.asarray(topic_embeddings)\n",
    "topic_embeddings_size = len(topic_embeddings[0])\n",
    "topic_embeddings = topic_embeddings.astype(dtype='float32')\n",
    "print(\"topic emb size: \",topic_embeddings_size)\n",
    "\n",
    "topics_dict = {}\n",
    "for i in range(len(topic_ids)):\n",
    "    try:\n",
    "        topics_dict[topic_ids[i]] = int(i)\n",
    "    except TypeError:\n",
    "        print(i)\n",
    "\n",
    "max_l = 100\n",
    "\n",
    "x_text = []\n",
    "author_text_id = []\n",
    "topic_text_id = []\n",
    "comment_id = []\n",
    "y = []\n",
    "\n",
    "\n",
    "test_x = []\n",
    "test_topic = []\n",
    "test_author = []\n",
    "test_y = []\n",
    "text2id={}\n",
    "for i in range(len(revs)):\n",
    "\n",
    "    text2id[revs[i][\"text\"]] = revs[i][\"id\"]\n",
    "    if revs[i]['split']==1:\n",
    "        x_text.append(revs[i]['text'])\n",
    "        try:\n",
    "            author_text_id.append(wgcca_dict['\"'+revs[i]['author']+'\"'])\n",
    "        except KeyError:\n",
    "            author_text_id.append(0)\n",
    "        try:\n",
    "            topic_text_id.append(topics_dict['\"'+revs[i]['topic']+'\"'])\n",
    "        except KeyError:\n",
    "            topic_text_id.append(0)\n",
    "        y.append(revs[i]['label'])\n",
    "        comment_id.append(revs[i][\"id\"])\n",
    "    else:\n",
    "        test_x.append(revs[i]['text'])\n",
    "        try:\n",
    "            test_author.append(wgcca_dict['\"'+revs[i]['author']+'\"'])\n",
    "        except:\n",
    "            test_author.append(0)\n",
    "        try:\n",
    "            test_topic.append(topics_dict['\"'+revs[i]['topic']+'\"'])\n",
    "        except:\n",
    "            test_topic.append(0)\n",
    "        test_y.append(revs[i]['label'])  \n",
    "\n",
    "\n",
    "y_test = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fffe580a-2e61-4c50-9946-326544f960a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarc_train_file =  \"my_train_balanced.csv\"\n",
    "sarc_test_file = \"my_test_balanced.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4871f510-cd78-4351-9146-35e44a8e7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training = pd.read_csv(\"../data/my_train_balanced.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d3e5ad-ae90-4105-b521-1a9cbaf0a38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c07fd66</td>\n",
       "      <td>['7uaac']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c07fjge</td>\n",
       "      <td>['7uaac']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c07f3md</td>\n",
       "      <td>['7u896']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c07f3ls</td>\n",
       "      <td>['7u896']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1  2\n",
       "0  c07fd66  ['7uaac']  1\n",
       "1  c07fjge  ['7uaac']  0\n",
       "2  c07f3md  ['7u896']  1\n",
       "3  c07f3ls  ['7u896']  0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad5160f6-b174-4921-9cf2-3842521591dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training.columns = [\"comment\",\"post\",\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a778ea2d-7383-455b-9ba9-a1d939505edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c07fd66</td>\n",
       "      <td>['7uaac']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment       post  label\n",
       "0  c07fd66  ['7uaac']      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training[all_training[\"comment\"]==\"c07fd66\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b56e144-5aa7-4492-be78-78f94c6f332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec23422-d0f5-4a93-950f-c8aa9a01d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = json.loads(open(\"../data/comments.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae68b528-b794-4ad0-8049-96b1a36d8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev Sample Percentage\n",
    "dev_sample_percentage = 0.1\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 768\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 128\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.5\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 60\n",
    "num_epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a311eaf6-067d-47be-a5f6-d5cc229aa3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 139232/15470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 19:40:48.875762: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-10 19:40:48.977051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22460 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6\n",
      "2024-01-10 19:40:49.364263: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "2024-01-10 19:40:51.498065: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700\n",
      "/big/xw384/venvs/BERT/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:329: UserWarning: Cluster 14 is empty! self.labels_[self.medoid_indices_[14]] may not be labeled with its corresponding cluster (14).\n",
      "  warnings.warn(\n",
      "/big/xw384/venvs/BERT/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:329: UserWarning: Cluster 15 is empty! self.labels_[self.medoid_indices_[15]] may not be labeled with its corresponding cluster (15).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "topic_train = np.asarray(topic_text_id)\n",
    "topic_test = np.asarray(test_topic)\n",
    "author_train = np.asarray(author_text_id)\n",
    "author_test = np.asarray(test_author)\n",
    "\n",
    "\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = np.asarray(x_text)[shuffle_indices]\n",
    "y_shuffled = np.asarray(y)[shuffle_indices]\n",
    "\n",
    "\n",
    "topic_train_shuffled = topic_train[shuffle_indices]\n",
    "author_train_shuffled = author_train[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "topic_train, topic_dev = topic_train_shuffled[:dev_sample_index], topic_train_shuffled[dev_sample_index:]\n",
    "author_train, author_dev = author_train_shuffled[:dev_sample_index], author_train_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "x_dev = np.asarray(x_dev)\n",
    "author_train = np.asarray(author_train)\n",
    "author_dev = np.asarray(author_dev)\n",
    "topic_train = np.asarray(topic_train)\n",
    "topic_dev = np.asarray(topic_dev)\n",
    "y_train = np.asarray(y_train)\n",
    "y_dev = np.asarray(y_dev)\n",
    "# word_idx_map[\"@\"] = 0\n",
    "# rev_dict = {v: k for k, v in word_idx_map.items()}\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = TFBertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "k_protos, vect_size = 16, 384\n",
    "\n",
    "ProtoCNN = TextCNN(\n",
    "    sequence_length=max_l,\n",
    "    num_classes=len(y_train[0]) ,\n",
    "    tokenizer = tokenizer,\n",
    "    bert_model = bert_model,\n",
    "    user_embeddings = user_embeddings,\n",
    "    topic_embeddings = topic_embeddings,\n",
    "    embedding_size=embedding_dim,\n",
    "    filter_sizes=list(map(int, filter_sizes)),\n",
    "    num_filters=num_filters,\n",
    "    l2_reg_lambda=l2_reg_lambda,\n",
    "    dropout_keep_prob = dropout_keep_prob,\n",
    "    k_protos = k_protos,\n",
    "    vect_size = vect_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# random.shuffle(x_text)\n",
    "sample_sentences = x_text[:15000]\n",
    "sample_sentences_vects = []\n",
    "for i in range(3):\n",
    "    batch = sample_sentences[i * 50:(i + 1) * 50]\n",
    "    vect = ProtoCNN.embed_res(batch)\n",
    "    sample_sentences_vects.append(vect.numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_sentences_vect = np.concatenate(sample_sentences_vects, axis=0)\n",
    "\n",
    "\n",
    "kmedoids = KMedoids(n_clusters=k_protos, random_state=0).fit(sample_sentences_vect)\n",
    "res_cents = kmedoids.cluster_centers_\n",
    "\n",
    "\n",
    "# random.shuffle(x_text)\n",
    "sample_users = author_text_id[:15000]\n",
    "sample_user_vects = []\n",
    "for i in range(3):\n",
    "    batch = sample_users[i * 50:(i + 1) * 50]\n",
    "    vect = ProtoCNN.embed_user(np.asarray(batch))\n",
    "    sample_user_vects.append(vect.numpy())\n",
    "\n",
    "\n",
    "\n",
    "sample_users_vect = np.concatenate(sample_user_vects, axis=0)\n",
    "\n",
    "\n",
    "kmedoids = KMedoids(n_clusters=k_protos, random_state=0).fit(sample_users_vect)\n",
    "user_cents = kmedoids.cluster_centers_\n",
    "\n",
    "\n",
    "\n",
    "ProtoCNN.init_prototypelayer(res_cents, user_cents)\n",
    "\n",
    "\n",
    "\n",
    "predictions = ProtoCNN([x_train[:2].tolist(), author_train[:2], topic_train[:2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0d53895b-f779-4b5b-82c3-3a21776eeca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fb605151720>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProtoCNN.load_weights(\"runs/two_prototype_layer-15_protos-diverge-loss-5-30/127_best_classifier.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33896981-f311-469d-b373-071c18026972",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(list(zip(x_train, author_train, topic_train, y_train)), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cc781-0981-4c15-a861-ab6f78bf8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_res_vects = []\n",
    "sample_res_labels = []\n",
    "for i, inputs in enumerate(train_loader): \n",
    "    \n",
    "    x_batch, author_batch, topic_batch, y_batch = inputs  \n",
    "    vect = ProtoCNN.embed_res(x_batch)\n",
    "    sample_res_vects.append(vect.numpy())\n",
    "    sample_res_labels.extend([x.tolist() for x in y_batch])\n",
    "    \n",
    "\n",
    "   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700a032-140e-4247-813d-a4a8751e684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_res_vect = np.concatenate(sample_res_vects, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bad321-a3e2-4228-a50b-adc5adba9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_res_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7f87b-fb09-4a64-8153-71c549256120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(x_text)\n",
    "sample_user_vects = []\n",
    "sample_user_labels = []\n",
    "for i, inputs in enumerate(train_loader): \n",
    "    x_batch, author_batch, topic_batch, y_batch = inputs  \n",
    "    vect = ProtoCNN.embed_user(np.array(author_batch))\n",
    "    sample_user_vects.append(vect.numpy())\n",
    "    sample_user_labels.extend([x.tolist() for x in y_batch])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c363ef-e789-4026-abc0-a53c672f1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_vect = np.concatenate(sample_user_vects, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "326f22ce-5c77-4ff6-a66c-e29a4c26e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPrototypes(sample_sentences,sample_sent_vects, sample_y, k_protos=10,printOutput=False, k_closest_sents = 20):\n",
    "    \n",
    "    prototypes = ProtoCNN.proto_layer.prototypes.numpy()\n",
    "    #data_size = 10000\n",
    "    d_pos = {}\n",
    "    data_size = 150000\n",
    "    for p_count, p in enumerate(prototypes):\n",
    "       \n",
    "        s_count = 0\n",
    "        d_pos[p_count] = {}\n",
    "        for i, s in enumerate(sample_sent_vect[:data_size]):\n",
    "            #if len(sample_sentences[i]) < 20 or len(sample_sentences[i]) > 100:\n",
    "            if len(sample_sentences[i]) < 30 or sample_y[i][1]==0:\n",
    "                continue\n",
    "            d_pos[p_count][i] = np.linalg.norm(sample_sent_vect[i] - p)\n",
    "            s_count += 1\n",
    " \n",
    "\n",
    "    mappedPrototypes = {}    \n",
    "   \n",
    "    recorded_protos_score = {}\n",
    "    print(\"Prototypes: \")\n",
    "    for l in range(k_protos):\n",
    "        # print(\"prototype index = \", l)\n",
    "        recorded_protos_score[l] = {}\n",
    "        sorted_d = sorted(d_pos[l].items(), key=operator.itemgetter(1))\n",
    "        print(l)\n",
    "        mappedPrototypes[l]=[]\n",
    "        for k in range(k_closest_sents):\n",
    "            i = sorted_d[k][0]\n",
    "            score = sorted_d[k][1]\n",
    "            # print(\"[db] sorted_d \",sorted_d[0])\n",
    "            # print(\"[db] sample_sentences[sorted_d[0][0]]: \",sample_sentences[sorted_d[0][0]])\n",
    "            mappedPrototypes[l].append((sample_sentences[i].strip(), score, sample_y[i][1]))\n",
    "            if k<10:\n",
    "                print(sorted_d[k], sample_sentences[i],sample_y[i][1])\n",
    "        #print(mappedPrototypes[l])\n",
    "\n",
    "    \n",
    "    return mappedPrototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f764654-27ce-4d18-94bf-b56224b1a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e29cd2ac-d9e3-41f2-9b3c-7352bb9dda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2id = comments[\"d1nhsrd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad0cb9b1-60e7-49a6-973f-46606d424feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c07fd66</td>\n",
       "      <td>['7uaac']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment       post  label\n",
       "0  c07fd66  ['7uaac']      1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training[all_training[\"comment\"]==\"c07fd66\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "56e77ce2-0a67-413e-bbdb-6ada8398fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33bbc7f5-a2dd-482f-81ab-f9a96c37c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6304c664-4a13-463d-8606-48016e25102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPrototypes(prototypes, sample_sentences,sample_sent_vects,k_protos=15,printOutput=False):\n",
    "   \n",
    "    #data_size = 10000\n",
    "    d_pos = {}\n",
    "    for p_count, p in enumerate(prototypes):\n",
    "        print('p_count = ', p_count)\n",
    "        s_count = 0\n",
    "        d_pos[p_count] = {}\n",
    "        for i, s in enumerate(sample_sent_vects):\n",
    "            if len(sample_sentences[i]) < 20 or len(sample_sentences[i]) >100 :\n",
    "                 continue\n",
    "            #d_pos[p_count][i] = cosine_distances(sample_sent_vects[i].reshape(1, -1) - p.reshape(1, -1))\n",
    "            d_pos[p_count][i] = np.linalg.norm(sample_sent_vects[i] - p)\n",
    "            s_count += 1\n",
    "        print('count = ', s_count)\n",
    "    \n",
    "    \n",
    "    mappedPrototypes = {} \n",
    "    k_closest_sents = 10\n",
    "    recorded_protos_score = {}\n",
    "    print(\"Prototypes: \")\n",
    "    for l in range(k_protos):\n",
    "        # print(\"prototype index = \", l)\n",
    "        recorded_protos_score[l] = {}\n",
    "        sorted_d = sorted(d_pos[l].items(), key=operator.itemgetter(1))     #sent_id, distance\n",
    "\n",
    "\n",
    "        \n",
    "        for k in range(k_closest_sents):\n",
    "            i = sorted_d[k][0]\n",
    "            # print(\"[db] sorted_d \",sorted_d[0])\n",
    "            # print(\"[db] sample_sentences[sorted_d[0][0]]: \",sample_sentences[sorted_d[0][0]])\n",
    "            mappedPrototypes[l] = sample_sentences[sorted_d[0][0]].strip()\n",
    "         \n",
    "           \n",
    "            #print(sorted_d[k], sample_sentences[i])\n",
    "        print(mappedPrototypes[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6caba8cc-b4b9-4c93-9201-be38f5d85366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pw_distance(A):\n",
    "    r = tf.reduce_sum(A * A, 1)\n",
    "    r = tf.reshape(r, [-1, 1])\n",
    "    D = r - 2 * tf.matmul(A, tf.transpose(A)) + tf.transpose(r)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "715e2924-f333-4a22-b7b8-653c6975735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "protos = ProtoCNN.response_proto_layer.prototypes.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d38fe226-d716-4464-bda2-efe3fc7b9165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 384)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "19e4e690-00ed-4bbb-8ce4-cb417af6b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pw_distance(protos)\n",
    "diag_ones = tf.convert_to_tensor(np.eye(k_protos, dtype=float))\n",
    "diag_ones = tf.dtypes.cast(diag_ones, tf.float32)\n",
    "d1 = d + diag_ones * tf.reduce_max(d)\n",
    "d2 = tf.reduce_min(d1, axis=1)\n",
    "min_d2_dist = tf.reduce_min(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "16ec3f52-7961-4dd4-aee8-27b82c68122f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6972017>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_d2_dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "44b0d530-be87-4b67-808b-9933c5098317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.5901184>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protos = ProtoCNN.user_proto_layer.prototypes.numpy()\n",
    "\n",
    "d = pw_distance(protos)\n",
    "diag_ones = tf.convert_to_tensor(np.eye(k_protos, dtype=float))\n",
    "diag_ones = tf.dtypes.cast(diag_ones, tf.float32)\n",
    "d1 = d + diag_ones * tf.reduce_max(d)\n",
    "d2 = tf.reduce_min(d1, axis=1)\n",
    "min_d2_dist = tf.reduce_min(d2)\n",
    "\n",
    "min_d2_dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "610d87d2-3cbf-405a-84ce-a82660936212",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1cc7c613-1d8a-4c29-bcda-7879c8d2b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_count =  0\n",
      "count =  139232\n",
      "p_count =  1\n",
      "count =  139232\n",
      "p_count =  2\n",
      "count =  139232\n",
      "p_count =  3\n",
      "count =  139232\n",
      "p_count =  4\n",
      "count =  139232\n",
      "p_count =  5\n",
      "count =  139232\n",
      "p_count =  6\n",
      "count =  139232\n",
      "p_count =  7\n",
      "count =  139232\n",
      "p_count =  8\n",
      "count =  139232\n",
      "p_count =  9\n",
      "count =  139232\n",
      "p_count =  10\n",
      "count =  139232\n",
      "p_count =  11\n",
      "count =  139232\n",
      "p_count =  12\n",
      "count =  139232\n",
      "p_count =  13\n",
      "count =  139232\n",
      "p_count =  14\n",
      "count =  139232\n",
      "p_count =  15\n",
      "count =  139232\n",
      "Prototypes: \n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n",
      "fk me , i have been wanting one for ages \\( \\( \\( \\( \\( \\( \\( \\( \\( \\(\n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n",
      "it 's funny how everybody like to cite the cbo on everything when it bolsters their agenda , but completely ignore it when it does n't\n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n",
      "it 's funny how everybody like to cite the cbo on everything when it bolsters their agenda , but completely ignore it when it does n't\n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n",
      "it 's funny how everybody like to cite the cbo on everything when it bolsters their agenda , but completely ignore it when it does n't\n",
      "it 's funny how everybody like to cite the cbo on everything when it bolsters their agenda , but completely ignore it when it does n't\n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n",
      "bf3 had the best menu layout , level progression , and unlock system out of any other bf game imo the battlepack system in bf4 was a half assed attempt at packaging something that should never be packaged for the sake of making it feel special just give me my freaking unlocks and be done with it , i got on battlelog about 6 months ago to play some bf4 for the first time in ages , i had over 70 battlepacks to open and it took me almost ten minutes to open them all\n"
     ]
    }
   ],
   "source": [
    "#show the list of prototypes\n",
    "showPrototypes( ProtoCNN.response_proto_layer.prototypes.numpy(), x_train,sample_res_vect )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f14d7fa8-4d64-4f00-936e-b7f9203d8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPrototypes(prototypes, sample_sentences,sample_sent_vects,k_protos=15,printOutput=False):\n",
    "   \n",
    "    #data_size = 10000\n",
    "    d_pos = {}\n",
    "    for p_count, p in enumerate(prototypes):\n",
    "        print('p_count = ', p_count)\n",
    "        s_count = 0\n",
    "        d_pos[p_count] = {}\n",
    "        for i, s in enumerate(sample_sent_vects):\n",
    "           \n",
    "            #d_pos[p_count][i] = cosine_distances(sample_sent_vects[i].reshape(1, -1) - p.reshape(1, -1))\n",
    "            d_pos[p_count][i] = np.linalg.norm(sample_sent_vects[i] - p)\n",
    "            s_count += 1\n",
    "        print('count = ', s_count)\n",
    "    \n",
    "    \n",
    "    mappedPrototypes = {} \n",
    "    k_closest_sents = 16\n",
    "    recorded_protos_score = {}\n",
    "    print(\"Prototypes: \")\n",
    "    for l in range(k_protos):\n",
    "        # print(\"prototype index = \", l)\n",
    "        recorded_protos_score[l] = {}\n",
    "        sorted_d = sorted(d_pos[l].items(), key=operator.itemgetter(1))\n",
    "        for k in range(k_closest_sents):\n",
    "            i = sorted_d[k][0]\n",
    "            # print(\"[db] sorted_d \",sorted_d[0])\n",
    "            # print(\"[db] sample_sentences[sorted_d[0][0]]: \",sample_sentences[sorted_d[0][0]])\n",
    "            # mappedPrototypes[l] = sample_sentences[sorted_d[0][0]].strip()\n",
    "            mappedPrototypes[l] = sample_sentences[sorted_d[0][0]]\n",
    "           \n",
    "            #print(sorted_d[k], sample_sentences[i])\n",
    "        print(mappedPrototypes[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "58970369-d48f-4852-a221-77f0c6e882fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_count =  0\n",
      "count =  139232\n",
      "p_count =  1\n",
      "count =  139232\n",
      "p_count =  2\n",
      "count =  139232\n",
      "p_count =  3\n",
      "count =  139232\n",
      "p_count =  4\n",
      "count =  139232\n",
      "p_count =  5\n",
      "count =  139232\n",
      "p_count =  6\n",
      "count =  139232\n",
      "p_count =  7\n",
      "count =  139232\n",
      "p_count =  8\n",
      "count =  139232\n",
      "p_count =  9\n",
      "count =  139232\n",
      "p_count =  10\n",
      "count =  139232\n",
      "p_count =  11\n",
      "count =  139232\n",
      "p_count =  12\n",
      "count =  139232\n",
      "p_count =  13\n",
      "count =  139232\n",
      "p_count =  14\n",
      "count =  139232\n",
      "p_count =  15\n",
      "count =  139232\n",
      "Prototypes: \n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#show the list of prototypes\n",
    "showPrototypes( ProtoCNN.user_proto_layer.prototypes.numpy(), author_train,sample_user_vect )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eafb0d-b760-4998-8491-fa27adec6822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c687251c-01b4-4b6d-885d-2a7f67784d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pretty sure it was basically mushu from mulan \\\\?'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[604]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7fc63a4d-0359-4f98-8e32-f15e5d7920d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c07fd66</td>\n",
       "      <td>['7uaac']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment       post  label\n",
       "0  c07fd66  ['7uaac']      1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training[all_training[\"comment\"]==\"c07fd66\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8fd7ebe4-f838-4973-a867-1ec89ab59ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"I only fear failing on the bench press, but that's why I only BP in a power rack.\",\n",
       " 'author': 'Thojos',\n",
       " 'score': 9,\n",
       " 'ups': 9,\n",
       " 'downs': 0,\n",
       " 'created_utc': 1439499693,\n",
       " 'date': '2015-08',\n",
       " 'subreddit': 'bodybuilding'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[\"cu217s2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769f0268-be80-45d6-acd1-c077bb015c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05041444-b63d-4d8a-bc44-a5c204983c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0e71606e-65fb-4777-8a17-1de984a49977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from the file\n",
    "with open(\"runs/two_prototype_layer-15_protos-diverge-loss-5-30/valid_losses.pkl\", 'rb') as file:\n",
    "    dev_losses = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4f38882c-2ca8-49cc-af53-ec18156628e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.19946387>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_losses[2][126]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6a296249-7ac4-4afc-a43c-b4eb44e75798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=178.01305>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=177.3868>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=175.17155>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=171.45692>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=166.22993>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=159.60324>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=151.0718>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=141.00546>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=129.67528>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=117.19717>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=104.114204>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=90.82105>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=78.19996>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=66.61316>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=56.3646>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=47.727154>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=40.336433>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=34.444107>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=29.378752>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=25.166159>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=21.764463>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=18.917631>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=16.504171>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=14.515371>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=12.820837>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=11.398993>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.223048>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=9.195637>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=8.298595>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.5031104>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.8290634>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.2045927>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.7206974>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.2411056>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.794494>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.413844>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.132913>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.833547>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.5859067>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.3119113>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.0258977>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.87788>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.6869657>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.511905>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.3936772>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.2161748>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.134398>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0125794>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.8679979>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.7946205>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.7359827>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.6339052>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5320277>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.4657072>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.4023417>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3201815>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2523841>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2323407>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1414788>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0906658>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0506862>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0031686>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9754611>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9291089>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.88917017>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.87385446>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.80977374>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.77141714>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.78083295>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.75249165>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.74301976>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.71341485>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6619753>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.64954984>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.630266>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.61589503>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.59248996>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.57349837>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.5571671>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.53592896>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.5226516>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.48295176>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.4912409>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.47994533>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.46490726>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.47768477>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.44972202>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.45276502>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.43381116>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.43087822>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.4304207>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.42233366>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.39310592>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.37903488>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.37177074>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.37345996>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3667904>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36179602>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3331411>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3438437>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34292722>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33438683>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3209801>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33126736>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30298954>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3027697>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29342255>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.28659287>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2825816>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.28097123>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2677077>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2648161>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2640095>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2611981>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25937283>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25479782>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24027355>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23207419>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24006285>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23454607>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22155568>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22427551>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22386515>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21014798>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20487309>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20656586>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19946387>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20221956>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19421928>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19470845>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18751906>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19307463>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18960305>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18620072>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18914145>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18714736>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19125876>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18685454>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17692138>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16957285>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17331031>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16489331>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16352203>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15561612>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14928189>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1592045>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15108508>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15483463>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15070021>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14973181>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14724687>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1531557>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14766328>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13761212>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14565614>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14328194>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14126398>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13665518>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13108765>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13215119>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_losses[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "832c4a89-411b-479e-81ea-ecf1a66f7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the file\n",
    "with open(\"runs/two_prototype_layer-15_protos-three-loss-scale-5-threshold-0.8/valid_losses.pkl\", 'rb') as file:\n",
    "    dev_losses = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f828f52d-8eca-4444-8b81-fb1fe59ce503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(), dtype=float32, numpy=11786.606>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11435.2705>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11396.335>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11204.472>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11091.839>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11072.724>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11068.077>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11104.567>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11039.528>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11302.68>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11197.604>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11136.393>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11320.007>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11216.691>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11431.395>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11634.682>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11700.49>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11716.378>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12361.273>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11999.113>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12473.716>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12404.171>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12839.0205>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13113.462>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13248.35>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13354.459>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13106.855>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13984.222>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13594.892>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13807.706>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=14023.248>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=14108.395>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=14157.562>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=14378.262>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=14659.507>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=15073.7295>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=14994.036>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=14849.968>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=15546.9>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=15266.072>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=15360.102>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=15550.669>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=15382.257>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=15543.442>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=15708.276>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=15362.602>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16309.457>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16834.256>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16349.423>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16238.784>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16270.03>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16571.61>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16237.573>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16869.895>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16731.775>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16711.49>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16688.26>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16916.309>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16981.004>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16725.307>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=17364.605>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=17454.979>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=16933.074>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=17822.398>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=17109.395>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=17632.795>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=17289.934>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=17309.098>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=7166.254>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6850.4355>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6776.5845>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6624.5737>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6527.2065>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6525.255>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6528.3003>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6576.2812>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6533.0176>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6794.5796>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6712.4634>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6669.7197>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6862.111>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6781.6895>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7003.1104>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7219.213>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7300.016>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7331.5513>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7980.4795>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7639.2524>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8121.238>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8069.0376>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8512.527>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8798.501>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8944.989>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=9063.416>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8831.898>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=9714.694>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=9341.943>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=9565.96>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=9791.535>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=9888.376>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=9949.517>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=10179.538>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=10470.098>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=10893.848>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=10827.738>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=10695.984>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11399.218>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11130.623>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11236.281>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11436.009>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11278.389>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11448.795>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11622.539>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11290.008>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12241.629>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12774.721>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12300.479>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12199.151>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12241.117>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12549.906>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12226.477>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12866.331>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12737.56>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12726.693>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12711.598>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12947.289>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13021.194>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=12774.567>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13419.827>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13518.739>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13007.2705>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13901.874>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13199.141>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13729.287>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13395.838>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13423.49>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02171>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01561>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02028>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01923>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01923>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.02171>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01668>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01668>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01668>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01668>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01414>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01923>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01668>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=178.01668>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=45.7125>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=45.57406>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=48.52059>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=51.946404>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=55.44812>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=58.57688>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=61.673607>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=65.035385>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=68.66926>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=71.894516>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=75.69047>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=79.72682>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=83.84392>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=87.938965>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=91.768196>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=95.696266>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=99.26359>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=102.823074>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=106.03882>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=109.02275>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=111.60398>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=114.046326>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=116.370316>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=118.36877>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=120.2155>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=121.8981>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=123.329475>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=124.54408>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=125.7439>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=126.75851>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=127.63982>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=128.49449>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=129.10086>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=129.7307>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=130.21443>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=130.68481>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.0117>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.37378>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.604>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.77496>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.88962>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=132.03526>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=132.09439>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=132.06485>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=132.07211>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.9861>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.89105>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.69278>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.62152>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.50818>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.21643>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=131.03687>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=130.77794>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=130.33995>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=130.05295>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=129.70122>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=129.4177>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=129.00395>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=128.58112>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=128.2327>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=127.62202>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=127.14136>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=126.639915>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=126.05146>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=125.55298>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=124.9589>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=124.39436>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=123.80692>],\n",
       " [0.79023916,\n",
       "  0.79023916,\n",
       "  0.79017454,\n",
       "  0.7971558,\n",
       "  0.8078216,\n",
       "  0.8069166,\n",
       "  0.8061409,\n",
       "  0.8067873,\n",
       "  0.8060116,\n",
       "  0.80200386,\n",
       "  0.80400777,\n",
       "  0.8061409,\n",
       "  0.8017453,\n",
       "  0.8010343,\n",
       "  0.8001293,\n",
       "  0.7970265,\n",
       "  0.79663867,\n",
       "  0.79599226,\n",
       "  0.7907563,\n",
       "  0.7941823,\n",
       "  0.79088557,\n",
       "  0.79172593,\n",
       "  0.78926957,\n",
       "  0.7860375,\n",
       "  0.78422755,\n",
       "  0.7867485,\n",
       "  0.7884939,\n",
       "  0.7811248,\n",
       "  0.78442144,\n",
       "  0.7829347,\n",
       "  0.78060764,\n",
       "  0.7848093,\n",
       "  0.7840336,\n",
       "  0.7815126,\n",
       "  0.7823529,\n",
       "  0.7794441,\n",
       "  0.7793148,\n",
       "  0.78047836,\n",
       "  0.77847445,\n",
       "  0.7793148,\n",
       "  0.7799612,\n",
       "  0.77782804,\n",
       "  0.77970266,\n",
       "  0.7794441,\n",
       "  0.77840984,\n",
       "  0.779638,\n",
       "  0.77912086,\n",
       "  0.77310926,\n",
       "  0.7780866,\n",
       "  0.77550095,\n",
       "  0.78002584,\n",
       "  0.77530706,\n",
       "  0.77692306,\n",
       "  0.77672917,\n",
       "  0.77394956,\n",
       "  0.7759535,\n",
       "  0.77860373,\n",
       "  0.77563024,\n",
       "  0.7774402,\n",
       "  0.7747899,\n",
       "  0.7759535,\n",
       "  0.774596,\n",
       "  0.7751778,\n",
       "  0.7729153,\n",
       "  0.7764706,\n",
       "  0.7751131,\n",
       "  0.7767938,\n",
       "  0.77608275])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e18ede1f-9b82-4cdd-9c70-00a0624234ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=16.605793>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_losses[3][126]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f38d506a-4535-4915-88e5-49a91611895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the file\n",
    "with open(\"runs/two_prototype_layer-15_protos-add-loss-10/test_acc.pkl\", 'rb') as file:\n",
    "    test_acc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8aaa11-72de-416d-b8b5-6c3b3c8ec976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.7915597),\n",
       " (2, 0.79152876),\n",
       " (3, 0.795101),\n",
       " (4, 0.7975443),\n",
       " (5, 0.8001577),\n",
       " (6, 0.7993536),\n",
       " (7, 0.7991216),\n",
       " (8, 0.80277115),\n",
       " (9, 0.8029103),\n",
       " (10, 0.80332786),\n",
       " (11, 0.803931),\n",
       " (12, 0.8050289),\n",
       " (13, 0.80300313),\n",
       " (15, 0.80521446),\n",
       " (16, 0.8047196)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de82061-3b35-43d2-b943-758e9a9e6de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af7fe3-dc92-44a9-a23d-aafa239791f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29cb46-341f-4ea5-86f0-57c984e01f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84896f51-d012-40a6-a6fa-a66f6506c514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db6bc4-62ea-45bd-9c12-6063fcd16b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827aaf0-9e07-4783-b8b9-ee4ced29772e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d83efa-91b7-4005-83e0-0b9e6747ddd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
